{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J0Qjg6vuaHNt"
   },
   "source": [
    "# Transformer model for language understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M-f8TnGpE_ex"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JjJJyJTZYebt"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow_datasets as tfds\n",
    "# !pip install tf-nightly\n",
    "import tensorflow as tf\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nPO9PiDqHQRA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0-dev20200310'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fd1NWMxjfsDd"
   },
   "source": [
    "## Setup input pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cojrr3BBwbF8"
   },
   "outputs": [],
   "source": [
    "# !unzip 1mcorpus.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8q9t4FmN96eN"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.\n"
     ]
    }
   ],
   "source": [
    "examples, metadata = tfds.load('ted_hrlr_translate/ru_to_en', with_info=True,\n",
    "                               as_supervised=True)\n",
    "train_examples, val_examples = examples['train'], examples['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YTI5M4Apywrk"
   },
   "outputs": [],
   "source": [
    "next(iter(examples['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O00AovQFv7i1"
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.TextLineDataset([\"corpus.en_ru.1m.ru\", \"corpus.en_ru.1m.en\"])\n",
    "train_examples = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8CEmglu6wqKb"
   },
   "outputs": [],
   "source": [
    "ens, rus = [], []\n",
    "for ru, en in train_examples:\n",
    "    ens.append(en.numpy())\n",
    "    rus.append(ru.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RCEKotqosGfq"
   },
   "source": [
    "Create a custom subwords tokenizer from the training dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KVBg5Q8tBk5z"
   },
   "outputs": [],
   "source": [
    "tokenizer_en = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "    (ens), target_vocab_size=2**14)\n",
    "\n",
    "tokenizer_ru = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "    (rus), target_vocab_size=2**15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4DYWukNFkGQN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized string is [16378, 15104, 3391, 13, 3588, 16340]\n",
      "The original string: Transformer is awesome.\n"
     ]
    }
   ],
   "source": [
    "sample_string = 'Transformer is awesome.'\n",
    "\n",
    "tokenized_string = tokenizer_en.encode(sample_string)\n",
    "print ('Tokenized string is {}'.format(tokenized_string))\n",
    "\n",
    "original_string = tokenizer_en.decode(tokenized_string)\n",
    "print ('The original string: {}'.format(original_string))\n",
    "\n",
    "assert original_string == sample_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bf2ntBxjkqK6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16378 ----> T\n",
      "15104 ----> rans\n",
      "3391 ----> former \n",
      "13 ----> is \n",
      "3588 ----> awesome\n",
      "16340 ----> .\n"
     ]
    }
   ],
   "source": [
    "for ts in tokenized_string:\n",
    "  print ('{} ----> {}'.format(ts, tokenizer_en.decode([ts])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bcRp7VcQ5m6g"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 20000\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kGi4PoVakxdc"
   },
   "source": [
    "Add a start and end token to the input and target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UZwnPr4R055s"
   },
   "outputs": [],
   "source": [
    "def encode(lang1, lang2):\n",
    "  lang1 = [tokenizer_ru.vocab_size] + tokenizer_ru.encode(\n",
    "      lang1.numpy()) + [tokenizer_ru.vocab_size+1]\n",
    "\n",
    "  lang2 = [tokenizer_en.vocab_size] + tokenizer_en.encode(\n",
    "      lang2.numpy()) + [tokenizer_en.vocab_size+1]\n",
    "  \n",
    "  return lang1, lang2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mah1cS-P70Iz"
   },
   "outputs": [],
   "source": [
    "def tf_encode(pt, en):\n",
    "  result_pt, result_en = tf.py_function(encode, [pt, en], [tf.int64, tf.int64])\n",
    "  result_pt.set_shape([None])\n",
    "  result_en.set_shape([None])\n",
    "\n",
    "  return result_pt, result_en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6JrGp5Gek6Ql"
   },
   "source": [
    "Note: To keep this example small and relatively fast, drop examples with a length of over 40 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2QEgbjntk6Yf"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c081xPGv1CPI"
   },
   "outputs": [],
   "source": [
    "def filter_max_length(x, y, max_length=MAX_LENGTH):\n",
    "  return tf.logical_and(tf.size(x) <= max_length,\n",
    "                        tf.size(y) <= max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9mk9AZdZ5bcS"
   },
   "outputs": [],
   "source": [
    "train_dataset = train_examples.map(tf_encode)\n",
    "train_dataset = train_dataset.filter(filter_max_length)\n",
    "# cache the dataset to memory to get a speedup while reading from it.\n",
    "train_dataset = train_dataset.cache()\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE)\n",
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "val_dataset = val_examples.map(tf_encode)\n",
    "val_dataset = val_dataset.filter(filter_max_length).padded_batch(BATCH_SIZE)\n",
    "\n",
    "test_examples = examples['test']\n",
    "test_dataset = test_examples.map(tf_encode)\n",
    "test_dataset = test_dataset.filter(filter_max_length).padded_batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_fXvfYVfQr2n"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(128, 46), dtype=int64, numpy=\n",
       " array([[32783,    15, 22401, ...,     0,     0,     0],\n",
       "        [32783,   323,    30, ...,     0,     0,     0],\n",
       "        [32783,   422,   429, ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [32783, 18485,  7690, ...,     0,     0,     0],\n",
       "        [32783,    14,   170, ...,     0,     0,     0],\n",
       "        [32783,     7,  4277, ...,     0,     0,     0]])>,\n",
       " <tf.Tensor: shape=(128, 48), dtype=int64, numpy=\n",
       " array([[16550,    55,    24, ...,     0,     0,     0],\n",
       "        [16550,   611,    57, ...,     0,     0,     0],\n",
       "        [16550,    58,     6, ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [16550,     3,  2642, ...,     0,     0,     0],\n",
       "        [16550,   162,    70, ...,     0,     0,     0],\n",
       "        [16550,    40,     8, ...,     0,     0,     0]])>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ru_batch, en_batch = next(iter(val_dataset))\n",
    "ru_batch, en_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WhIOZjMNKujn"
   },
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "  return pos * angle_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Rz82wEs5biZ"
   },
   "outputs": [],
   "source": [
    "def positional_encoding(position, d_model):\n",
    "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "  \n",
    "  # apply sin to even indices in the array; 2i\n",
    "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "  \n",
    "  # apply cos to odd indices in the array; 2i+1\n",
    "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "  pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    \n",
    "  return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1kLCla68EloE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 50, 512)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEKCAYAAAD+XoUoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOydd3wc1dWGnzOzu9Kqrbpky5Z7pbhgXDDN9A6hkxBKCCShfEAIBPIFSEghpEBIAiGQkEAKPQSbz8QUAwYbbFPcjZvcZcvq0krbZvZ+f+zsaiVL9tqWbMvc5/e73pnZmdm7snT37nvueY8opdBoNBrNlwPjQHdAo9FoNPsPPehrNBrNlwg96Gs0Gs2XCD3oazQazZcIPehrNBrNlwg96Gs0Gs2XiB4d9EVkg4gsFZFFIvKJcyxfRN4SkTXOY15P9kGj0WgOFCLytIjsEJFlXTwvIvI7EVkrIktEZHzSc1c74+QaEbm6u/q0P2b605RSY5VSE5z9u4F3lFLDgHecfY1GozkU+Rtwxi6ePxMY5rQbgD9CbHIM3A9MAiYC93fXBPlAyDvnA884288AFxyAPmg0Gk2Po5SaA9Tt4pTzgWdVjI+BXBHpA5wOvKWUqlNK1QNvsesPj5RxdcdNdoEC3hQRBfxJKfUkUKKU2gaglNomIsWdXSgiNxD75CMzw3tUq8pg7KgBLFq5kbEjy9n8+XIGHDaIRZsayczz0a95G/UNIUrHHcaSNdtwezM4rNBE2Rarml201tdS1LeEMtVI5fpq0g2hcORA1rcI9TtqMd0eCotyqa6qRUWjZBfkM6TAS2hzBXW1AWwFuRlusspLaHXnsKGqmZL8DArSwKreTsuOZpqtKAAZpkFmbhppRQXYXh9bP1+OR4TMNJP0XC/uvDyi6dk0h23qW8K0BiysUBA7EoaozfghRURbmgg3txJpCRMORwlFFbZSRAEBTAGXCAVluViBEFbQwg7ZhKNRrCiJc+P51gaQc9hIwrYibEUJWzZhK0rUVrEWjaKittNi22NKPYjLjZhuMEyUYcYeEaIKbAVKxfq1qmIbIgIiCM6jYbTtGwYiBiKCO81EKUAplHOP2D6o2D/EM8WVipKVnY6IIIAhgvMyCIIhxJ5zjlVurU28axW7QYffyLb9IYP6IPHfN+cfcfba78dYuXZLqr/3HD6sf6fHRXY+tnTVppTvC3DkyPLO793JscVfpH7vsV3ctzMW7cF9Y/cesAf33pj6fUe1v++ilRtRgdoapVRRyjfpgJHTT2EFUzpXBWqXA8knP+mMc6lSBmxO2t/iHOvq+D7T04P+VKVUpTOwvyUiX6R6ofODexLgqCMPU0vNScyd+zi+KTcy58PH+F7mKB57+SkKbp7FMRefxYOzf8KrM9bw/blz6Xveg5QedhTzr8vEbqzlhPcL+PSlf3L5fbfzYPh1fvz1pxie5eGal5/i6wvSefWxv5FVOpBrbjibPz7yLyLBFo676lJe/trhrL/16/zzH0tpjES5cFQpx/z+eywuO4mrHvmAO64Yw1WDTWqe+Bnz/zCHd6tbARjvS2fSucMYcsPVNB9+Jv+bM5q+aS6mDPQx4oIj6XvxxbSMPIn3Nzby/CebWbKkih3rVuPfvgEr6GfBSzfQOv9Ntr6/iMqFW9m4qYkNrRHqwjbhqMIU8LlNCj0mV991PjVL1lG7qob6iga2+sNUh2zqIzYBO4rtjHEeQzjz5TfZ1BhkQ00LG2tbqKxtpaUpRGtjiGBrmFBzA+HWRqyAHyvYwtzvlWMWlGLmFUNmLtG0bKLeXCJmGq2RKC2RKAFL0RSyOOnyH2G6PRguD4bLjeHyYKZ5MV2exLbh8uDyuOk3rAArHMWK2FgRG9uKYkWiRK0oth3FtqJE7Si2ZRG1wkw5cQQel4HHZcYeTYM0l+Eca9/uvf9vqKgd+x1yPrxi27HHqPMI8Phff4AhYIpgiGAasQ+VjvsiYCAcdd5d7e61K2a8+QjQNsjHv1KLc8BIGqEHTLsl1T8LAN55/w+dDvBGJweLj7s55fu+/+Fj7fY7e404+VNvSvm+AB/OfTzlc3OPuTHlc+d2uK9vyo1EFv019U+NzrCCuEacl9KpkUV/DSZJ13tDZz9mtYvj+0yPyjtKqUrncQfwKjFtqsr5+oLzuKMn+6DRaDR7hAhimCm1bmALkPy1sB9QuYvj+0yPDfoikiki2fFt4DRgGTAdiEeirwZe66k+aDQazZ4jzjfW3bduYDpwlbOKZzLQ6Mjfs4DTRCTPCeCe5hzbZ3pS3ikBXnW+zrqAfyml/isiC4EXReQ6YBNwSQ/2QaPRaPYMZ6bfPbeS54ATgUIR2UJsRY4bQCn1BDATOAtYC7QC1zrP1YnIT4CFzq0eUErtKiCcMj026CulKoAxnRyvBU7ek3ut2BFmyp1X8d7ISUy55VE+Pvp4Lj2imEvnxT5pp5+bx603ruSHPzubM/84n2BjDc/efhzvnnkakVdeZ8nMX1A+5RweOn0w74x4joCtOPVbU5ifPpr333gFFbUZffzRvDxzFa21lQw45lzuOmU40bf+zOLpq6kO2YzPTWfUpROwxp7NP/67hnHj+nDa0AKiC/7FhreWs7QxRDiq6O91M3hoHmXHj4Vhk1hRHSDLZTAo003xEcUUTjgMVX4Em5rCfLq5gfVbmmiqqSdYX4UV9AMQrlhOw+rNNKyvp2Gbn+qQjd+KEo7GJD2PIWSaBvkek5atNbTu8NNaE6AxaOG3orTYsXPjer4psVYfiFDfGqa2JUytP0woYBEOWIRDFpFgK3Y4gB0KELXCqKiNkZGNkZ6JeLxEXekoTwbKlUbYUrGAcFQRtqOErChixr/yGohhYrg9GM5XYMPlQQwT0+VCRLDj2r0dRUVjgWQVVURV7FEpRTSqEtq5aQimYcQeRZz9TlpSlFRFo7v+/bSde6eo57fdd/d6/p7QWWB3b+hMz5d9uHk3datXIoCY3TPoK6Wu2M3zCug0QKKUehp4uls6kkRPB3I1Go2mdyGC0U0z/YMRPehrNBpNB7pL3jkY0YO+RqPRJNONmv7BiB70NRqNJglBMFzuA92NHqNXuGyGmht456Qgb25pYvbZJq+urGby/Dm8/tif+elPruPt47/K0Xleaq75OfOff5GJl17C6LmP8erKam577COUbXPvdUdT89BtzNzaxJn9c+jz3R/z/ZeWULN6IcWHTeXe8w5j62fvklnUn7NOGcrkjAaWP/k6C+uD+NwG46eUUXTh13h7fQPvL9zM5RP6U9aynq1vzGb1smqqQhZeUxid46HfMQPJnHQS241c5m6soyTNRf+BuZROGEr6EVOo9xSwaFszn22sp25bMy07NhFuaQTA9HgJblhHw9pKmrY0UR2yabJiiVYQC+JmuQx8bieQu70Wf1ULrXUBGiPRRMDXTso8NUXwGEJdMMKOphC1/hDBQIRwIEI4ZMUSpEIBrHAsiBu1ItiRMEZmDkZmNlGPl6jbi3KlEVHEArhRhWVDa8QmaEUTQdt4EFeSg7hmPJgrmC4DFSUWsI0qbDvqBG1VIjlLOUFcFbVRtp0I1HrMWAJWPDGrYyDXEGkXaN1VYhZ0Hvzsij2JicZfL5XErL3hyxxk3S/s33X6+x0909doNJoO9NYBPRX0oK/RaDTJiHTbks2DET3oazQaTRLCoT3T7xWafv/yPvz8mJu57/eX8fCE67jzzhM4+n/fos+4U7iu6j/8p6Ker07/MRf+dDYZBX15/TuTeP6mfzA8K431H07nyLPP48r8amY8+gH5HpPjH7yEZ9Yrlr/zAZ5MH6eecTgnZtRghwMMnjSFW48bSMNzf+DjuVvwW1Em53sZ9fVpVOYfztPzNlC58gumDfQRmPMq699ex2p/GFvBwAwP/ceX0mfaZKwBR/FpZTPvrtzB0Cw3fcb3wTd2LJE+h7G2PsgnG+up3NxI845thP31RK0wYph4Mn3Ur95M48Ym6moDicSsZOO0eGJWRr6X5m1+WmsD1IVtGiM2QUdvT07M8hiC1zSo84epawnTEE/MCtlEQhZWwI8dDhCNOHp+PDkrMxvlyUS5M8CdTtSVRjCemGUrglaUoBWlNWK3M1oTw8RISsoyXB4MQzBNA9M0EolZthVFRUlo+QltP67p2zFdP260ZhqCK0nDb6fri2A6Ynd3J2ZJ4r6pJ2Z1ped3ds6+ohOzuhkxMF2elFpvRM/0NRqNJhk5tGf6etDXaDSaJAS9Tl+j0Wi+VBzKg36v0PRz6rdSmu7i8eHfAGDJ1Q+x5t1Xmf2Ls3j0a49z1fHlPGqNZ+O8GXz3jkuovO1rLKwPcsV9Z5DTbzh/u34in990J4sbg5x/0kBazrqd3zy3GH/VBgZOnsYPTxnKtj/+moKh4/nOuaMo3/oRi//8ISubQ/T3ujn8K6PwnHIV/1lVzfLPt9G0ZTXeNR+w7rWPWLKpkbqwTb7HZGRpJv1PHI1n3DTWNineW1PD1op6+h5RTOmk0bhGT2ZryOSzbU0s3lBHXZWfQP32xBp9lzeLNF8hDWuraNrSxPZgfI1+m9Falium5/vSXWSWZNBS1UJzY4jGSJRgVBGw24zZ4td4DCHdkMQa/VDAIhSIEAlZRIJB7HBsjb7trNOPa+nizUZ5vCh3GlG3l5AVTej5YVvRGrFpjUQJ2dGE0VrceC2h5ztr9g3TwHAZiCFErVjBFKVUrGBKB6O1uOFbvO3WaM1Zo28YktDzd7dGP05Xen5HUl1bvzvdP36fg1XPP9AcFF3X6/Q1Go3my4SWdzQajeZLg4hguHvnypxU0IO+RqPRJKMN1zQajebLhR70DzDbq/xcu/0Tcs77Nf4FT1J42x+Zdv11tNxyGS12lPFvvMHZF/ySISdewN19KvnB3xZx0cgCgt/4GZcPqqB8zhP8+O31HJ2XzriHf8Q1M1ayYd4sfOWjuPmiwylb+X9Mf+pjxv74Oq48vJB1t93OhxX1mCIcMyKfAVdeyqJANs+9/znVqz4laoXZMeNVKuZsYnMggscQhmd5KJ/aj7zjTqQhdwgfrqjmk1XV1G+tpM+EgWSOnUIgfzDLNjQyb00NNVubaaneRKi5PpYI5fLgycgho6CMhk8bqWoOUx9pC+KaAlkugxwnkJtZkklWSSZ1a+qpC8cSuJKra0H7IK7XNKhrCdHcEiYUjBAOWERCVpvRmpOYFU0KoCpPzGRNuTOwMAjbUafFgrghO0rIsmOVs5IM1swOQVzTZWC6jFig1GUkErFsSyWM1+KJWclGa+0CuR0Ss9olaDmJWfHKWbsK4sYTs6DzgG2c5MSsvQ3idrfR2v6gF3Rxv2D0hv+svaRXrN7RaDSa/YWIIEZqLcX7nSEiq0RkrYjc3cnzj4jIIqetFpGGpOfspOemd8f76xUzfY1Go9mfmGb3zIdFxAQeA04FtgALRWS6UmpF/Byl1O1J598CjEu6RUApNbZbOuOgZ/oajUaTjNCdM/2JwFqlVIVSKgw8D5y/i/OvAJ7rhnfRJb1i0C8p8DLuV8spO+pkTp4lmGle3jgFHnt+Bd977ApO+PU8rGAL/7nnRP57+v/gMYSTXvollz0xn4dPKmbmzc8QjirOuvNk3pYRvPXqXADGnDqFa0dmsuTBp5hT08pPzh6N/dojLPj3SrYHLcbnpnPEtcfSOuYcnvp4I+s/X0VrbSXevFLWzljM4sYQAVvRN93FsFGF9D9lAoycyufbW3hz+XaqNjXgr9pA0ZRxRAeNY119iAUb61m3oYHGqhqC9VVYQT8Ankwf3rxSsvOzaNjmZ3vQaqfRe00jYbTmy08nqziDzNJcGoMWjZEoLXZ0J6O1ZLO1LJdQGzdaC1iEQxaRYCt2OIAdCiQSoqKRtsSoqDsD5ckg6k4nFE/KiirCdpSQY7QWf4xr+IbRPjnLdLkwzVhSViw5K1ZAJWo7Wr6ToBVPzErW8yGmk5siXRZOiSdVGU6C1q5I1vOh68SsuJ6fzJ4mDe3qDyv5XvvyB3ioGa0dFIlZxF02u23QLwM2J+1vcY7t/LoiA4BBwOykw+ki8omIfCwiF+zlW2qHlnc0Go2mHbufQCRRKCKfJO0/qZR6st3NdkZ1cgzgcuBlpVTy7KRcKVUpIoOB2SKyVCm1LtXOdYYe9DUajSYZR95JkRql1IRdPL8F6J+03w+o7OLcy4Gbkg8opSqdxwoReY+Y3r9Pg36vkHc0Go1mf9KN8s5CYJiIDBIRD7GBfadVOCIyAsgDPko6liciac52ITAVWNHx2j2lVwz6gZIBrH3/dZY+fBbznn2G6b+/nj9Puo6LRhbwxoTv8Pmrz3HFzVeS+dgdzNjSxDduPoYn/UNY9Nq/WXvr9by9o4WvHNUH322/4e5nP6WuYjHlE0/l0YuOpOGpn/D2+5sAGBdezae/ncnC+iCl6S4mnDGY3Iu+yb+/qOGDjzZRv2EZhstD/tDxLF9Vx/aghc9tcES+lwEnjyRjyllsjGTyzupq1q2tpWHzaoKN1XiOPJ7t5DB/SyMframhdntsjX7CaC09ZrSWWVhKblEGWwMWTVZ0p2Lo+R6TwjQXmcWZZPXNJqusiLqwTYsd3clozZSYlp9uGGS5Yq21JUw4EHHM1sJYAf9OxdDjej7gmK1524qmtDNaa2uBiN1mrObyxJrbeXT0fNM0EoVUEuv07fbGa8kma8ktWcv3dND2jaQ1+qakbrSmovZujdb2ZY1+2z26XqPf3Xp+b+Zg0fMh1hfTJSm13aGUsoCbgVnASuBFpdRyEXlARM5LOvUK4HmlVLL0Mwr4REQWA+8Cv0he9bO3aHlHo9FoOtCdTqVKqZnAzA7H7uuw/6NOrpsHHNFtHXHQg75Go9EkIc5qsEMVPehrNBpNB/YgkNvr0IO+RqPRdOBQHvR7RSB3w8bt/ODnt/PeyElMufIqCn5+PRtaI5zw8Sxu/uHfKZ9yDk9MsPjTQ7M5f4AP771P8JNH3sCT6eO5F1cwxpfOMU/cx20zvmDV7DfI6TecGy8/kuGbZvPxb95hQ2uEqQVeKh75Ne8t2QHA8cPyGXb9V1khfXn6nXVsX/4pdjhATr/hDDmylNX+EKbA8CwPg6YNoPiUk2kqHs37G+r4YFkV1eu30FpTiYraBIpHsKSqhQ/XVLNjSxNN2zYQbKwhaoUxXB7SsvPIKCgjtziTgX2yqXEM1GwVS7DymkKOy6AozSSzJIPsvllklRWRWVZEY6SzIG7smnQnAJzlErLSXARbI4Qco7V4ENcOBbDDQewO1aoAlDuDiLgIWVGCTtWsYCRKa6QtMStoRwmEbScRa2ejtXjw1nQZGGYsQcu2VCyA24XRGtCuH50ZrSWqaQmJxKz4V/LOgqrJiVm7qm6VbLTW8VhX7EkQtycDlvurYtYerGHvnUjsPabSeiN6pq/RaDRJCLHJyaGKHvQ1Go0mGTm0rZX1oK/RaDQd6M3F5XdHr/gO487I5saVT/LmliZmnwmPPPUZdz/xNab+9jNCjTW88aNTmXnstZginDbzUS74/UfUrF7I8Zefi9+KcuEPTuUt7zimv/A+Kmpz1JnH8Z3Dslj849/z9o4W+nvdTLr2aD58bimVjtHamBtOIDDhKzw6p4KKz76gpXoz3rxS+h8+mq9PGUDAVvT3uhl1RDEDzpwER5zEJ9taeH3JNratr8NftQEr6MdweVhbH2JuRS2rK+qp37q9U6O1nEIfRSVZHNk/dyejtRyXmTBay+6TRWZpLlllRbiKypzErPZGa/HiKXGjNZ/bJC0njXDAiiVmJRmt2eHgTkZrceJGa8Eko7V4QlbcaC0QjrWEnt/BaM1wGQmjtXghla6M1pL7kDB965CclUjSMo2Eju82jJi23+EPNZ6Y1ZWevzujNUO6V4M/WI3WDjQHW9djhmuptd5Ij3dbREwR+VxEXnf2B4nIfBFZIyIvOKnJGo1Gc3AQXxyQQuuN7I/PqluJpR/HeQh4RCk1DKgHrtsPfdBoNJoUEQzTSKn1Rnq01yLSDzgb+LOzL8BJwMvOKc8A3eIRrdFoNN2B6Jn+PvFb4C4g6uwXAA2OCRHsuqDADU7xgE9K0oLcf9sr3P/4FTw88Qa+NrmMZ0dey+L/PM+t91yHPHAdr29r5lv3nc4vtvVl0WsvM/j483nxqnFcPm0gru88xJ1/XkhdxWIGTz2Dxy45kupH72XmuxsxBU6Z2o/+N36XhfUB+nvdTP7KCHIuuZHnlu3gw7kbqd+wDNPjpWjkBE6bXM6ZQ/PJ95iMLc1i0BlHkH7MuawNpjNzRRXrVtfSsOkLgo3ViGHizSvho80NfLymhprKJqcYeh0QM1pLzyshu7gP+SWZHF7mY0RR1k5Ga0VpJkUZbjKLM8nu5yO7vIS00lJcpeU7rdGPa/mZZsxkzec28WS4ScvxEApECAcCWAE/kaDfMVoL72S0Fie2Pj9mshayFM2hnY3W/EGL1rC9S6M10+U8Ohp/qkZr8SLtqRitGUZ7w7WujNaS2Z3RWvxwx3X7yeyr0Vp3aPH7U8/v7rXpB5ueH6c7a+QebPTYoC8i5wA7lFKfJh/u5NROCwoopZ5USk1QSk0oLCjokT5qNBpNR0ToPBmwk9Yb6cklm1OB80TkLCAdyCE2888VEZcz299VQQGNRqM5IPTWAT0Vemymr5S6RynVTyk1kFjhgNlKqa8R84W+2DntauC1nuqDRqPR7ClCarP83vrBcCCSs74PPC8iPwU+B/5yAPqg0Wg0nSICHm3DsG8opd4D3nO2K4CJe3J9zbJVXDJ+HI8NuQYPLzPsv29y1rn3c8Q5l3Kv9zPufmIhX5tcRt01D/Lwdb8nq3QgT99+LFu/dxUT/vxbzv3nIta+/zoFQ8dz/zVH0W/hP3jp93OoDFqc2y+Hsfdcyzy7Hx5DOHF8KUNv+jZz/dk8Peszti39CDscoHD40YyZUMaV4/tRWLWIw3PSGHLaEIpOO4vq3KG8tayKeUu3U7N+Pa21MaO1tOx8skoG8faKKrZvbKBpWwXBxppYcNLjJd1XSGZROXklWYzon8sRZT6G5mcw0zFay3IZ5LlNitJMsvtmkdMvVi0rs28xrpJy8BXvFMT1GG1Gaz63gddjkp6XjjcvnVAg0lYtKxKOGa1FYsHczgK5sUpZUULWztWyWpISswIRu10Q13TFDNbiRmsikkjSMk1jJ6O1qBVG2e2DuNBmutYuKctlJIK37qQErWQDrOQg7t4YrSVP4PbGaC1xbSdGa90dxE319bvnfl+SIK7ETP4OVbQNg0aj0SQhHNqavh70NRqNJhnpvXp9Khy6wpVGo9HsBbGZvpFSS+l+ImeIyCoRWSsid3fy/DUiUi0ii5z2zaTnrnYsa9aIyNXd8f56xUzfVjDgv29y5tl341/wJEPueJ3Mov7M+/4Unug7kVHZaUx641WOuHc2rbWV3PXALYz99K/88i+fkXNZFvNefglPpo9Lv3oCF+Xs4P27/8rc2gBjfOlM/v7pVI+9kPue/Yw7irMYd/v5bO4/lYdeXsr6Tz4j2FhNdp8hDB4/km8eM5DhRi01019kxOQy+p97Mtbok5izpp7pn25lW8UO/FUbsMMBXOlZZJUMpLC8hIp1dTRWbqW1thI7HEAME0+mj8yicnKLMinrm82R/X2MLMykNDP2X5Ks5/uKM8npl01OeTHZ5SWYJeUYxeXY2SU7Ga15naSsLJdBjtvE6+j56XnpWAE/djjgPHZeOCWZkKVihmtWNEnPjxKyYoVT4olZ8SIqhsvTVjQlbrZmSkLfNwzBdAm2FSVqR7EtK1E4pbPErDjtDNdEcBttOn7caM2Unb+S70rPj8UK2hutdVU4paPOv6cciMIpB7uef7DTXTN9ETGBx4BTiSWjLhSR6UqpFR1OfUEpdXOHa/OB+4EJxPKZPnWurd+XPumZvkaj0SRhSFsG+O5aCkwE1iqlKpRSYeB54PwUu3I68JZSqs4Z6N8CztirN5WEHvQ1Go2mA7EVYrtvQGHcLsZpN3S4VRmwOWm/K+uZi0RkiYi8LCL99/DaPaJXyDsajUazv5BOpMJdUKOUmrCr23VyrKP1zAzgOaVUSES+TcyI8qQUr91jesVMv/SwwUz51tOUHXUyJ88SqpbOYcYjVzH3mFOpDEa45vUHOP1vX7D+w+lMuvxS7h/m58Ub/kJjxOY3j79DoL6KceeeyUOnD2bZXfcwc2UNpekuTr3ySLKu+SE/m72OlR98zlG3HA9n3cxvP9jAkg9W0rRlNem+IvodOY6vnziYaeVZhGf/kzX/+YxhF07BmHguC7e18p9FW9m0qobGTSsINddhuDxkFPYlt185g4fkU7u1Bn/VBiItjUCscEpGQV98JYUUl+UwfkAeo4uy6JfjIStU5xRCj+n5BXnpZPXNIrtfHtnlJXjKBuDuO5BoViEhTzaQrOcLmWZsfb7PbZDm85Du6PnpeZlYQT+RQJvRWmeFU+KIYRK0YwXR/WELv7MevzVi4w9ZbXp+xCYQtjDcHkyXK2Y5G1+T75J26/UNM2ZZm1w4ZVdGa3G9P2G4lrQuv6PRWnydfmeFU7oilcIpe2q0lnyfjtfvL6O13rDw5GAPEXRjRu4WoH/S/k7WM0qpWqVUyNl9Cjgq1Wv3hl4x6Gs0Gs3+Ip6clUpLgYXAMKd4lIeYJc309q8nfZJ2z6Ot/sgs4DQRyRORPOA059g+oeUdjUajSUKQbrNhUEpZInIzscHaBJ5WSi0XkQeAT5RS04H/EZHzAAuoA65xrq0TkZ8Q++AAeEApVbevfdKDvkaj0SSxh5r+blFKzQRmdjh2X9L2PcA9XVz7NPB0t3UGPehrNBpNOw51G4ZeoemvqI4Qaq5j6cNnMe/ZZ7jrgVvIffB6Xly6g+/+9Gx+0TqGj/75LwYffz7//fZE3rvgRj6uC3DFKYOo/uJjhk07n79efRQ1D93GazPWYCvFWSeUM+j79/LU0jpmzlxOXcViCq+7k6cXbWPm22upWb0Q0+OlePQkzj1hEF8ZWYjMe5HVL8xhybJqMk+6iLVWDi8vrmTpsh3UrV9BoL4qUS0rt/9w+g7KY9qoYpor17arluUt6EtOaT8K+mQxfkAeR/TJYVBuOvlGCFfdRnxOUlMqqr4AACAASURBVFZRhpucftn4ynPJGdiH9P79cfcdiJ1Tip1VRH0wFkzsrFpWRk4a6blOYlaul7TcbCLBWHJW3GhtV0FcMcxOq2W1hHcO4iYqZ5nJRmtdJGm5jHbVspKDyZ0FcZMN15KrZcUTtNzx405AtzM6S8za6T3volqWIe2XUewuiJt8zzhdBXH3dmzR1bJ6EF1ERaPRaL48xP30D1X0oK/RaDQd0IO+RqPRfEkwDvEiKr3inQWbGnjzz7fx3shJTLnyKu6qe5nf/ukTvn3hCJZ+5T5+8+Az5A8ew2v/O40137iIF5fu4ILBeYx/+o+UjpnGb781iZK3HmXGox9QGbQ4a1g+435yG2/4i/njS8uoWjoHd6aPN2rS+fOMlVQunoOK2hQOP5pjjx3I1Uf1I3/DXDa8OINlH25mtT/E1uwhTF9ZxdxFlVStWUVL9eZE4ZScfiMoHZDHSYeVMKVfHoH6qkThFG9eCdklAygsy+GIgfmM6edjRGEGfTIMXLUbCFcsp9BjUpruiun5/XLIGdSHzPIy3H0GovL6Es0upj4UpSFoJwqntOn5BpleVzujNW+Bj/SCHOxQgKgV2WXhlLieL4aZ0PGbw22FU/xBK2a2FrLwByMJw7W4Xu9ym20Ga0mFUxJavyFdFk7pqOfH8bgM3IbRZeEU02hfRGV3RmuJ97qLwinJen5X16eKLpzSxkGv54PW9DUajebLhJDw1Tkk0YO+RqPRdOBQtpLWg75Go9EkIdDl8t9DgV4x6PfrX4px06W8uaWJ2WfCD8Y+zXlD88l/6hXOuP4viGHw2L0XkPnYHTzx0kom53s55eUH+dHiKP/7neM5fse7/N/tz7G4McgpxZkc+9DVLO97PD/+8wI2LJiNGCblR0/joekr2LBgHpGWRvIHj2H0lGHcctxgBresYevzz7FqxmqWNYUI2Io31tQyY/5mtq3eiH/7BqJWGHemj5yy4ZQOLOKY0cUcOzCfEQVpRK0whstDuq+QrNJB5PfJZlh5LuMH5HJYcRZlWW7ctWux1i+jZe2amJ5flk3uQB85g0rJGdgHV99BSGE/rOwSGi2D+qDNtuZQwmQtruf70l0Jk7WMwgzSHT0/vcAXW6O/i8IpyXq+GCb+sI0/bLUZrQVja/SbQ1ZifX4gbGNF7JiWn2ysFtfwk9bsuxwP8rieH91NEZdEYfQkczVDBLcp7QqnJG+nqufDznp+Z+ZrEBsEDJE90vM7myh21PO7e43+wa7n9xqc37VDlV4x6Gs0Gs3+QgB3iqUQeyN60NdoNJoktLyj0Wg0XyacJcGHKnrQ12g0miTiMZxDlV4hXOU2buOp19dw/+NX8PDEGxiVncaJn73LtB/MoqlyHf977zWcuvgp/vTQbPqmu7nsr9/hWXs0f3rida4vrOL9bz7ErKoWxuemc8pPzmfH1G9w6/OLWP3+e1gBP6VjpnHlOSP5Ys5HtNZWkt1nCMMmH8l3Tx7GGFc1NS/9lRUvLmJhfYDGSJSiNJPnP9rI5i+20rB5JVbQjys9i5w+QygZXMb40cVMG1bI4cUZeHesQgwzFsQtGURhWT6DB+QyaXA+Y0py6J/tJr1hE/amlQTWfkHDms3kl2SSOyAH38ASfEPKcPcbilk6CNvXhxZJpz5kU9kcYmtzEG9SEDfPE0vKyijMIKPAS3pBdiKI68rNx3aqZcUDqJ0RD+Kabg/NoVjFrGbHZC1htBa2E4/hsI1tRXc2VosnZLli1bJcScWkOyZldWW0Fm9t5mpGokpWcqJWW+WstveRqsla8nY8iNsuuLtvv7pd/oF1f9C1u+/X/YNebxpHY8Z+u2+9ET3T12g0miTEmVAcquhBX6PRaJI41OUdPehrNBpNB3qrdJMKveI7zLbtzXz/juN4bMg1AFy5+GUm/nQuWxb+lytv/wb/Y8/jDzf8HVOE6x++mPeGX8Z9j7xF46aVfHT1HfxnVS3Dszyce9fJ2Ff8kJtfWcrSt+YQqN9O8eipnHfmCG6a1I/mbevILOrP0MkTue2MEUwrsmj+z19Y/o/5LKhspjpk43MbjM9NZ/2ybTRsWEakpRHT4yWrdCDFQwZzxKhiThtZzLg+Wfga1xNeNpd0XxFZJYMo6F9M/wG5HDOskHF9chiY6yGzpQq1eSXB1cuoX72Z+jXV5A3OxTeoGN/QMjz9BuPqOxjbV0qrK4vagM325jDbmkNsqQ+Q4zLI95jke8yYuVqhl4xCL97CbNILfGQU5+HOy8PIKdilnp+clGW6PYnkrHZJWUELf8iiORhJ6PlWxMaKRDFcBi53e9M1l9tIFFaJ6/lpLqPNcG03en6cZD3fZRpJGn6bnu822/xSUtHzE/eW3ev5hshe6dHdXTily9fpBQNUb5o4C20GfrtrKd1P5AwRWSUia0Xk7k6e/66IrBCRJSLyjogMSHrOFpFFTpve8dq9Qc/0NRqNJplurJErIibwGHAqsAVYKCLTlVIrkk77HJiglGoVke8AvwQuc54LKKXGdktnHHrFTF+j0Wj2FzFNP7WWAhOBtUqpCqVUGHgeOD/5BKXUu0qpVmf3Y6BfN76dndCDvkaj0SQRt2FIpQGFIvJJUruhw+3KgM1J+1ucY11xHfBG0n66c9+PReSC7nh/vULeKc5L58OvPshPv/Mg/gVPcuyz21k562VO/871PD5iB0+d8HPqIza3/fhM1pxxJ9954E12rJjLkBMv4IXf3UrfdDcX3jgF322/4VuvLOOjGe/jr9pA4fCjOe3sMdxz0mDSZv8Zb14pgydN4cazR3LOgHSCr/yWpX+bw0dr66kMWmS5Ynr+sJMHUl+xmGBjNYbL4+j5wxk5spAzDivh6L7ZFLZWYi2bS83Hn5FZNIG8slL6lucydVgh4/v4GJybRk6wBtmyguDaJdR9sZG6VdupX9/AkDNGkDe8P+kDhuAuH47l60sgLY+aVovt/jBbm4Jsqm9lY20rx7hja/Qz8mNafkZhBt6CLLxFeTE9PzcXI7cYM69ot4XQDZcHMePbbvxhyymW0qbnB8KxIiqBoJXQ862IHTNVS1qfb5iS0PO9HjOh53tcZkrr8yFuuBbtVM93J23HiqLLHpmiqajdrhA6dK3n7w2p6vn7nAfQA1r5obxyJSUE9mDFZo1SasKu77YTqtMTRa4EJgAnJB0uV0pVishgYLaILFVKrUu5d53QYzN9EUkXkQUislhElovIj53jg0RkvoisEZEXRMTTU33QaDSaPSW+ZLObArlbgP5J+/2Ayp1eU+QU4H+B85RSofhxpVSl81gBvAeM2+s35tCT8k4IOEkpNQYYC5whIpOBh4BHlFLDgHpiX2c0Go3mIEEcO+/dtxRYCAxzJrse4HKg3SocERkH/InYgL8j6XieiKQ524XAVCA5ALxX9Nigr2L4nV230xRwEvCyc/wZoFt0Ko1Go+kOunOmr5SygJuBWcBK4EWl1HIReUBEznNO+xWQBbzUYWnmKOATEVkMvAv8osOqn72iRzV9Z7nSp8BQYsuW1gENzg8CdhHUcAIiNwD0yUjvyW5qNBpNgpgNQ/fFNZRSM4GZHY7dl7R9ShfXzQOO6LaOOPTo6h2llO2sMe1HbOnSqM5O6+LaJ5VSE5RSEzIHDefb//Mbyo46mZNnCZ++9E+mXn0Nr51s8o+TbmW1P8RNd5xA3TUPcsUv3qXy01kMOOZcnrxlKvkek8u+MY4+9/6OO/5vFbNemUPTltXkDx7DiWdP4P7ThpH70T/57JcvMWjysdxwziguH5mL9X+Ps/Qv7zJvWTWbAxGyXAZjfGmMPHEAgy+cRqB+e1IQdyQjRhdx/pi+HNPfR0m4CmvpHGo+Wkjl/Ary+/en78BYEPeoMh9D89PJjdQjW1YQWv05dcvWU79qG/UVDVTXBMgbXk76wFgQ1/b1JZRRQG3AYkdLmM2NATY1BNhY28qWulbyPSZZeelkFHrJLMkkszgbb1Ee3gIfnoJ8zLxiTF8BRnY+USu808+5YxDXdHkwXG4Mlwd/yKKxNdIuiNsctAglJWVZEZuoFU1UznJ5TAxTEglayUlZHpfZVjkrxSAukKia1VUQ1x2vntXJb3NXFbmgLYgbr6CV+Jk4j/GZ3L7ENQ9kEHdv7v+lD+I6iKTWeiP7ZfWOUqpBRN4DJgO5IuJyZvudBjU0Go3mQGLs80fywUtPrt4pEpFcZ9sLnEJM03oXuNg57WrgtZ7qg0aj0ewpgp7p7y19gGccXd8gFsB4XURWAM+LyE+JpR//pQf7oNFoNHtMb/Az2lt6bNBXSi2hkzWlznrTiXtyr4oN2yn/6lSWPnwWvik3MuXKq3j7ghz+OeEKFjcG+Z/bjqX1tke58Kez2fTR65RPOYc/3X4sE1Y8T+k1Y+n/4JPcMWsjrz73Hg0blpE78HBOOHcKD549iuJPXuCzB//BO59u41u/Hs3VRxRiz/gdix6bxbxFVWxojeA1hTG+NI44oZxhl0zDfdzFGK5fkFU6kKKhoxk2uogLxpYxtTyXPpFqosvmUDP3Yyrnr6NqWTWl5+dy/IgipgzIY1RhBgVWPcbWFYRXf07tknXUrtxK7Zp6duxoYXvQwjtkGJ6BI7Hz+hPKLEokZW1qDLKpIUBFdQsba1poagiSlZdOZnFmTNN39PyM4jzSigtjen5eMYavkKjXt9PPdVd6vuH2tNPz/cFIQs8PhyysSJSoFWtWJEp6hrtTPd/rMdvp+R7T2CM9X0Vtx3BNdqvnd9Sjd6Xnx0nW8w3pWs/fm6/EWs/vpfTiWXwqpDzoi8gxwMDka5RSz/ZAnzQajeaAIaS8Br9XktKgLyJ/B4YAi4D4VEkBetDXaDSHHFreiflBjFZKdbq8UqPRaA4lDuExP+VBfxlQCmzrwb5oNBrNAUeXS4xRCKwQkQXEPHUAUEqd1/Ul3YfLm8XKR8/m3ZGTmHLLo7z7lSyePSoWxL31juNpvf33nP/AO2ycN4PyKefwlzuOZ+LyfzH9m09wwbp53OYEcesqFpM/eAwnnDuFX503OhbE/dkzvL2gksqgxV1HFhGd8Ts+//1MPvh8eyKIOz43nSNPGsiwy07GffylfGH5EkHc0UeUcMHYMo4fkEtfq5ro0veo/mAeW+etpWpZNauaw0wbVbxTEDe0YkGnQdyasJ0I4gYzi6h2grgb6gM7BXFbm0JkFme2S8rqKojbMZC7qyCumebFdHl2G8SNJ2jZVjTlIK7HZexREBdIOYibrMPqIK5mXziEx/yUB/0f9WQnNBqN5mDiUC40ktKgr5R6X0RKgKOdQwuS3eA0Go3mUEG6sVziwUhKH2gicimwALgEuBSYLyIX7/oqjUaj6Z3ojNyYuf/R8dm9iBQBb9NmkdyjHN4vmzcGTWBOTSuzz4Q/H3Ulq/1hvnfvaVR/8yG+ct+bbF04k8HHn8/f7ziewz56gpdvepa5tQHemFHB/73wDo2bVlIwdDynf+UYfnbmCPLn/o2FP/sX7yyqYnvQYmCGm8jLv+Lzx97kg6VtJmvjc9M54pSBDL3sVMzjL2NlMJMXF1dSMuwwDj+yhK+MK+PY/j5KQ9uwFr9L9Yfz2TJvLdtX1LDWH6EqZHHuwHxGFHopCNfGKmWtWEDNknXUrqikbm0d1TUBtgYs6iM2fiuKlT+AUEYB1a0WW5tiJmvr62KVspL1/NbmUDs9P7NPQZvJWkEpklNIND07pumnZSd+nqno+XHDtc70/LjJWlzPt+1oynp+msvYIz0/VuEqNT0/rsWnoufDritlddTzZS//wnen53f3hLKXjkMHFYKWdwCMDnJOLYf2z0Wj0XyJ2dsP+d5AqoP+f0VkFvCcs38ZHfyhNRqN5pBAdHIWSqk7ReQiYuW6BHhSKfVqj/ZMo9FoDgBCrIbDoUrK3jtKqVeAV3qwL11St3QVHxt9uP/xK3h44g00WTb3PHIRn59+F9fe/R+qls1h1OkX88Ltx1L62i/4x/f/zWcNQaYVZfCdZ1/HX7WB4tFTueDCo3ngtKGk//cPfPzzV5i9sobqkM2QTA/HTylj4a9n8uGaOiqDFj63wdF5Xg47awiDLjsHY8qFLG40ee7zzXywqJLx4/twgVM0pahlE5HPZ1P1wQIqP15P5Re1rPWHqQpZBGzF6KIMcgNVsGkpgZWfJfT82rX17KgLsD1oJ/T8cFTRmp5PTYvF1qYQmxqDrK9tSej5/oYgLU0hAv4QoeYmsvr4kvT8AozcYsy8opie7/WhvD7stCxaIzGtPFnPN90eZ9uN6fFiuD2YLk9s2+WhoTVMIGwTCFrtiqZYYZuorbCdtfp2vIiKo+UnF03xekw8Znw/1vZEzwfa6flus02/76jnm0bqej50rud3l5affP84Ws/vPRzK8s4udXkR+dB5bBaRpqTWLCJN+6eLGo1Gs/+IZeSm1lK6n8gZIrJKRNaKyN2dPJ8mIi84z88XkYFJz93jHF8lIqd3x/vb5UxfKXWs85i9q/M0Go3mUKK75vlOPZHHgFOJ1QRfKCLTOxQ4vw6oV0oNFZHLgYeAy0RkNHA5cBjQF3hbRIYrpTr/6poiqa7T/3sqxzQajab3E5MLU2kpMBFYq5SqUEqFgeeB8zuccz7wjLP9MnCyxPSl84HnlVIhpdR6YC17WIukM1JddnlY8o6IuICj9vXFNRqN5qAjxcQsZ8wvFJFPktoNHe5WBmxO2t/iHOv0HKd2eCNQkOK1e8wu5R0RuQf4AeBN0vAFCANP7uuLp0o4qvjx63fzG/eJeHiZH7x4K8/1vYC77/o7TVtWc9QlX+PVmyZj//a7PPWrd1nXEubcfjmc9Ng3uerHSyk7+iyuveQI7jq2nODff8Kch97g7U2N+K0oh+ekMXXaAEZ9+2J+dsFDVIdsitJMJuVnMPLCUfS/+HzUxAv4sLKV5z/byIJFlVStqeCByy5hYlk2ubWrCX36NtvmfMLWjzexpaKBtf4wNWGbcFRhCuT5NxNdv5jW5YuoXV5BzYoq6isa2N4QZHuwLSnLdoyrq1pjQdwNDQE21rZSUe2nsi6QCOIGW8OEmpuItDaSMbqAzNIC3AVxk7UiyCpImKzZ7gxawlFaI9G2hCzDxHTHErCSK2W5nABuPEnLH7QIh+1Og7hW2Ma2Y8lZUTuKxwngelwGGR6zXVJWchDX4zLaBXHbgrZtQdzkwGs0aqccxO1s5tVVEDdOqkHcPQ266iBu70WUQnbze5NEjVJqwq5u18mxjhb1XZ2TyrV7zC5n+kqpBx09/1dKqRynZSulCpRS9+zri2s0Gs3BiKhoSi0FtgD9k/b7AZVdneOoKD6gLsVr95jdrd4Z6Wy+JCLjO7Z9fXGNRqM5+FCgoqm13bMQGCYig0TEQywwO73DOdOBq53ti4HZTsGq6cDlzuqeQcAwYh5o+8Tu1ul/F7gB+E0nzyngpH3tgEaj0Rx0dFORQKWUJSI3A7MAE3haKbVcRB4APlFKTQf+AvxdRNYSm+Ff7ly7XEReBFYAFnDTvq7cgd0v2bzBeZy2ry+0L/Q5bBBfqxzDzD89in/Bk9y5ppC/3PUEUSvMGd+6hhcuH0XFLVfwr+eW47eifHViX6b87i4WlhzH0BPmcdfXxvLVcsWOX97GR49/yJyaVgCmFng5+oIRDLn+GhpGnUZ16Of097qZOCCHkReNpc9Fl9Ay/ARmVzTw/CebWbakih3rvsC/fQMnDPCRvvlTWj5+i61zFlG5oJL1W5rYHLCoS9LzfW4T+4v5NC9bTO2y9dSuqqG+ooGt/jDVoVhSVsBu0/M9hlBRF2BTY5ANNS1UVPupqg/Q0hSitTFEqz9EpKWRcGsjVsBPVlkR7sISDKdoCpm5RNN9RNNziJhptIZtWiJRWiOqSz0/2WTNTIvp+i6Pm1DIwgrHi6XYTjJWrIBKsp5vW1aSlm/sUs/3JBuuJen5HROyokmaqts0MITd6vkdJf1U9PzdGaztq/be2eVazz/IUSrVWXyKt1Mz6WBbo5S6L2k7SMzBuLNrfwb8rNs6Q+pLNi8RkWxn+4ci8m8RGdedHdFoNJqDhW7U9A86Ul2yea9SqllEjgVOJ7am9Ime65ZGo9EcKBRErdRaLyTVQT/+Pfls4I9KqdcAT890SaPRaA4giu4M5B50pGq4tlVE/gScAjwkImnsRz/9lbU2S37/J8qnnMPJs4SP//UHskoHcvttF3L3YD/zTj2LlxZUku8x+cYloxj9y4f4V3Uev/jdPB6/aQrHUcHqe37Ouy9/weLGID63wXGFmYz5xkTKrv0WFTmj+NvcjYzKTmPCkcWMuHQieed+jW2+4fx3+Q6eX7CZDSt2UFexjNbaSqJWGM+Kd6ibO5utH65g26fbWVvTSmXQojFiY6uYNu9zG/RNd1M3fz61yzZQt7ae2o2NbA3ECqA3RmLaf7Ken+UyWF3bQsWOFjbWtlBbH6C1KRRbn98SJNxcRyToxwr4scNB3MVDMAtKMfOKE8VSlNdHEBet4SgtkSgBK0pzyEoYqiWvzY/p9972er5jnhYJ2Yn1+DFNXyUKqMQ1fRW1iVrhhJ7v9bjaFUyJ6/imITtp+rB7PV/Zdjs9v+NafWjT840kdXt3en78OkhNz98b/62eXpvf2WtougMF0d45oKdCqgP3pcSiz2copRqAfODOHuuVRqPRHEAOZU0/VT/9VhFZB5zuOL19oJR6s2e7ptFoNAeIXjqgp0Kqq3duBf4JFDvtHyJyS092TKPRaA4ISkHUTq31QlLV9K8DJimlWgBE5CHgI+D3PdUxjUajOVD0VukmFVId9IW2FTw42/sthhRorGfq7V/nzVum4JtyI+VTzuHJ7x7HlC9e5NXJj/P2jhbG+NI5/3vTyPveI9w5ay0vvfw2VcvmMPmsGub//G+89dFWKoMWfdNdnHhkMWNuOImM825gbnMmT85axYL5W3jh1IEMv/xk3Cdcyhd2Pq98upU3Fm5h6+pKGjetJFC/HYC07HyqXp/O1o/WsH3xDlY1x6pk+a3YL4rXFAo9Lsq8LvoUeNk+fw21a+rZsaMlYbDWGIlVyYJYabZ4EDfHZbJ8axMba1poagjS2hSitTlEqMVPpKWxXRDXCgVwlZRj+AoTBmvRtGxaLUVrxKbFihKIRGkMWjSGrJ2CuIkArlM1y+VJwzANXB4Tl9vESjJbs53gbbvELCscC+RGwrEArpOQ1TGIm2imgSGyyypZ8SCuspOSswxjlwlZ8QCuSGoB3OTX210Qd28LKKUSxN2X6kw6gNuTdG9y1sFGqoP+X4H5IhKvi3sBsdRhjUajOfT4sg/6SqmHReQ94Fhik4xrlVKf92THNBqN5oDQzTYMBxu789NPB74NDAWWAo87Jv8ajUZzSCJ8uTX9Z4AI8AFwJjAKuK2nO9WRvv1Kefe0CG+NnMQxt/6O/1x/NPU/+hYPP/4xlcEIXxmWzwl/uImKIy/hq3+cz5JZ7+Ov2oCvfBRvX/sI7273E7CjjM9NZ8oZgxl+/eWEJ1/Cv1bW8PR7S1n32TrqNy5j9K9uwB53NrM3NvHi5+v4ZNE2qtasoalyHVbQj+HykO4rJKffCNbMeJzNGxpY3xJpVzAly2VQkhbT84vLsskflk/lgkoqm0JsD9o0We0LppgCXtMgy2WQ5zbJ9xjMrGzC3xgk0Bwm4A8Ram5IGKzZ4SBWOEA0EiZqhZH8Pthex2DN5aU1HCVgKVoiUVrCNo0hi8ZgBH/YxvSk76znJxmsmaaBy21iuAxcboNwyOrSYC2u5ceTs7wes0uDNdMQPKaB2xAMQ3ZZMAXa6/kqau9Wz0/o8ikK3cl6/q4KpiRL7vuSiXio6fn70PVeggK7d67MSYXdDfqjlVJHAIjIX9gDL2cR6Q88C5QCUeBJpdSjIpIPvAAMBDYAlyql6ve86xqNRtMDxG0YDlF2N4GJxDf2QtaxgDuUUqOAycBNTnX3u4F3lFLDgHecfY1Gozlo+DJn5I7pUBs3XitXAKWUyunqQqXUNmCbs90sIiuJFfU9HzjROe0Z4D3g+3v7BjQajaZ7+RIHcpVSZne8iIgMBMYB84ES5wMBpdQ2ESnu4pobiFXtosyXxUNTbqImbPHO6Yr3jzmRV5btoCTNxS3fGMuQn/6Gpze6ePhns9m04C0AyqecwyVnj2TGOY+R7zE5pTyPI6+dTMmV32KNdzBPvrWOt+ZupHLZIvxVG1BRm23DT2fmou28+PEmNn1RTV3FElprK1FRG1d6FpnF/ckvH0bpwFyWvVrH5kAkoc97DCHfY1KS5qI820Pe4FwKRhSSN7w/H8yqSBisBey2ijxta/MNfI6e78tOo6G6hYA/nDBYC7c2YocCWMEWbEfLj+vhdnYRKi2bgDIJJBmsNQQs/OHY+nx/yKIpZCXp950brLncJi6PkdD2W5pCO63Nj2v4cT0/oem7zZ30/LjJmtswMCVWDMVtyG4N1hLbznG3YexU/DxZzzckNZ254xr+VAzWDiYtf89fv3tf69DX8pM4hAf9HnfKFJEs4BXgNqVU0+7Oj6OUelIpNUEpNaEg09tzHdRoNJpkDnEbhh4d9EXETWzA/6dS6t/O4SoR6eM83wfY0ZN90Gg0mj1DoaxISm1fEJF8EXlLRNY4j3mdnDNWRD4SkeUiskRELkt67m8isl5EFjltbCqv22ODvsS+x/4FWKmUejjpqeTK71cDr/VUHzQajWaPUeyvmX4qi1pagauUUocBZwC/FZHcpOfvVEqNddqiVF40VRuGvWEq8HVgqYjEO/MD4BfAiyJyHbCJLgoCazQazYFAodrFlnqQ3S5qUUqtTtquFJEdQBHQsLcv2mODvlLqQ7rOIzl5T+5VWdlIUW4+N/32Eh6eeAPrWsKc2y+Hk35/LVunfpMzX1jMolkf0rRlNdl9hjB62jH88LzDODmnkSdy0pg6bQCjvn0x6sSreGlVLX/6zyLWLdpI7drPiLQ04krPNt+GBgAAIABJREFUwtdvOL94dx0ff17J9tXraNq2jkhLI2KYZBT0JbvPUIoHljJ0SD4njixmpT+cSMjyuY2EwVppnyzyh+WRP7wveaMGkDZoJJsDM7pMyMpxGeR7TArTXGQUesksyaS5PkCouYlIayPhlsadErKSA5KWN5+WSJTWRIUsm+awRWPQwh+2aQxF8ActGlsjuNOzYslZThC3s4SseEDXdBlOtayuE7ISgdyonaic1VVCVjyY6zKNlBKyktldQlbHqlmd0ZkR254kZO1pAPZABnG7O4ALX7YgLntSOatQRD75//bOPTqOu8rzn1vV3VJLsvWWLFt25PgdEhISxyF4YUISSIYlj80mIYFhmF0yHhYY4ABDEjIEmLOcDcxswmFhAfNmJgMDgRwCBEwS8lgeITiJndixHTt+xy9JlmQ9Wuqurt/+Ub9uVcvdUssPSe2+n3PqVNWvnj+7dfvX3/u794b21xpj1hZ5bVGTWjKIyCqCMrWvhJo/JyJ3Y38pGGNGJnro6RzpK4qilCBmMtJNlzFmZaGDIvIoQYDqWO6azBtZ/+e/Au8xJju16E7gEMEXwVqCXwn/NNG91OgriqKEMeaknbSjtzJXFjomIodFpM2O8gtOahGR2cAvgX80xjwduvdBuzkiIt8BPl7MO01ZcXNFUZTSwGSly4mWk2TCSS0iEgMeBL5vjPnxmGOZWZBCkO5+UzEPLYmRfnNdJf9tyy/5wuY0MR7gEx95A+1338u9G/r5xqd+w6vPPoITiXH2m67j3deu4AOXtFP51PfY8OUHeMdn3kb9zWt4Seby1V9s46k/7OXgS88z2LkPgJrWDhoXncfZr2nhV7/eSu/uF0n0HMb4aaLVtcxq7aCuvYO2hfWsXtbM6oUNvKalmo2+Ie4K9VGXOZUR5tdW0LCkgYbFjdSvOIuaxYuJdqzAbzyLvtSoPhh3hbg7quU3xFxm1VZQ01JNVVOcmrbZDHUfzkmwNjYgK0zPcDqr52eKpQxYTX8wGWj5vUMpBkY83Fg8JyArEnMDTT8UkBXW9rOafqhYSjggyw99+OMhTd+1Gn7UDZKkjer6ktWbJwrICu9H3ZCGnycgK6zxj2WiP8xTreXnY7x7FJskrlg0IOsUkJm9c/rJO6lFRFYC7zPG3AbcDLwJaBSRv7HX/Y2dqXO/iDQT+E43EGREnpCSMPqKoihTh5mMI/fEn2JMN3kmtRhj1gO32e1/A/6twPWXn8hz1egriqKEMUzVlM1pQY2+oihKDpOavVNylITR99oXcuG9W9nx5MMMPLOWR91zuOne53j5yUdJDvbRvPz1rH7LeXz2L5ezpPs5dt3xjzz7o008fTTBx+9/iPteOMgDTz7Dno0v0bf/ZdLJBJW1zdR1nMv85fO48nVzefuKVt70vX8nnUzgxuJUNc5ldvsy5nTUc8HSJt64qJEL586mY3aU6OFto8nVqiI0nlVL07JG6pa2U7dsIdGO5UjbIry6dnrSwT9xzBHirlDtjmr59dVR4k1V1LRUUd1aTbylnuo5DSR+dShb+LyQli+OizguvcOj8/Izen7/SKDlDwwH2wPDKfqHPaLVtaPz8LMF0B2r44/R9yMOXjIVPD+dOy/f+GnSmX07Ispo+hkNP2qLoEfdQMePOoJrNf1i5uaH98cmVxvbBsdr48U42cbq+eNp+SeqvRfS81XLn8Gcwtk7M5GSMPqKoihTh470FUVRyoepm70zLajRVxRFCWEw2TrOZyJq9BVFUcLoSH/6eWX3IaKP/5z2i9/KFeuEF9Z9jYHDu6ldsIKVN1zLZ645h9UVhzn8tX/g19/8I78/MsjRZJo5lRHe+e0/s3PDbo7u2khqsI9odS31Hecyd/nZrL5gLtecO4eVbdXMPvISxk9nHbgtZ7WwdFEDf7GshUvaa1lUX0G8Zzf++o0c27SB82sraJk3KwjIssnVYh3LcectJV3fzjGnis5Bj/3HhqiJBMnV6m11rPpYhOrWKqqaqqhuqaKqpZbqtkbizfVEm1tJ/mRH3uRqGcRxcSIxxHE5ODBCv62M1Z8cdeD2JlIMDKcYSqYZGPZIJtPEKiI5AVjZ4KyoixsRHNchFgqySo8k8iZXyzp006HgrKibN7lapmKWI5LdLtaBm8ENVbYKJ1fLcewy6swsNlKymICscnPgQpk7cSFw5KaS0/0Wp42SMPqKoihTx9QEZ00XavQVRVHGovKOoihKmWDMqUimNmMpCaMfqazm9v/5Ee74iw5qL30/s9oWseqWd3PXdedwZd0AR7//WR775u/53d4+OkfSNFe4vL1tFstvWME/P/izbKGUxsUXMmfpIi4+v41rz2vj0vZZ1B3dzsi6R9j11Hqal19N04JWli5p5LLlLayaV8ei+hg1/a/iP/88g1tfoOuFHXS9dJhlb5yfUyjFbQ+0/D63hs6Ex6vHBtndm2BP9xBzKyPHFUrJaPlBQFYj0cYm3PoW3PpmvMSGCbV8NxoUQznYPxIkWEuk6BsKgrAGRjz6h1NZLd9LpfFSPrF49LhCKZGoc5yWXxFxiMcipJOJCbX8zHtWRJxxtfxMoJZbQHcv9Edm/PSEWj6MFliZ7B9rsVr+ycrcquWXFjp7R1EUpVwwBpNWo68oilIWGGPwU950v8ZpQ42+oihKGIOO9Kebc+fP5kPbv8UTf/cb3vDhL3H3NefwpngXR77zGR795h/4fwcHOJoMtPxr2mez4sZzab/xevwLr8FcfjtNSy+mbelCLrXz8i+eW0Nt11ZGfv0ou55cz4E/72fPKz288d6PZeflL6yroLpvL/7zzzOwJdDyu7d1cnT7UQ4cG+HmL95CxaJzsvPye50qOoc89h8bZG9fgp2dg+zpHmR/1xAfm1VxnJafnZff2ITbOAe3vgWq6/HjtXmTq43V8p1IFCcSY2/PUJBYbdijL5HMmZefGgn0/HTax0umqayOjjsvPyhubvdd57hCKfm0/Iz2Wek6BeflOxLMtc8UOA/3bzwtP0PGDzCelg+TLwOXOX86tfwTuf/p0POVXNToK4qilAnGGHzNp68oilI+nMmzd7QwuqIoShg7e6eY5WQQkQYReUREttt1fYHz0iKywS4PhdoXisif7PX/YYuoT4gafUVRlBCZ2TvFLCfJHcBjxpglwGN2Px8JY8wFdrk21P554D57fQ/w3mIeWhLyztEXtnL3h3qIOcJjVxl2ffED/MRWxkqkDR1VUa5c1sjymy+i9YZ30Dt/FT/d2cMP79/I6667PlsZ67zmSqK7/sTAjx5l25MbOfDsIXYe6GdfIsXRZJq7r1qWrYyV+v2z9GzeRPfmXXRv7aZnZy/7hlJ0jngc83zil99Euq6dznSEziGPPb397OtLsMs6cA91DzF4bITBYyPMuaAlpzJWvKWeSH0zTn0LkcY5+FV1+BWz8OO1JJ3gyzpTGUscFycaw7HO3IwD162I40Zi7O9JZCtjDQx7QSBW0rcBWdaR6xl8z6d6dmVOZax4zKXCOnHDDtxMm5dMZJOj5XPgjm4HCdcylbFGq2TlOnAD5+74SdHyBqUVSKw21oFbKMlZIQo5cPPdZbLBVafDgatMHf7UOHKvAy6z298DngBuL+ZCCT68lwPvDF3/GeCrE12rI31FUZQwdspmkfJOk4isDy1rJvGkVmPMQQC7bilwXqW999Micr1tawR6jTGZnxv7gXnFPLQkRvqKoihTxuQicruMMSsLHRSRR4E5eQ7dNYk3WmCMOSAiZwO/FZEXgWN5zjPF3EyNvqIoSgjDqZu9Y4y5stAxETksIm3GmIMi0gYcKXCPA3a9U0SeAF4H/ASoE5GIHe23AweKeaeSMPpJ33DThW1csObN3LtqDa8MJom7wvm1lZz/xvksu/XNRC+7he2mke9sPsTDDz3Nvq2v0rd3Cxt+dCftfjf+pp/T9Z3f8+oft3No4xF2DCQ5MOwx4AX/uXFXWHzoaZJPPMuBF3fQtWkf3dt76O4c5NWER08qTV/KJ+kHX6b7KhdwuDvF7t4Bdh8dYmfnIPuPDtHXO8zgsWES/UkS/f2kBvtou2QxVS31VDQ1ZAOxnNom/HgtXuUs/MpahjzDUNIn4XlWu48hrosb0vGdaIxILB5o+rE4TjTGnq5BRkJJ1bzQdtrzSad9fLuurI4Sy9HyR3X8TKK1WGjxU8kc3T7zhxBuA/D9NJURG5Bltfyo4+To+GFdv9hkaxncjHY/gZZ/srr72MtPdZI01fFLBGPwk1OShuEh4D3APXb9s7En2Bk9Q8aYERFpAlYDXzDGGBF5HLgR+GGh6/Ohmr6iKEoYA77vF7WcJPcAbxGR7cBb7D4islJEvmnPWQGsF5GNwOPAPcaYl+yx24GPisgOAo3/W8U8tCRG+oqiKFOFYWqybBpjuoEr8rSvB26z238Azitw/U5g1WSfq0ZfURQljCGnjvOZRkkY/bZzzmLhukf4yoYDxHiAWy5qY8XNK2m+4V0caT6Pn7zSww8e3MsrmzfS9cpmBjv34XtJ3Ficuh9/jm2/e5GDzx1ix8FBDgwHc/LTBmKO0Fzh0loRYUFVlO1f/DJdW7vp2d3HqwkvOyc/kfZJW794zBHirvDL7V3sPBLMye/qSTDQO8zQQJLhwSTJ/qMkh/rwEgOkk8M0XnJRtkCKX1WHX1lLunIWSSfGYMpnaNAjkTL0jaToH0kTjddk5+S7FVbDD+n4biyeLYRyrG8k75z8TKI14xvSnofvJWmsiWXn5Mejbo6O7zqSo+dHnSDhGhw/Jx8CHT+DSaepiDh55+SH98OFUML3Go+giMrxSdXy6fgnkoes2Dn5k40BmOgZykzGaBqGE0FEvi0iR0RkU6itqLBjRVGUaWNy8/RLjtPpyP0ucPWYtmLDjhVFUaYFYwzppFfUUoqcNqNvjHkKODqm+TqCcGHs+noURVFmFMZKmhMvpchUa/o5YcciUijsGBvOvAZgQVvrFL2eoihlj1bOmh6MMWuBtQDV85aaS9Z8i779LzPwzFp656/ikZ09/PCJfWzb/BidO15i8Mg+0skEbixOdfN8Zrcvo3VBHT/+5AezCdXSJgj0qY1mnLcRGs+qpWlZI3VL2/nZvzxe0HlbExGqXYeGmEtDzOXrf9iTTaiWz3nrjSTwvSC4Kfqav8KvrCVlE6oNpnyGhn0SqRT9SY++YY++EY+BpEf/iEespj6bUC2f89Z1HSIxl0jUYaAvkXXeptNBQJaf9rPOW5NOZ9+joaYiJ6Ha2MW1ydIyla98LxX8XxRw3ma3w8FZBZy34aRpEzlwxx7PBGeN57w9kZ+sYQfrmeS81cJaJ4kBky4qo0FJMtVGv6iwY0VRlOnCYKYqy+a0MNURuZmwY5hE2LCiKMqUYcD4pqilFDltI30R+QFBrugmEdkPfJogzPhHIvJeYC9w0+l6vqIoyolgDKSTGpw1aYwxtxY4dFzY8UQkenuI9R9l3kVXcMU6Yc+Wh+nd/SJD3QcCzby6lllzF9GwYBFzOuq4dGkzq89u5LyWav7Xp4ZtEFaEuZUR5tXEaFhST8PiRhpWnEXNksXEOpbjN3Ww8VO/yj4z7gpx12F2xKE26tJc4TKrtoKqxjg1rdXs3nyA1GBfjo6fTiWz+nlYl+6vX8RgypBI+Aylkjka/sCIx7ERj74hWwhlxCNePycblBWJukRiGR3fFkCJujgRh0jU4cjevlEt3z47kyjN+IGe79vtllkVo/q9DcaKOg5RV7J6vuPYtU2MNp6OH6Yq6uYkRAvr+KO6uxTUm8fT+UVktIhK6HpnzDmT5biEa+Pc41QnX3NOsfCuOv4pxBjV9BVFUcoJX42+oihKmaBTNhVFUcoHA/gl6qQtBjX6iqIoYYxRR+50M2deKz/9xoc5v7WK2kvfjxuLE69vZe5FV9G6oI7XLm3ijYubuHjebDpmR4ke3kbq5V/Q/+tNXNVaTeNZtTQsrqdhxQLqli0k2rEcaVtEuq6dnnSEziGPPT3D1EadnACs+uoo8aYqalqqqG6tJt5ST1VzHVVtjfR8Y2NOANZYR6Q4bnbZ2j1M33DguO0bCQKw+oZSDAwH2wPD1ok77OGl0tQ0NeUEYDmhoCw3IoFz11bA2rN5f04AVmZJZ/bTo9kxm2dXHBeAFXUDp23UyVS9Gt1Op5LZ/kxU7SrqODkBWOGMmjntBa4fD9d6bMdz3J6oo7WQ81Ydt+WL0eAsRVGUMkKNvqIoSjmhEbmKoijlwxRF5BZTX0RE3iwiG0LLsIhcb499V0R2hY5dUMxzS2Kk35LopOLDt/DbZw/xho/+n5zgq7bIMO7BLSS3Ps7Rn29l+9Z9dG3tpvvAAK8mPNb8+4eywVde3Tw6hzw6Bz129wyxZ9do9auenmE+1VGXDb6qaqmhqqWeqjkNxJsbcOpbiDTOwalrxo/XMvwvn895x7CG70SDSldOJIoTifGHvT05wVcZDX/IavheysdLprPVrmobq7LBV5kka5mgqoqIQzwWCfZdh6f7j2aDrzIafrjKVbAEo5aGymhO8JU7ZtsRAs3fHQ3OypBPgw+3Rdzc4CtHRvX7cNBWoXuNh0Ou9n5cUNWk7ha6bpx7HnfuJO99qjX8MKrnn14MUzZPP1Nf5B4RucPu357zLsY8DlwAwZcEsAP4TeiUfzDGPDCZh5aE0VcURZkyjMGfmtk71xGkqoGgvsgTjDH6Y7gR+JUxZuhkHqryjqIoSghjgpF+MctJklNfBChYX8RyC/CDMW2fE5EXROQ+Eako5qE60lcURRnDJKpiNYnI+tD+WlsLBAAReRSYk+e6uybzPjYV/XnAulDzncAhIEZQe+R24J8muldJGP1X9/fy9f0vE3eFx64yjGx5mKM/3Eb3lv28srWbzkMDHBpO05X0GPB8EqFv4E2vfSe7ehPs2TbEzs5t7OkapK93mMFjwyT6kwwPDmUTp6380BXEW5tx65tx61uy+r0fr8WvmMWALwymDEMpHycSy6vfO9EYkViQLM2JxHAr4jy+5UhB/d5LBsVPwkVQVlw4l1jEoSrmEou4Wf0+o+mHC58kB/uA4/X7sK4PQQGU+ng0R7+POk622Em+4icTafphYk6mwEmufp/5KZmvAEqxuKGLxl5+MvPpC12rknmZYyY1iu8yxqwsfCtzZaFjIjKZ+iI3Aw8aY1Khex+0myMi8h3g48W8sMo7iqIoYew8/WKWk2Qy9UVuZYy0Y78okGBEdT2wqZiHlsRIX1EUZaowTFnCtbz1RURkJfA+Y8xtdr8DmA88Oeb6+0WkmeDH6QbgfcU8VI2+oihKGGNIJ0+/0TfGdJOnvogxZj1wW2h/NzAvz3mXn8hz1egriqKEMAZ8o2kYppXm2RV84r+/gYYVHdy7ag09qTQDnk/SRsS5EjgSayIOcyujNMQcmisiVDXEue3//pGh/hFGBgcCh+1gH97wIL6XxBtJZKtLAcz6q3tIV85mIOUzmPJJeD6JlE/fUY++kX4GRmzFqxGPWXMXWQduDDcWtw7cilBVKzebHG3vzh7S1lHrJdMYY7KVrsZWuTJ+mnPnrcipbpVdQknSoo6DK+ANDwK5DlvIX+WqPh7N67AdmxitmCCq4xOuZe6R67AtVOlqMgj5na4nUi1r7H2LRROmlRdpNfqKoijlgQHO4HxravQVRVHGoiN9RVGUMsE3ZKXjM5GSMPr+grN5+q+/wK6jQ8R4gEXVURpiLrMaq6hqilPdWk11yyyq5jRS1VJPrLEBt7ENt76Zbbf9NKvZh8kmR7MBVG4kxgO7kvSNHAq0e5sgrS+RIpH06B/2SIQCrOYsOyer2WcKnjiu3bcFTjLBVE+teyFHs8+XIC0cTLW8bVZWs4+4wTrQ8ke3M8nSMn6JseRrm10RydHsMwnSxhY4yejXk0mMFnGlYJGTky1I4o65wakucBLcUzV7ZRSVdxRFUcoEg1F5R1EUpVxQR66iKEqZoUZ/mtm+5zB/+/f/Gz+VZOCZtVBdHyRBq5xNKhJnKOWT8Ay9KZ9Xk2n6Rjz6hlMMJNPE61uPS4TmVgTrSCwa6PF2bv19D71kk6HlJkDz0z5pzwv0eDuv/qprLszOnR+bBC07x951iDrCw9/flZMILayV55tXv6ShetxEaOFiJelkoqh/Q+OnqYkFqvvYJGiQf179ZIgVobuf6Lz6cEGWU8lkdHzV6MsHY3T2jqIoStlg0Nk7iqIoZYNq+oqiKGWGyjuKoihlQqDpT/dbnD5Kwui7sUpazlmNG3G4Yp3gJY/ipTptoFSatGfwPT9bjcr4hrTn4XtJ3vrO/2ydqy7xqJtTfWpsQrNPffq7oSApP2/1qQy3XnQtjnCcozWf43W4ryt7XTEBTwtqg1KXxVSfmkwAVXU0uFM+n+TJBjxF3dwbnEq/p3uavKjqnFUKoSN9RVGUMsEAU1JCZZpQo68oihLCYHT2jqIoSrkQzN5Roz+tnHtWA7//0tsBqL30/ZO69rtfu6nocz/aua/oc1fPn1X0ufkSvo1HS/Xp+W+pip5oGZOJiZyOLGgW1d6VKeUMd+SePiswDiJytYhsE5EdInLHdLyDoihKPjIj/WKWk0FEbhKRzSLi22Lohc7Lay9FZKGI/ElEtovIf4hIrJjnTrnRFxEX+Arwl8A5wK0ics5Uv4eiKEoh0qa45STZBNwAPFXohAns5eeB+4wxS4Ae4L3FPHQ6RvqrgB3GmJ3GmCTwQ+C6aXgPRVGU4/AJ0jAUs5wMxpgtxphtE5yW115KMH/7cuABe973gOuLea6YKXZYiMiNwNXGmNvs/ruBS4wxHxxz3hpgjd09l+Bb8UyhCeia8KzS4UzrD5x5fSqn/pxljGk+0RuLyK/t/YuhEhgO7a81xqyd5POeAD5ujFmf51heewl8BnjaGLPYts8HfmWMOXei502HIzefW+64bx77D7cWQETWG2MKal6lhvZn5nOm9Un7UzzGmKtP1b1E5FFgTp5DdxljflbMLfK0mXHaJ2Q6jP5+YH5ovx04MA3voSiKcloxxlx5krcoZC+7gDoRiRhjPCZhR6dD0/8zsMR6nmPALcBD0/AeiqIoM5289tIEuvzjwI32vPcAxfxymHqjb7+VPgisA7YAPzLGbJ7gsklpZCWA9mfmc6b1SfszwxCR/yIi+4FLgV+KyDrbPldEHoYJ7eXtwEdFZAfQCHyrqOdOtSNXURRFmT6mJThLURRFmR7U6CuKopQRM9rol2q6BhH5togcEZFNobYGEXnEhkw/IiL1tl1E5Eu2jy+IyIXT9+b5EZH5IvK4iGyxYeMftu0l2ScRqRSRZ0Rko+3PZ2173rB2Eamw+zvs8Y7pfP9CiIgrIs+LyC/sfqn3Z7eIvCgiG0RkvW0ryc/cTGLGGv0ST9fwXWDsXN87gMdsyPRjdh+C/i2xyxrgq1P0jpPBAz5mjFkBvB74gP2/KNU+jQCXG2POBy4ArhaR11M4rP29QI8NhLnPnjcT+TCBsy9DqfcH4M3GmAtCc/JL9TM3czDGzMiFwKO9LrR/J3DndL/XJN6/A9gU2t8GtNntNmCb3f46cGu+82bqQjA17C1nQp+AKuA5gijHLiBi27OfP4KZE5fa7Yg9T6b73cf0o53ACF4O/IIgeKdk+2PfbTfQNKat5D9z073M2JE+MA8I5zreb9tKlVZjzEEAu26x7SXVTysFvA74EyXcJyuFbACOAI8ArwC9JpgiB7nvnO2PPd5HMEVuJvFF4BOMFn1qpLT7A0GE6W9E5FmblgVK+DM3U5jJ+fRPOMy4xCiZfopIDfAT4CPGmGNSONH9jO+TMSYNXCAidcCDwIp8p9n1jO6PiLwdOGKMeVZELss05zm1JPoTYrUx5oCItACPiMjWcc4tlT5NOzN5pH+mpWs4LCJtAHZ9xLaXRD9FJEpg8O83xvzUNpd0nwCMMb3AEwS+ijoRyQyEwu+c7Y89Xgscndo3HZfVwLUispsgC+PlBCP/Uu0PAMaYA3Z9hOCLeRVnwGduupnJRv9MS9fwEEGoNOSGTD8E/LWdffB6oC/z83WmIMGQ/lvAFmPMvaFDJdknEWm2I3xEJA5cSeAALRTWHu7njcBvjRWOZwLGmDuNMe3GmA6Cv5PfGmPeRYn2B0BEqkVkVmYbeCtBpt2S/MzNKKbbqTDeArwNeJlAb71rut9nEu/9A+AgkCIYgbyXQDN9DNhu1w32XCGYpfQK8CKwcrrfP09//hPBT+UXgA12eVup9gl4LfC87c8m4G7bfjbwDLAD+DFQYdsr7f4Oe/zs6e7DOH27DPhFqffHvvtGu2zO/P2X6mduJi2ahkFRFKWMmMnyjqIoinKKUaOvKIpSRqjRVxRFKSPU6CuKopQRavQVRVHKCDX6yrQjImmbSXGzzXz5URE54c+miHwytN0hoWynilLuqNFXZgIJE2RSfA1BIre3AZ8+ift9cuJTFKU8UaOvzChMEHK/Bvigja50ReSfReTPNk/63wGIyGUi8pSIPCgiL4nI10TEEZF7gLj95XC/va0rIt+wvyR+Y6NwFaUsUaOvzDiMMTsJPpstBNHMfcaYi4GLgb8VkYX21FXAx4DzgEXADcaYOxj95fAue94S4Cv2l0Qv8F+nrjeKMrNQo6/MVDJZE99KkFNlA0E650YCIw7wjDFmpwkyZv6AIF1EPnYZYzbY7WcJah0oSlkyk1MrK2WKiJwNpAkyKArw98aYdWPOuYzjU+cWyikyEtpOAyrvKGWLjvSVGYWINANfA75sgsRQ64D/YVM7IyJLbdZFgFU2C6sDvAP4nW1PZc5XFCUXHekrM4G4lW+iBPV4/xXIpHD+JoEc85xN8dwJXG+P/RG4h0DTf4og5zrAWuAFEXkOuGsqOqAopYJm2VRKEivvfNwY8/bpfhdFKSVU3lEURSkjdKSvKIpSRuhIX1EUpYxQo68oilJGqNFXFEUpI9ToK4qilBFq9BVFUcqI/w+WeEyJ0TjpAAAAAUlEQVRf0+wxjQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pos_encoding = positional_encoding(50, 512)\n",
    "print (pos_encoding.shape)\n",
    "\n",
    "plt.pcolormesh(pos_encoding[0], cmap='RdBu')\n",
    "plt.xlabel('Depth')\n",
    "plt.xlim((0, 512))\n",
    "plt.ylabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U2i8-e1s8ti9"
   },
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "  \n",
    "  # add extra dimensions to add the padding\n",
    "  # to the attention logits.\n",
    "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A7BYeBCNvi7n"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 1, 1, 5), dtype=float32, numpy=\n",
       "array([[[[0., 0., 1., 1., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., 1., 1.]]],\n",
       "\n",
       "\n",
       "       [[[1., 1., 1., 0., 0.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.constant([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]])\n",
    "create_padding_mask(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dVxS8OPI9uI0"
   },
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "  return mask  # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yxKGuXxaBeeE"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[0., 1., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.random.uniform((1, 3))\n",
    "temp = create_look_ahead_mask(x.shape[1])\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LazzUq3bJ5SH"
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "  \"\"\"Calculate the attention weights.\n",
    "  q, k, v must have matching leading dimensions.\n",
    "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "  The mask has different shapes depending on its type(padding or look ahead) \n",
    "  but it must be broadcastable for addition.\n",
    "  \n",
    "  Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable \n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "    \n",
    "  Returns:\n",
    "    output, attention_weights\n",
    "  \"\"\"\n",
    "\n",
    "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "  \n",
    "  # scale matmul_qk\n",
    "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "  # add the mask to the scaled tensor.\n",
    "  if mask is not None:\n",
    "    scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "  # add up to 1.\n",
    "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "  return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FiqETnhCkoXh"
   },
   "source": [
    "As the softmax normalization is done on K, its values decide the amount of importance given to Q.\n",
    "\n",
    "The output represents the multiplication of the attention weights and the V (value) vector. This ensures that the words you want to focus on are kept as-is and the irrelevant words are flushed out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n90YjClyInFy"
   },
   "outputs": [],
   "source": [
    "def print_out(q, k, v):\n",
    "  temp_out, temp_attn = scaled_dot_product_attention(\n",
    "      q, k, v, None)\n",
    "  print ('Attention weights are:')\n",
    "  print (temp_attn)\n",
    "  print ('Output is:')\n",
    "  print (temp_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yAzUAf2DPlNt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tf.Tensor([[0. 1. 0. 0.]], shape=(1, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor([[10.  0.]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "temp_k = tf.constant([[10,0,0],\n",
    "                      [0,10,0],\n",
    "                      [0,0,10],\n",
    "                      [0,0,10]], dtype=tf.float32)  # (4, 3)\n",
    "\n",
    "temp_v = tf.constant([[   1,0],\n",
    "                      [  10,0],\n",
    "                      [ 100,5],\n",
    "                      [1000,6]], dtype=tf.float32)  # (4, 2)\n",
    "\n",
    "# This `query` aligns with the second `key`,\n",
    "# so the second `value` is returned.\n",
    "temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zg6k-fGhgXra"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tf.Tensor([[0.  0.  0.5 0.5]], shape=(1, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor([[550.    5.5]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# This query aligns with a repeated key (third and fourth), \n",
    "# so all associated values get averaged.\n",
    "temp_q = tf.constant([[0, 0, 10]], dtype=tf.float32)  # (1, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UAq3YOzUgXhb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tf.Tensor([[0.5 0.5 0.  0. ]], shape=(1, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor([[5.5 0. ]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# This query aligns equally with the first and second key, \n",
    "# so their values get averaged.\n",
    "temp_q = tf.constant([[10, 10, 0]], dtype=tf.float32)  # (1, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aOz-4_XIhaTP"
   },
   "source": [
    "Pass all the queries together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6dlU8Tm-hYrF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tf.Tensor(\n",
      "[[0.  0.  0.5 0.5]\n",
      " [0.  1.  0.  0. ]\n",
      " [0.5 0.5 0.  0. ]], shape=(3, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor(\n",
      "[[550.    5.5]\n",
      " [ 10.    0. ]\n",
      " [  5.5   0. ]], shape=(3, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "temp_q = tf.constant([[0, 0, 10], [0, 10, 0], [10, 10, 0]], dtype=tf.float32)  # (3, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BSV3PPKsYecw"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "    \n",
    "    assert d_model % self.num_heads == 0\n",
    "    \n",
    "    self.depth = d_model // self.num_heads\n",
    "    \n",
    "    self.wq = tf.keras.layers.Dense(d_model)\n",
    "    self.wk = tf.keras.layers.Dense(d_model)\n",
    "    self.wv = tf.keras.layers.Dense(d_model)\n",
    "    \n",
    "    self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "  def split_heads(self, x, batch_size):\n",
    "    \"\"\"Split the last dimension into (num_heads, depth).\n",
    "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "    \"\"\"\n",
    "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "  def call(self, v, k, q, mask):\n",
    "    batch_size = tf.shape(q)[0]\n",
    "    \n",
    "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "    \n",
    "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "    \n",
    "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "        q, k, v, mask)\n",
    "    \n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "    concat_attention = tf.reshape(scaled_attention, \n",
    "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "        \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0D8FJue5lDyZ"
   },
   "source": [
    "Create a `MultiHeadAttention` layer to try out. At each location in the sequence, `y`, the `MultiHeadAttention` runs all 8 attention heads across all other locations in the sequence, returning a new vector of the same length at each location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hu94p-_-2_BX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([1, 60, 512]), TensorShape([1, 8, 60, 60]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
    "y = tf.random.uniform((1, 60, 512))  # (batch_size, encoder_sequence, d_model)\n",
    "out, attn = temp_mha(y, k=y, q=y, mask=None)\n",
    "out.shape, attn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RdDqGayx67vv"
   },
   "source": [
    "## Point wise feed forward network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gBqzJXGfHK3X"
   },
   "source": [
    "Point wise feed forward network consists of two fully-connected layers with a ReLU activation in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ET7xLt0yCT6Z"
   },
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "  return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mytb1lPyOHLB"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 50, 512])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_ffn = point_wise_feed_forward_network(512, 2048)\n",
    "sample_ffn(tf.random.uniform((64, 50, 512))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ncyS-Ms3i2x_"
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "    super(EncoderLayer, self).__init__()\n",
    "\n",
    "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "  def call(self, x, training, mask):\n",
    "\n",
    "    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "    attn_output = self.dropout1(attn_output, training=training)\n",
    "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "    \n",
    "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "    ffn_output = self.dropout2(ffn_output, training=training)\n",
    "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "    \n",
    "    return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AzZRXdO0mI48"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 43, 512])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_encoder_layer = EncoderLayer(512, 8, 2048)\n",
    "\n",
    "sample_encoder_layer_output = sample_encoder_layer(\n",
    "    tf.random.uniform((64, 43, 512)), False, None)\n",
    "\n",
    "sample_encoder_layer_output.shape  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9SoX0-vd1hue"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "    super(DecoderLayer, self).__init__()\n",
    "\n",
    "    self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "    self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    " \n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    \n",
    "  def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "    # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "    attn1 = self.dropout1(attn1, training=training)\n",
    "    out1 = self.layernorm1(attn1 + x)\n",
    "    \n",
    "    attn2, attn_weights_block2 = self.mha2(\n",
    "        enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "    attn2 = self.dropout2(attn2, training=training)\n",
    "    out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "    \n",
    "    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "    ffn_output = self.dropout3(ffn_output, training=training)\n",
    "    out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "    \n",
    "    return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ne2Bqx8k71l0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 50, 512])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder_layer = DecoderLayer(512, 8, 2048)\n",
    "\n",
    "sample_decoder_layer_output, _, _ = sample_decoder_layer(\n",
    "    tf.random.uniform((64, 50, 512)), sample_encoder_layer_output, \n",
    "    False, None, None)\n",
    "\n",
    "sample_decoder_layer_output.shape  # (batch_size, target_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jpEox7gJ8FCI"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "    super(Encoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "    \n",
    "    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "    self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
    "                                            self.d_model)\n",
    "    \n",
    "    \n",
    "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                       for _ in range(num_layers)]\n",
    "  \n",
    "    self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "  def call(self, x, training, mask):\n",
    "\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    \n",
    "    # adding embedding and position encoding.\n",
    "    x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "    x = self.dropout(x, training=training)\n",
    "    \n",
    "    for i in range(self.num_layers):\n",
    "      x = self.enc_layers[i](x, training, mask)\n",
    "    \n",
    "    return x  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8QG9nueFQKXx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 62, 512)\n"
     ]
    }
   ],
   "source": [
    "sample_encoder = Encoder(num_layers=2, d_model=512, num_heads=8, \n",
    "                         dff=2048, input_vocab_size=8500,\n",
    "                         maximum_position_encoding=10000)\n",
    "temp_input = tf.random.uniform((64, 62), dtype=tf.int64, minval=0, maxval=200)\n",
    "\n",
    "sample_encoder_output = sample_encoder(temp_input, training=False, mask=None)\n",
    "\n",
    "print (sample_encoder_output.shape)  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d5_d5-PLQXwY"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "    super(Decoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "    \n",
    "    self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "    \n",
    "    self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
    "                       for _ in range(num_layers)]\n",
    "    self.dropout = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "  def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    attention_weights = {}\n",
    "    \n",
    "    x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x += self.pos_encoding[:, :seq_len, :]\n",
    "    \n",
    "    x = self.dropout(x, training=training)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                             look_ahead_mask, padding_mask)\n",
    "      \n",
    "      attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "      attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "    \n",
    "    # x.shape == (batch_size, target_seq_len, d_model)\n",
    "    return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a1jXoAMRZyvu"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 26, 512]), TensorShape([64, 8, 26, 62]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder = Decoder(num_layers=2, d_model=512, num_heads=8, \n",
    "                         dff=2048, target_vocab_size=8000,\n",
    "                         maximum_position_encoding=5000)\n",
    "temp_input = tf.random.uniform((64, 26), dtype=tf.int64, minval=0, maxval=200)\n",
    "\n",
    "output, attn = sample_decoder(temp_input, \n",
    "                              enc_output=sample_encoder_output, \n",
    "                              training=False,\n",
    "                              look_ahead_mask=None, \n",
    "                              padding_mask=None)\n",
    "\n",
    "output.shape, attn['decoder_layer2_block2'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y54xnJnuYgJ7"
   },
   "source": [
    "## Create the Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uERO1y54cOKq"
   },
   "source": [
    "Transformer consists of the encoder, decoder and a final linear layer. The output of the decoder is the input to the linear layer and its output is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PED3bIpOYkBu"
   },
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
    "               target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "    super(Transformer, self).__init__()\n",
    "\n",
    "    self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
    "                           input_vocab_size, pe_input, rate)\n",
    "\n",
    "    self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
    "                           target_vocab_size, pe_target, rate)\n",
    "\n",
    "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "    \n",
    "  def call(self, inp, tar, training, enc_padding_mask, \n",
    "           look_ahead_mask, dec_padding_mask):\n",
    "\n",
    "    enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "    \n",
    "    # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "    dec_output, attention_weights = self.decoder(\n",
    "        tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "    \n",
    "    final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "    \n",
    "    return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tJ4fbQcIkHW1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 36, 8000])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_transformer = Transformer(\n",
    "    num_layers=2, d_model=512, num_heads=8, dff=2048, \n",
    "    input_vocab_size=8500, target_vocab_size=8000, \n",
    "    pe_input=10000, pe_target=6000)\n",
    "\n",
    "temp_input = tf.random.uniform((64, 38), dtype=tf.int64, minval=0, maxval=200)\n",
    "temp_target = tf.random.uniform((64, 36), dtype=tf.int64, minval=0, maxval=200)\n",
    "\n",
    "fn_out, _ = sample_transformer(temp_input, temp_target, training=False, \n",
    "                               enc_padding_mask=None, \n",
    "                               look_ahead_mask=None,\n",
    "                               dec_padding_mask=None)\n",
    "\n",
    "fn_out.shape  # (batch_size, tar_seq_len, target_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wsINyf1VEQLC"
   },
   "source": [
    "## Set hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zVjWCxFNcgbt"
   },
   "source": [
    "To keep this example small and relatively fast, the values for *num_layers, d_model, and dff* have been reduced. \n",
    "\n",
    "The values used in the base model of transformer were; *num_layers=6*, *d_model = 512*, *dff = 2048*. See the [paper](https://arxiv.org/abs/1706.03762) for all the other versions of the transformer.\n",
    "\n",
    "Note: By changing the values below, you can get the model that achieved state of the art on many tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lnJn5SLA2ahP"
   },
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "d_model = 256\n",
    "dff = 1024\n",
    "num_heads = 8\n",
    "\n",
    "input_vocab_size = tokenizer_ru.vocab_size + 2\n",
    "target_vocab_size = tokenizer_en.vocab_size + 2\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xYEGhEOtzn5W"
   },
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GOmWW--yP3zx"
   },
   "source": [
    "Use the Adam optimizer with a custom learning rate scheduler according to the formula in the [paper](https://arxiv.org/abs/1706.03762).\n",
    "\n",
    "$$\\Large{lrate = d_{model}^{-0.5} * min(step{\\_}num^{-0.5}, step{\\_}num * warmup{\\_}steps^{-1.5})}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iYQdOO1axwEI"
   },
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "    \n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "    \n",
    "  def __call__(self, step):\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "    \n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7r4scdulztRx"
   },
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
    "                                     epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f33ZCgvHpPdG"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Train Step')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp_learning_rate_schedule = CustomSchedule(d_model)\n",
    "\n",
    "plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.xlabel(\"Train Step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YgkDE7hzo8r5"
   },
   "source": [
    "## Loss and metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oxGJtoDuYIHL"
   },
   "source": [
    "Since the target sequences are padded, it is important to apply a padding mask when calculating the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MlhsJMm0TW_B"
   },
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "67oqVHiT0Eiu"
   },
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "  \n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "phlyxMnm-Tpx"
   },
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "    name='train_accuracy')\n",
    "\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aeHumfr7zmMa"
   },
   "source": [
    "## Training and checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UiysUa--4tOU"
   },
   "outputs": [],
   "source": [
    "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
    "                          input_vocab_size, target_vocab_size, \n",
    "                          pe_input=input_vocab_size, \n",
    "                          pe_target=target_vocab_size,\n",
    "                          rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZOJUSB1T8GjM"
   },
   "outputs": [],
   "source": [
    "def create_masks(inp, tar):\n",
    "  # Encoder padding mask\n",
    "  enc_padding_mask = create_padding_mask(inp)\n",
    "  \n",
    "  # Used in the 2nd attention block in the decoder.\n",
    "  # This padding mask is used to mask the encoder outputs.\n",
    "  dec_padding_mask = create_padding_mask(inp)\n",
    "  \n",
    "  # Used in the 1st attention block in the decoder.\n",
    "  # It is used to pad and mask future tokens in the input received by \n",
    "  # the decoder.\n",
    "  look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "  dec_target_padding_mask = create_padding_mask(tar)\n",
    "  combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "  \n",
    "  return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fzuf06YZp66w"
   },
   "source": [
    "Create the checkpoint path and the checkpoint manager. This will be used to save checkpoints every `n` epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hNhuYfllndLZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest checkpoint restored!!\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"./checkpoints/train-2\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "  print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Di_Yaa1gf9r"
   },
   "source": [
    "The target is divided into tar_inp and tar_real. tar_inp is passed as an input to the decoder. `tar_real` is that same input shifted by 1: At each location in `tar_input`, `tar_real` contains the  next token that should be predicted.\n",
    "\n",
    "For example, `sentence` = \"SOS A lion in the jungle is sleeping EOS\"\n",
    "\n",
    "`tar_inp` =  \"SOS A lion in the jungle is sleeping\"\n",
    "\n",
    "`tar_real` = \"A lion in the jungle is sleeping EOS\"\n",
    "\n",
    "The transformer is an auto-regressive model: it makes predictions one part at a time, and uses its output so far to decide what to do next. \n",
    "\n",
    "During training this example uses teacher-forcing (like in the [text generation tutorial](./text_generation.ipynb)). Teacher forcing is passing the true output to the next time step regardless of what the model predicts at the current time step.\n",
    "\n",
    "As the transformer predicts each word, *self-attention* allows it to look at the previous words in the input sequence to better predict the next word.\n",
    "\n",
    "To prevent the model from peaking at the expected output the model uses a look-ahead mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LKpoA6q1sJFj"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iJwmp9OE29oj"
   },
   "outputs": [],
   "source": [
    "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
    "# execution. The function specializes to the precise shape of the argument\n",
    "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
    "# batch sizes (the last batch is smaller), use input_signature to specify\n",
    "# more generic shapes.\n",
    "\n",
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "]\n",
    "\n",
    "@tf.function(input_signature=train_step_signature)\n",
    "def train_step(inp, tar):\n",
    "  tar_inp = tar[:, :-1]\n",
    "  tar_real = tar[:, 1:]\n",
    "  \n",
    "  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "  \n",
    "  with tf.GradientTape() as tape:\n",
    "    predictions, _ = transformer(inp, tar_inp, \n",
    "                                 True, \n",
    "                                 enc_padding_mask, \n",
    "                                 combined_mask, \n",
    "                                 dec_padding_mask)\n",
    "    loss = loss_function(tar_real, predictions)\n",
    "\n",
    "  gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "  \n",
    "  train_loss(loss)\n",
    "  train_accuracy(tar_real, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(input_signature=train_step_signature)\n",
    "def test_step(inp, tar):\n",
    "  tar_inp = tar[:, :-1]\n",
    "  tar_real = tar[:, 1:]\n",
    "  \n",
    "  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "  \n",
    "  with tf.GradientTape() as tape:\n",
    "    predictions, _ = transformer(inp, tar_inp, \n",
    "                                 True, \n",
    "                                 enc_padding_mask, \n",
    "                                 combined_mask, \n",
    "                                 dec_padding_mask)\n",
    "#     loss = loss_function(tar_real, predictions)\n",
    "\n",
    "#   gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "#   optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "  \n",
    "#   train_loss(loss)\n",
    "  test_accuracy(tar_real, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qM2PDWGDJ_8V"
   },
   "source": [
    "Portuguese is used as the input language and English is the target language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bbvmaKNiznHZ",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 4.4519 Accuracy 0.0000\n",
      "Epoch 1 Batch 50 Loss 4.1490 Accuracy 0.0072\n",
      "Epoch 1 Batch 100 Loss 4.0486 Accuracy 0.0146\n",
      "Epoch 1 Batch 150 Loss 4.0052 Accuracy 0.0180\n",
      "Epoch 1 Batch 200 Loss 3.9320 Accuracy 0.0226\n",
      "Epoch 1 Batch 250 Loss 3.8297 Accuracy 0.0276\n",
      "Epoch 1 Batch 300 Loss 3.7156 Accuracy 0.0312\n",
      "Epoch 1 Batch 350 Loss 3.6057 Accuracy 0.0341\n",
      "Epoch 1 Batch 400 Loss 3.5050 Accuracy 0.0366\n",
      "Epoch 1 Batch 450 Loss 3.4180 Accuracy 0.0401\n",
      "Epoch 1 Batch 500 Loss 3.3383 Accuracy 0.0439\n",
      "Epoch 1 Batch 550 Loss 3.2670 Accuracy 0.0477\n",
      "Epoch 1 Batch 600 Loss 3.2023 Accuracy 0.0515\n",
      "Epoch 1 Batch 650 Loss 3.1385 Accuracy 0.0550\n",
      "Epoch 1 Batch 700 Loss 3.0779 Accuracy 0.0584\n",
      "Epoch 1 Batch 750 Loss 3.0236 Accuracy 0.0616\n",
      "Epoch 1 Batch 800 Loss 2.9744 Accuracy 0.0647\n",
      "Epoch 1 Batch 850 Loss 2.9299 Accuracy 0.0676\n",
      "Epoch 1 Batch 900 Loss 2.8873 Accuracy 0.0703\n",
      "Epoch 1 Batch 950 Loss 2.8487 Accuracy 0.0729\n",
      "Epoch 1 Batch 1000 Loss 2.8127 Accuracy 0.0754\n",
      "Epoch 1 Batch 1050 Loss 2.7776 Accuracy 0.0776\n",
      "Epoch 1 Batch 1100 Loss 2.7454 Accuracy 0.0796\n",
      "Epoch 1 Batch 1150 Loss 2.7175 Accuracy 0.0817\n",
      "Epoch 1 Batch 1200 Loss 2.6889 Accuracy 0.0836\n",
      "Epoch 1 Batch 1250 Loss 2.6633 Accuracy 0.0854\n",
      "Epoch 1 Batch 1300 Loss 2.6389 Accuracy 0.0871\n",
      "Epoch 1 Batch 1350 Loss 2.6154 Accuracy 0.0887\n",
      "Epoch 1 Batch 1400 Loss 2.5928 Accuracy 0.0902\n",
      "Epoch 1 Batch 1450 Loss 2.5704 Accuracy 0.0916\n",
      "Epoch 1 Batch 1500 Loss 2.5501 Accuracy 0.0930\n",
      "Epoch 1 Loss 2.5487 Accuracy 0.0931\n",
      "Time taken for 1 epoch: 515.5405519008636 secs\n",
      "\n",
      "Epoch 2 Batch 0 Loss 2.1671 Accuracy 0.1411\n",
      "Epoch 2 Batch 50 Loss 1.9420 Accuracy 0.1375\n",
      "Epoch 2 Batch 100 Loss 1.9326 Accuracy 0.1375\n",
      "Epoch 2 Batch 150 Loss 1.9284 Accuracy 0.1381\n",
      "Epoch 2 Batch 200 Loss 1.9196 Accuracy 0.1385\n",
      "Epoch 2 Batch 250 Loss 1.9177 Accuracy 0.1393\n",
      "Epoch 2 Batch 300 Loss 1.9119 Accuracy 0.1399\n",
      "Epoch 2 Batch 350 Loss 1.9029 Accuracy 0.1405\n",
      "Epoch 2 Batch 400 Loss 1.8976 Accuracy 0.1412\n",
      "Epoch 2 Batch 450 Loss 1.8903 Accuracy 0.1421\n",
      "Epoch 2 Batch 500 Loss 1.8798 Accuracy 0.1427\n",
      "Epoch 2 Batch 550 Loss 1.8734 Accuracy 0.1435\n",
      "Epoch 2 Batch 600 Loss 1.8657 Accuracy 0.1441\n",
      "Epoch 2 Batch 650 Loss 1.8568 Accuracy 0.1446\n",
      "Epoch 2 Batch 700 Loss 1.8503 Accuracy 0.1454\n",
      "Epoch 2 Batch 750 Loss 1.8436 Accuracy 0.1462\n",
      "Epoch 2 Batch 800 Loss 1.8367 Accuracy 0.1470\n",
      "Epoch 2 Batch 850 Loss 1.8303 Accuracy 0.1477\n",
      "Epoch 2 Batch 900 Loss 1.8227 Accuracy 0.1483\n",
      "Epoch 2 Batch 950 Loss 1.8165 Accuracy 0.1489\n",
      "Epoch 2 Batch 1000 Loss 1.8102 Accuracy 0.1496\n",
      "Epoch 2 Batch 1050 Loss 1.8047 Accuracy 0.1503\n",
      "Epoch 2 Batch 1100 Loss 1.7982 Accuracy 0.1509\n",
      "Epoch 2 Batch 1150 Loss 1.7915 Accuracy 0.1514\n",
      "Epoch 2 Batch 1200 Loss 1.7857 Accuracy 0.1521\n",
      "Epoch 2 Batch 1250 Loss 1.7791 Accuracy 0.1526\n",
      "Epoch 2 Batch 1300 Loss 1.7723 Accuracy 0.1532\n",
      "Epoch 2 Batch 1350 Loss 1.7664 Accuracy 0.1538\n",
      "Epoch 2 Batch 1400 Loss 1.7613 Accuracy 0.1545\n",
      "Epoch 2 Batch 1450 Loss 1.7560 Accuracy 0.1551\n",
      "Epoch 2 Batch 1500 Loss 1.7502 Accuracy 0.1557\n",
      "Epoch 2 Loss 1.7499 Accuracy 0.1557\n",
      "Time taken for 1 epoch: 465.1382427215576 secs\n",
      "\n",
      "Epoch 3 Batch 0 Loss 1.6040 Accuracy 0.1830\n",
      "Epoch 3 Batch 50 Loss 1.5695 Accuracy 0.1733\n",
      "Epoch 3 Batch 100 Loss 1.5683 Accuracy 0.1740\n",
      "Epoch 3 Batch 150 Loss 1.5565 Accuracy 0.1741\n",
      "Epoch 3 Batch 200 Loss 1.5492 Accuracy 0.1746\n",
      "Epoch 3 Batch 250 Loss 1.5499 Accuracy 0.1752\n",
      "Epoch 3 Batch 300 Loss 1.5443 Accuracy 0.1759\n",
      "Epoch 3 Batch 350 Loss 1.5385 Accuracy 0.1763\n",
      "Epoch 3 Batch 400 Loss 1.5350 Accuracy 0.1771\n",
      "Epoch 3 Batch 450 Loss 1.5313 Accuracy 0.1776\n",
      "Epoch 3 Batch 500 Loss 1.5283 Accuracy 0.1780\n",
      "Epoch 3 Batch 550 Loss 1.5235 Accuracy 0.1786\n",
      "Epoch 3 Batch 600 Loss 1.5198 Accuracy 0.1790\n",
      "Epoch 3 Batch 650 Loss 1.5154 Accuracy 0.1794\n",
      "Epoch 3 Batch 700 Loss 1.5096 Accuracy 0.1798\n",
      "Epoch 3 Batch 750 Loss 1.5060 Accuracy 0.1802\n",
      "Epoch 3 Batch 800 Loss 1.5031 Accuracy 0.1807\n",
      "Epoch 3 Batch 850 Loss 1.4989 Accuracy 0.1811\n",
      "Epoch 3 Batch 900 Loss 1.4953 Accuracy 0.1814\n",
      "Epoch 3 Batch 950 Loss 1.4903 Accuracy 0.1818\n",
      "Epoch 3 Batch 1000 Loss 1.4862 Accuracy 0.1822\n",
      "Epoch 3 Batch 1050 Loss 1.4818 Accuracy 0.1825\n",
      "Epoch 3 Batch 1100 Loss 1.4776 Accuracy 0.1829\n",
      "Epoch 3 Batch 1150 Loss 1.4732 Accuracy 0.1833\n",
      "Epoch 3 Batch 1200 Loss 1.4688 Accuracy 0.1837\n",
      "Epoch 3 Batch 1250 Loss 1.4648 Accuracy 0.1841\n",
      "Epoch 3 Batch 1300 Loss 1.4608 Accuracy 0.1845\n",
      "Epoch 3 Batch 1350 Loss 1.4573 Accuracy 0.1849\n",
      "Epoch 3 Batch 1400 Loss 1.4538 Accuracy 0.1854\n",
      "Epoch 3 Batch 1450 Loss 1.4499 Accuracy 0.1858\n",
      "Epoch 3 Batch 1500 Loss 1.4459 Accuracy 0.1861\n",
      "Epoch 3 Loss 1.4456 Accuracy 0.1861\n",
      "Time taken for 1 epoch: 463.800598859787 secs\n",
      "\n",
      "Epoch 4 Batch 0 Loss 1.3439 Accuracy 0.1975\n",
      "Epoch 4 Batch 50 Loss 1.2969 Accuracy 0.1981\n",
      "Epoch 4 Batch 100 Loss 1.2987 Accuracy 0.1986\n",
      "Epoch 4 Batch 150 Loss 1.3035 Accuracy 0.1993\n",
      "Epoch 4 Batch 200 Loss 1.3010 Accuracy 0.2003\n",
      "Epoch 4 Batch 250 Loss 1.2960 Accuracy 0.2008\n",
      "Epoch 4 Batch 300 Loss 1.2923 Accuracy 0.2010\n",
      "Epoch 4 Batch 350 Loss 1.2879 Accuracy 0.2011\n",
      "Epoch 4 Batch 400 Loss 1.2862 Accuracy 0.2018\n",
      "Epoch 4 Batch 450 Loss 1.2841 Accuracy 0.2024\n",
      "Epoch 4 Batch 500 Loss 1.2802 Accuracy 0.2027\n",
      "Epoch 4 Batch 550 Loss 1.2767 Accuracy 0.2032\n",
      "Epoch 4 Batch 600 Loss 1.2740 Accuracy 0.2036\n",
      "Epoch 4 Batch 650 Loss 1.2706 Accuracy 0.2040\n",
      "Epoch 4 Batch 700 Loss 1.2673 Accuracy 0.2042\n",
      "Epoch 4 Batch 750 Loss 1.2645 Accuracy 0.2046\n",
      "Epoch 4 Batch 800 Loss 1.2612 Accuracy 0.2050\n",
      "Epoch 4 Batch 850 Loss 1.2576 Accuracy 0.2054\n",
      "Epoch 4 Batch 900 Loss 1.2549 Accuracy 0.2057\n",
      "Epoch 4 Batch 950 Loss 1.2520 Accuracy 0.2060\n",
      "Epoch 4 Batch 1000 Loss 1.2500 Accuracy 0.2063\n",
      "Epoch 4 Batch 1050 Loss 1.2463 Accuracy 0.2067\n",
      "Epoch 4 Batch 1100 Loss 1.2431 Accuracy 0.2070\n",
      "Epoch 4 Batch 1150 Loss 1.2401 Accuracy 0.2074\n",
      "Epoch 4 Batch 1200 Loss 1.2370 Accuracy 0.2076\n",
      "Epoch 4 Batch 1250 Loss 1.2334 Accuracy 0.2079\n",
      "Epoch 4 Batch 1300 Loss 1.2310 Accuracy 0.2082\n",
      "Epoch 4 Batch 1350 Loss 1.2282 Accuracy 0.2084\n",
      "Epoch 4 Batch 1400 Loss 1.2256 Accuracy 0.2087\n",
      "Epoch 4 Batch 1450 Loss 1.2226 Accuracy 0.2089\n",
      "Epoch 4 Batch 1500 Loss 1.2198 Accuracy 0.2092\n",
      "Epoch 4 Loss 1.2199 Accuracy 0.2093\n",
      "Time taken for 1 epoch: 456.8824300765991 secs\n",
      "\n",
      "Epoch 5 Batch 0 Loss 1.1253 Accuracy 0.2100\n",
      "Epoch 5 Batch 50 Loss 1.1292 Accuracy 0.2187\n",
      "Epoch 5 Batch 100 Loss 1.1223 Accuracy 0.2188\n",
      "Epoch 5 Batch 150 Loss 1.1236 Accuracy 0.2191\n",
      "Epoch 5 Batch 200 Loss 1.1267 Accuracy 0.2198\n",
      "Epoch 5 Batch 250 Loss 1.1272 Accuracy 0.2203\n",
      "Epoch 5 Batch 300 Loss 1.1254 Accuracy 0.2203\n",
      "Epoch 5 Batch 350 Loss 1.1220 Accuracy 0.2204\n",
      "Epoch 5 Batch 400 Loss 1.1193 Accuracy 0.2206\n",
      "Epoch 5 Batch 450 Loss 1.1163 Accuracy 0.2205\n",
      "Epoch 5 Batch 500 Loss 1.1134 Accuracy 0.2208\n",
      "Epoch 5 Batch 550 Loss 1.1099 Accuracy 0.2212\n",
      "Epoch 5 Batch 600 Loss 1.1066 Accuracy 0.2210\n",
      "Epoch 5 Batch 650 Loss 1.1061 Accuracy 0.2214\n",
      "Epoch 5 Batch 700 Loss 1.1040 Accuracy 0.2215\n",
      "Epoch 5 Batch 750 Loss 1.1022 Accuracy 0.2217\n",
      "Epoch 5 Batch 800 Loss 1.1006 Accuracy 0.2220\n",
      "Epoch 5 Batch 850 Loss 1.0996 Accuracy 0.2223\n",
      "Epoch 5 Batch 900 Loss 1.0982 Accuracy 0.2226\n",
      "Epoch 5 Batch 950 Loss 1.0963 Accuracy 0.2228\n",
      "Epoch 5 Batch 1000 Loss 1.0944 Accuracy 0.2230\n",
      "Epoch 5 Batch 1050 Loss 1.0927 Accuracy 0.2232\n",
      "Epoch 5 Batch 1100 Loss 1.0906 Accuracy 0.2233\n",
      "Epoch 5 Batch 1150 Loss 1.0889 Accuracy 0.2235\n",
      "Epoch 5 Batch 1200 Loss 1.0871 Accuracy 0.2237\n",
      "Epoch 5 Batch 1250 Loss 1.0850 Accuracy 0.2238\n",
      "Epoch 5 Batch 1300 Loss 1.0832 Accuracy 0.2240\n",
      "Epoch 5 Batch 1350 Loss 1.0822 Accuracy 0.2243\n",
      "Epoch 5 Batch 1400 Loss 1.0810 Accuracy 0.2245\n",
      "Epoch 5 Batch 1450 Loss 1.0797 Accuracy 0.2247\n",
      "Epoch 5 Batch 1500 Loss 1.0782 Accuracy 0.2248\n",
      "Saving checkpoint for epoch 5 at ./checkpoints/train-2/ckpt-1\n",
      "Epoch 5 Loss 1.0782 Accuracy 0.2248\n",
      "Time taken for 1 epoch: 463.27598762512207 secs\n",
      "\n",
      "Epoch 6 Batch 0 Loss 1.1267 Accuracy 0.2201\n",
      "Epoch 6 Batch 50 Loss 1.0301 Accuracy 0.2316\n",
      "Epoch 6 Batch 100 Loss 1.0244 Accuracy 0.2322\n",
      "Epoch 6 Batch 150 Loss 1.0209 Accuracy 0.2322\n",
      "Epoch 6 Batch 200 Loss 1.0204 Accuracy 0.2324\n",
      "Epoch 6 Batch 250 Loss 1.0179 Accuracy 0.2328\n",
      "Epoch 6 Batch 300 Loss 1.0173 Accuracy 0.2328\n",
      "Epoch 6 Batch 350 Loss 1.0151 Accuracy 0.2325\n",
      "Epoch 6 Batch 400 Loss 1.0122 Accuracy 0.2325\n",
      "Epoch 6 Batch 450 Loss 1.0116 Accuracy 0.2326\n",
      "Epoch 6 Batch 500 Loss 1.0107 Accuracy 0.2328\n",
      "Epoch 6 Batch 550 Loss 1.0078 Accuracy 0.2327\n",
      "Epoch 6 Batch 600 Loss 1.0054 Accuracy 0.2326\n",
      "Epoch 6 Batch 650 Loss 1.0051 Accuracy 0.2328\n",
      "Epoch 6 Batch 700 Loss 1.0051 Accuracy 0.2330\n",
      "Epoch 6 Batch 750 Loss 1.0040 Accuracy 0.2332\n",
      "Epoch 6 Batch 800 Loss 1.0029 Accuracy 0.2334\n",
      "Epoch 6 Batch 850 Loss 1.0023 Accuracy 0.2337\n",
      "Epoch 6 Batch 900 Loss 1.0012 Accuracy 0.2339\n",
      "Epoch 6 Batch 950 Loss 0.9998 Accuracy 0.2339\n",
      "Epoch 6 Batch 1000 Loss 0.9995 Accuracy 0.2344\n",
      "Epoch 6 Batch 1050 Loss 0.9979 Accuracy 0.2345\n",
      "Epoch 6 Batch 1100 Loss 0.9968 Accuracy 0.2346\n",
      "Epoch 6 Batch 1150 Loss 0.9960 Accuracy 0.2348\n",
      "Epoch 6 Batch 1200 Loss 0.9948 Accuracy 0.2349\n",
      "Epoch 6 Batch 1250 Loss 0.9938 Accuracy 0.2349\n",
      "Epoch 6 Batch 1300 Loss 0.9919 Accuracy 0.2350\n",
      "Epoch 6 Batch 1350 Loss 0.9907 Accuracy 0.2351\n",
      "Epoch 6 Batch 1400 Loss 0.9899 Accuracy 0.2353\n",
      "Epoch 6 Batch 1450 Loss 0.9888 Accuracy 0.2354\n",
      "Epoch 6 Batch 1500 Loss 0.9878 Accuracy 0.2355\n",
      "Epoch 6 Loss 0.9878 Accuracy 0.2354\n",
      "Time taken for 1 epoch: 460.69734144210815 secs\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.9182 Accuracy 0.2379\n",
      "Epoch 7 Batch 50 Loss 0.9300 Accuracy 0.2373\n",
      "Epoch 7 Batch 100 Loss 0.9443 Accuracy 0.2398\n",
      "Epoch 7 Batch 150 Loss 0.9382 Accuracy 0.2405\n",
      "Epoch 7 Batch 200 Loss 0.9421 Accuracy 0.2409\n",
      "Epoch 7 Batch 250 Loss 0.9427 Accuracy 0.2408\n",
      "Epoch 7 Batch 300 Loss 0.9429 Accuracy 0.2411\n",
      "Epoch 7 Batch 350 Loss 0.9419 Accuracy 0.2409\n",
      "Epoch 7 Batch 400 Loss 0.9402 Accuracy 0.2410\n",
      "Epoch 7 Batch 450 Loss 0.9408 Accuracy 0.2413\n",
      "Epoch 7 Batch 500 Loss 0.9386 Accuracy 0.2411\n",
      "Epoch 7 Batch 550 Loss 0.9387 Accuracy 0.2414\n",
      "Epoch 7 Batch 600 Loss 0.9376 Accuracy 0.2413\n",
      "Epoch 7 Batch 650 Loss 0.9369 Accuracy 0.2415\n",
      "Epoch 7 Batch 700 Loss 0.9370 Accuracy 0.2417\n",
      "Epoch 7 Batch 750 Loss 0.9361 Accuracy 0.2418\n",
      "Epoch 7 Batch 800 Loss 0.9350 Accuracy 0.2419\n",
      "Epoch 7 Batch 850 Loss 0.9348 Accuracy 0.2423\n",
      "Epoch 7 Batch 900 Loss 0.9338 Accuracy 0.2425\n",
      "Epoch 7 Batch 950 Loss 0.9328 Accuracy 0.2425\n",
      "Epoch 7 Batch 1000 Loss 0.9319 Accuracy 0.2426\n",
      "Epoch 7 Batch 1050 Loss 0.9309 Accuracy 0.2428\n",
      "Epoch 7 Batch 1100 Loss 0.9298 Accuracy 0.2429\n",
      "Epoch 7 Batch 1150 Loss 0.9283 Accuracy 0.2429\n",
      "Epoch 7 Batch 1200 Loss 0.9272 Accuracy 0.2430\n",
      "Epoch 7 Batch 1250 Loss 0.9258 Accuracy 0.2430\n",
      "Epoch 7 Batch 1300 Loss 0.9249 Accuracy 0.2431\n",
      "Epoch 7 Batch 1350 Loss 0.9241 Accuracy 0.2432\n",
      "Epoch 7 Batch 1400 Loss 0.9236 Accuracy 0.2433\n",
      "Epoch 7 Batch 1450 Loss 0.9226 Accuracy 0.2434\n",
      "Epoch 7 Batch 1500 Loss 0.9222 Accuracy 0.2435\n",
      "Epoch 7 Loss 0.9221 Accuracy 0.2435\n",
      "Time taken for 1 epoch: 468.56915044784546 secs\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.8502 Accuracy 0.2460\n",
      "Epoch 8 Batch 50 Loss 0.8891 Accuracy 0.2484\n",
      "Epoch 8 Batch 100 Loss 0.8917 Accuracy 0.2479\n",
      "Epoch 8 Batch 150 Loss 0.8937 Accuracy 0.2496\n",
      "Epoch 8 Batch 200 Loss 0.8889 Accuracy 0.2489\n",
      "Epoch 8 Batch 250 Loss 0.8851 Accuracy 0.2487\n",
      "Epoch 8 Batch 300 Loss 0.8847 Accuracy 0.2488\n",
      "Epoch 8 Batch 350 Loss 0.8833 Accuracy 0.2486\n",
      "Epoch 8 Batch 400 Loss 0.8849 Accuracy 0.2492\n",
      "Epoch 8 Batch 450 Loss 0.8852 Accuracy 0.2490\n",
      "Epoch 8 Batch 500 Loss 0.8836 Accuracy 0.2491\n",
      "Epoch 8 Batch 550 Loss 0.8827 Accuracy 0.2491\n",
      "Epoch 8 Batch 600 Loss 0.8817 Accuracy 0.2492\n",
      "Epoch 8 Batch 650 Loss 0.8818 Accuracy 0.2491\n",
      "Epoch 8 Batch 700 Loss 0.8811 Accuracy 0.2492\n",
      "Epoch 8 Batch 750 Loss 0.8803 Accuracy 0.2492\n",
      "Epoch 8 Batch 800 Loss 0.8792 Accuracy 0.2491\n",
      "Epoch 8 Batch 850 Loss 0.8786 Accuracy 0.2493\n",
      "Epoch 8 Batch 900 Loss 0.8783 Accuracy 0.2494\n",
      "Epoch 8 Batch 950 Loss 0.8778 Accuracy 0.2494\n",
      "Epoch 8 Batch 1000 Loss 0.8772 Accuracy 0.2496\n",
      "Epoch 8 Batch 1050 Loss 0.8764 Accuracy 0.2497\n",
      "Epoch 8 Batch 1100 Loss 0.8758 Accuracy 0.2499\n",
      "Epoch 8 Batch 1150 Loss 0.8747 Accuracy 0.2499\n",
      "Epoch 8 Batch 1200 Loss 0.8743 Accuracy 0.2500\n",
      "Epoch 8 Batch 1250 Loss 0.8734 Accuracy 0.2500\n",
      "Epoch 8 Batch 1300 Loss 0.8728 Accuracy 0.2501\n",
      "Epoch 8 Batch 1350 Loss 0.8716 Accuracy 0.2501\n",
      "Epoch 8 Batch 1400 Loss 0.8707 Accuracy 0.2501\n",
      "Epoch 8 Batch 1450 Loss 0.8705 Accuracy 0.2503\n",
      "Epoch 8 Batch 1500 Loss 0.8705 Accuracy 0.2504\n",
      "Epoch 8 Loss 0.8704 Accuracy 0.2504\n",
      "Time taken for 1 epoch: 461.98932218551636 secs\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.7910 Accuracy 0.2479\n",
      "Epoch 9 Batch 50 Loss 0.8364 Accuracy 0.2530\n",
      "Epoch 9 Batch 100 Loss 0.8310 Accuracy 0.2529\n",
      "Epoch 9 Batch 150 Loss 0.8391 Accuracy 0.2538\n",
      "Epoch 9 Batch 200 Loss 0.8391 Accuracy 0.2538\n",
      "Epoch 9 Batch 250 Loss 0.8403 Accuracy 0.2542\n",
      "Epoch 9 Batch 300 Loss 0.8408 Accuracy 0.2544\n",
      "Epoch 9 Batch 350 Loss 0.8404 Accuracy 0.2546\n",
      "Epoch 9 Batch 400 Loss 0.8391 Accuracy 0.2548\n",
      "Epoch 9 Batch 450 Loss 0.8391 Accuracy 0.2549\n",
      "Epoch 9 Batch 500 Loss 0.8392 Accuracy 0.2551\n",
      "Epoch 9 Batch 550 Loss 0.8389 Accuracy 0.2551\n",
      "Epoch 9 Batch 600 Loss 0.8384 Accuracy 0.2551\n",
      "Epoch 9 Batch 650 Loss 0.8363 Accuracy 0.2548\n",
      "Epoch 9 Batch 700 Loss 0.8353 Accuracy 0.2547\n",
      "Epoch 9 Batch 750 Loss 0.8351 Accuracy 0.2549\n",
      "Epoch 9 Batch 800 Loss 0.8349 Accuracy 0.2549\n",
      "Epoch 9 Batch 850 Loss 0.8337 Accuracy 0.2549\n",
      "Epoch 9 Batch 900 Loss 0.8330 Accuracy 0.2551\n",
      "Epoch 9 Batch 950 Loss 0.8327 Accuracy 0.2552\n",
      "Epoch 9 Batch 1000 Loss 0.8329 Accuracy 0.2555\n",
      "Epoch 9 Batch 1050 Loss 0.8320 Accuracy 0.2555\n",
      "Epoch 9 Batch 1100 Loss 0.8313 Accuracy 0.2556\n",
      "Epoch 9 Batch 1150 Loss 0.8307 Accuracy 0.2557\n",
      "Epoch 9 Batch 1200 Loss 0.8302 Accuracy 0.2558\n",
      "Epoch 9 Batch 1250 Loss 0.8294 Accuracy 0.2558\n",
      "Epoch 9 Batch 1300 Loss 0.8288 Accuracy 0.2559\n",
      "Epoch 9 Batch 1350 Loss 0.8285 Accuracy 0.2560\n",
      "Epoch 9 Batch 1400 Loss 0.8280 Accuracy 0.2561\n",
      "Epoch 9 Batch 1450 Loss 0.8276 Accuracy 0.2562\n",
      "Epoch 9 Batch 1500 Loss 0.8271 Accuracy 0.2562\n",
      "Epoch 9 Loss 0.8271 Accuracy 0.2562\n",
      "Time taken for 1 epoch: 465.1774516105652 secs\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.8037 Accuracy 0.2484\n",
      "Epoch 10 Batch 50 Loss 0.7920 Accuracy 0.2571\n",
      "Epoch 10 Batch 100 Loss 0.7935 Accuracy 0.2590\n",
      "Epoch 10 Batch 450 Loss 0.7992 Accuracy 0.2603\n",
      "Epoch 10 Batch 500 Loss 0.7994 Accuracy 0.2602\n",
      "Epoch 10 Batch 550 Loss 0.7989 Accuracy 0.2601\n",
      "Epoch 10 Batch 600 Loss 0.7975 Accuracy 0.2602\n",
      "Epoch 10 Batch 650 Loss 0.7975 Accuracy 0.2602\n",
      "Epoch 10 Batch 700 Loss 0.7977 Accuracy 0.2603\n",
      "Epoch 10 Batch 750 Loss 0.7976 Accuracy 0.2603\n",
      "Epoch 10 Batch 800 Loss 0.7972 Accuracy 0.2605\n",
      "Epoch 10 Batch 850 Loss 0.7964 Accuracy 0.2605\n",
      "Epoch 10 Batch 900 Loss 0.7959 Accuracy 0.2605\n",
      "Epoch 10 Batch 950 Loss 0.7955 Accuracy 0.2606\n",
      "Epoch 10 Batch 1000 Loss 0.7951 Accuracy 0.2608\n",
      "Epoch 10 Batch 1050 Loss 0.7943 Accuracy 0.2607\n",
      "Epoch 10 Batch 1100 Loss 0.7934 Accuracy 0.2607\n",
      "Epoch 10 Batch 1150 Loss 0.7927 Accuracy 0.2607\n",
      "Epoch 10 Batch 1200 Loss 0.7923 Accuracy 0.2607\n",
      "Epoch 10 Batch 1250 Loss 0.7922 Accuracy 0.2608\n",
      "Epoch 10 Batch 1300 Loss 0.7914 Accuracy 0.2608\n",
      "Epoch 10 Batch 1350 Loss 0.7911 Accuracy 0.2609\n",
      "Epoch 10 Batch 1400 Loss 0.7908 Accuracy 0.2610\n",
      "Epoch 10 Batch 1450 Loss 0.7902 Accuracy 0.2610\n",
      "Epoch 10 Batch 1500 Loss 0.7896 Accuracy 0.2610\n",
      "Saving checkpoint for epoch 10 at ./checkpoints/train-2/ckpt-2\n",
      "Epoch 10 Loss 0.7895 Accuracy 0.2610\n",
      "Time taken for 1 epoch: 464.85156631469727 secs\n",
      "\n",
      "Epoch 11 Batch 0 Loss 0.8244 Accuracy 0.2692\n",
      "Epoch 11 Batch 50 Loss 0.7670 Accuracy 0.2629\n",
      "Epoch 11 Batch 100 Loss 0.7672 Accuracy 0.2638\n",
      "Epoch 11 Batch 150 Loss 0.7681 Accuracy 0.2645\n",
      "Epoch 11 Batch 200 Loss 0.7678 Accuracy 0.2639\n",
      "Epoch 11 Batch 250 Loss 0.7655 Accuracy 0.2643\n",
      "Epoch 11 Batch 300 Loss 0.7648 Accuracy 0.2643\n",
      "Epoch 11 Batch 350 Loss 0.7637 Accuracy 0.2645\n",
      "Epoch 11 Batch 400 Loss 0.7645 Accuracy 0.2645\n",
      "Epoch 11 Batch 450 Loss 0.7647 Accuracy 0.2645\n",
      "Epoch 11 Batch 500 Loss 0.7647 Accuracy 0.2646\n",
      "Epoch 11 Batch 550 Loss 0.7643 Accuracy 0.2648\n",
      "Epoch 11 Batch 600 Loss 0.7641 Accuracy 0.2649\n",
      "Epoch 11 Batch 650 Loss 0.7633 Accuracy 0.2648\n",
      "Epoch 11 Batch 700 Loss 0.7637 Accuracy 0.2650\n",
      "Epoch 11 Batch 750 Loss 0.7641 Accuracy 0.2650\n",
      "Epoch 11 Batch 800 Loss 0.7638 Accuracy 0.2652\n",
      "Epoch 11 Batch 850 Loss 0.7630 Accuracy 0.2651\n",
      "Epoch 11 Batch 900 Loss 0.7623 Accuracy 0.2651\n",
      "Epoch 11 Batch 950 Loss 0.7620 Accuracy 0.2652\n",
      "Epoch 11 Batch 1000 Loss 0.7613 Accuracy 0.2653\n",
      "Epoch 11 Batch 1050 Loss 0.7613 Accuracy 0.2654\n",
      "Epoch 11 Batch 1100 Loss 0.7605 Accuracy 0.2655\n",
      "Epoch 11 Batch 1150 Loss 0.7596 Accuracy 0.2655\n",
      "Epoch 11 Batch 1200 Loss 0.7590 Accuracy 0.2655\n",
      "Epoch 11 Batch 1250 Loss 0.7589 Accuracy 0.2656\n",
      "Epoch 11 Batch 1300 Loss 0.7583 Accuracy 0.2656\n",
      "Epoch 11 Batch 1350 Loss 0.7581 Accuracy 0.2656\n",
      "Epoch 11 Batch 1400 Loss 0.7582 Accuracy 0.2657\n",
      "Epoch 11 Batch 1450 Loss 0.7576 Accuracy 0.2657\n",
      "Epoch 11 Batch 1500 Loss 0.7575 Accuracy 0.2657\n",
      "Epoch 11 Loss 0.7575 Accuracy 0.2657\n",
      "Time taken for 1 epoch: 464.26050996780396 secs\n",
      "\n",
      "Epoch 12 Batch 0 Loss 0.7893 Accuracy 0.2734\n",
      "Epoch 12 Batch 50 Loss 0.7337 Accuracy 0.2677\n",
      "Epoch 12 Batch 100 Loss 0.7353 Accuracy 0.2685\n",
      "Epoch 12 Batch 150 Loss 0.7379 Accuracy 0.2701\n",
      "Epoch 12 Batch 200 Loss 0.7407 Accuracy 0.2699\n",
      "Epoch 12 Batch 250 Loss 0.7369 Accuracy 0.2693\n",
      "Epoch 12 Batch 300 Loss 0.7347 Accuracy 0.2688\n",
      "Epoch 12 Batch 350 Loss 0.7350 Accuracy 0.2690\n",
      "Epoch 12 Batch 400 Loss 0.7356 Accuracy 0.2694\n",
      "Epoch 12 Batch 450 Loss 0.7343 Accuracy 0.2694\n",
      "Epoch 12 Batch 500 Loss 0.7350 Accuracy 0.2695\n",
      "Epoch 12 Batch 550 Loss 0.7358 Accuracy 0.2698\n",
      "Epoch 12 Batch 600 Loss 0.7357 Accuracy 0.2698\n",
      "Epoch 12 Batch 650 Loss 0.7345 Accuracy 0.2695\n",
      "Epoch 12 Batch 700 Loss 0.7345 Accuracy 0.2695\n",
      "Epoch 12 Batch 750 Loss 0.7340 Accuracy 0.2695\n",
      "Epoch 12 Batch 800 Loss 0.7340 Accuracy 0.2695\n",
      "Epoch 12 Batch 850 Loss 0.7343 Accuracy 0.2696\n",
      "Epoch 12 Batch 900 Loss 0.7344 Accuracy 0.2698\n",
      "Epoch 12 Batch 950 Loss 0.7339 Accuracy 0.2699\n",
      "Epoch 12 Batch 1000 Loss 0.7337 Accuracy 0.2699\n",
      "Epoch 12 Batch 1050 Loss 0.7332 Accuracy 0.2701\n",
      "Epoch 12 Batch 1100 Loss 0.7325 Accuracy 0.2701\n",
      "Epoch 12 Batch 1150 Loss 0.7319 Accuracy 0.2700\n",
      "Epoch 12 Batch 1200 Loss 0.7315 Accuracy 0.2702\n",
      "Epoch 12 Batch 1250 Loss 0.7311 Accuracy 0.2702\n",
      "Epoch 12 Batch 1300 Loss 0.7307 Accuracy 0.2702\n",
      "Epoch 12 Batch 1350 Loss 0.7305 Accuracy 0.2703\n",
      "Epoch 12 Batch 1400 Loss 0.7307 Accuracy 0.2704\n",
      "Epoch 12 Batch 1450 Loss 0.7302 Accuracy 0.2703\n",
      "Epoch 12 Batch 1500 Loss 0.7299 Accuracy 0.2703\n",
      "Epoch 12 Loss 0.7297 Accuracy 0.2703\n",
      "Time taken for 1 epoch: 463.8919641971588 secs\n",
      "\n",
      "Epoch 13 Batch 0 Loss 0.7149 Accuracy 0.2684\n",
      "Epoch 13 Batch 50 Loss 0.7111 Accuracy 0.2718\n",
      "Epoch 13 Batch 100 Loss 0.7107 Accuracy 0.2720\n",
      "Epoch 13 Batch 150 Loss 0.7091 Accuracy 0.2725\n",
      "Epoch 13 Batch 200 Loss 0.7112 Accuracy 0.2734\n",
      "Epoch 13 Batch 250 Loss 0.7118 Accuracy 0.2738\n",
      "Epoch 13 Batch 300 Loss 0.7104 Accuracy 0.2734\n",
      "Epoch 13 Batch 350 Loss 0.7107 Accuracy 0.2734\n",
      "Epoch 13 Batch 400 Loss 0.7103 Accuracy 0.2735\n",
      "Epoch 13 Batch 450 Loss 0.7092 Accuracy 0.2731\n",
      "Epoch 13 Batch 500 Loss 0.7089 Accuracy 0.2729\n",
      "Epoch 13 Batch 550 Loss 0.7087 Accuracy 0.2732\n",
      "Epoch 13 Batch 600 Loss 0.7089 Accuracy 0.2732\n",
      "Epoch 13 Batch 650 Loss 0.7087 Accuracy 0.2732\n",
      "Epoch 13 Batch 700 Loss 0.7087 Accuracy 0.2732\n",
      "Epoch 13 Batch 750 Loss 0.7084 Accuracy 0.2732\n",
      "Epoch 13 Batch 800 Loss 0.7078 Accuracy 0.2733\n",
      "Epoch 13 Batch 850 Loss 0.7072 Accuracy 0.2732\n",
      "Epoch 13 Batch 900 Loss 0.7073 Accuracy 0.2733\n",
      "Epoch 13 Batch 950 Loss 0.7073 Accuracy 0.2734\n",
      "Epoch 13 Batch 1000 Loss 0.7063 Accuracy 0.2732\n",
      "Epoch 13 Batch 1050 Loss 0.7064 Accuracy 0.2734\n",
      "Epoch 13 Batch 1100 Loss 0.7060 Accuracy 0.2736\n",
      "Epoch 13 Batch 1150 Loss 0.7056 Accuracy 0.2736\n",
      "Epoch 13 Batch 1200 Loss 0.7050 Accuracy 0.2735\n",
      "Epoch 13 Batch 1250 Loss 0.7048 Accuracy 0.2736\n",
      "Epoch 13 Batch 1300 Loss 0.7049 Accuracy 0.2737\n",
      "Epoch 13 Batch 1350 Loss 0.7046 Accuracy 0.2738\n",
      "Epoch 13 Batch 1400 Loss 0.7047 Accuracy 0.2739\n",
      "Epoch 13 Batch 1450 Loss 0.7041 Accuracy 0.2738\n",
      "Epoch 13 Batch 1500 Loss 0.7040 Accuracy 0.2739\n",
      "Epoch 13 Loss 0.7040 Accuracy 0.2738\n",
      "Time taken for 1 epoch: 462.8948538303375 secs\n",
      "\n",
      "Epoch 14 Batch 0 Loss 0.6744 Accuracy 0.2594\n",
      "Epoch 14 Batch 50 Loss 0.6835 Accuracy 0.2765\n",
      "Epoch 14 Batch 100 Loss 0.6842 Accuracy 0.2765\n",
      "Epoch 14 Batch 150 Loss 0.6841 Accuracy 0.2764\n",
      "Epoch 14 Batch 200 Loss 0.6843 Accuracy 0.2758\n",
      "Epoch 14 Batch 250 Loss 0.6845 Accuracy 0.2762\n",
      "Epoch 14 Batch 300 Loss 0.6845 Accuracy 0.2761\n",
      "Epoch 14 Batch 350 Loss 0.6843 Accuracy 0.2759\n",
      "Epoch 14 Batch 400 Loss 0.6845 Accuracy 0.2763\n",
      "Epoch 14 Batch 450 Loss 0.6857 Accuracy 0.2766\n",
      "Epoch 14 Batch 500 Loss 0.6861 Accuracy 0.2767\n",
      "Epoch 14 Batch 550 Loss 0.6854 Accuracy 0.2764\n",
      "Epoch 14 Batch 600 Loss 0.6853 Accuracy 0.2765\n",
      "Epoch 14 Batch 650 Loss 0.6852 Accuracy 0.2764\n",
      "Epoch 14 Batch 700 Loss 0.6852 Accuracy 0.2764\n",
      "Epoch 14 Batch 750 Loss 0.6845 Accuracy 0.2766\n",
      "Epoch 14 Batch 800 Loss 0.6847 Accuracy 0.2766\n",
      "Epoch 14 Batch 850 Loss 0.6841 Accuracy 0.2767\n",
      "Epoch 14 Batch 900 Loss 0.6844 Accuracy 0.2769\n",
      "Epoch 14 Batch 950 Loss 0.6841 Accuracy 0.2770\n",
      "Epoch 14 Batch 1000 Loss 0.6842 Accuracy 0.2772\n",
      "Epoch 14 Batch 1050 Loss 0.6836 Accuracy 0.2771\n",
      "Epoch 14 Batch 1100 Loss 0.6835 Accuracy 0.2773\n",
      "Epoch 14 Batch 1150 Loss 0.6834 Accuracy 0.2774\n",
      "Epoch 14 Batch 1200 Loss 0.6824 Accuracy 0.2772\n",
      "Epoch 14 Batch 1250 Loss 0.6822 Accuracy 0.2773\n",
      "Epoch 14 Batch 1300 Loss 0.6817 Accuracy 0.2772\n",
      "Epoch 14 Batch 1350 Loss 0.6815 Accuracy 0.2773\n",
      "Epoch 14 Batch 1400 Loss 0.6813 Accuracy 0.2774\n",
      "Epoch 14 Batch 1450 Loss 0.6812 Accuracy 0.2773\n",
      "Epoch 14 Batch 1500 Loss 0.6814 Accuracy 0.2774\n",
      "Epoch 14 Loss 0.6814 Accuracy 0.2774\n",
      "Time taken for 1 epoch: 462.29803109169006 secs\n",
      "\n",
      "Epoch 15 Batch 0 Loss 0.7199 Accuracy 0.2791\n",
      "Epoch 15 Batch 50 Loss 0.6699 Accuracy 0.2828\n",
      "Epoch 15 Batch 100 Loss 0.6602 Accuracy 0.2805\n",
      "Epoch 15 Batch 150 Loss 0.6618 Accuracy 0.2808\n",
      "Epoch 15 Batch 200 Loss 0.6615 Accuracy 0.2807\n",
      "Epoch 15 Batch 250 Loss 0.6632 Accuracy 0.2812\n",
      "Epoch 15 Batch 300 Loss 0.6644 Accuracy 0.2808\n",
      "Epoch 15 Batch 350 Loss 0.6649 Accuracy 0.2805\n",
      "Epoch 15 Batch 400 Loss 0.6635 Accuracy 0.2804\n",
      "Epoch 15 Batch 450 Loss 0.6640 Accuracy 0.2805\n",
      "Epoch 15 Batch 500 Loss 0.6637 Accuracy 0.2803\n",
      "Epoch 15 Batch 550 Loss 0.6632 Accuracy 0.2801\n",
      "Epoch 15 Batch 600 Loss 0.6632 Accuracy 0.2801\n",
      "Epoch 15 Batch 650 Loss 0.6628 Accuracy 0.2799\n",
      "Epoch 15 Batch 700 Loss 0.6629 Accuracy 0.2798\n",
      "Epoch 15 Batch 750 Loss 0.6630 Accuracy 0.2799\n",
      "Epoch 15 Batch 800 Loss 0.6631 Accuracy 0.2800\n",
      "Epoch 15 Batch 850 Loss 0.6622 Accuracy 0.2798\n",
      "Epoch 15 Batch 900 Loss 0.6623 Accuracy 0.2799\n",
      "Epoch 15 Batch 950 Loss 0.6620 Accuracy 0.2799\n",
      "Epoch 15 Batch 1000 Loss 0.6618 Accuracy 0.2800\n",
      "Epoch 15 Batch 1050 Loss 0.6615 Accuracy 0.2801\n",
      "Epoch 15 Batch 1100 Loss 0.6614 Accuracy 0.2803\n",
      "Epoch 15 Batch 1150 Loss 0.6610 Accuracy 0.2803\n",
      "Epoch 15 Batch 1200 Loss 0.6607 Accuracy 0.2803\n",
      "Epoch 15 Batch 1250 Loss 0.6609 Accuracy 0.2804\n",
      "Epoch 15 Batch 1300 Loss 0.6606 Accuracy 0.2804\n",
      "Epoch 15 Batch 1350 Loss 0.6605 Accuracy 0.2805\n",
      "Epoch 15 Batch 1400 Loss 0.6603 Accuracy 0.2805\n",
      "Epoch 15 Batch 1450 Loss 0.6604 Accuracy 0.2806\n",
      "Epoch 15 Batch 1500 Loss 0.6601 Accuracy 0.2806\n",
      "Saving checkpoint for epoch 15 at ./checkpoints/train-2/ckpt-3\n",
      "Epoch 15 Loss 0.6602 Accuracy 0.2806\n",
      "Time taken for 1 epoch: 464.60785365104675 secs\n",
      "\n",
      "Epoch 16 Batch 0 Loss 0.6837 Accuracy 0.2955\n",
      "Epoch 16 Batch 50 Loss 0.6463 Accuracy 0.2820\n",
      "Epoch 16 Batch 100 Loss 0.6480 Accuracy 0.2833\n",
      "Epoch 16 Batch 150 Loss 0.6473 Accuracy 0.2845\n",
      "Epoch 16 Batch 200 Loss 0.6460 Accuracy 0.2838\n",
      "Epoch 16 Batch 250 Loss 0.6450 Accuracy 0.2835\n",
      "Epoch 16 Batch 300 Loss 0.6445 Accuracy 0.2834\n",
      "Epoch 16 Batch 350 Loss 0.6439 Accuracy 0.2829\n",
      "Epoch 16 Batch 400 Loss 0.6448 Accuracy 0.2833\n",
      "Epoch 16 Batch 450 Loss 0.6447 Accuracy 0.2835\n",
      "Epoch 16 Batch 500 Loss 0.6441 Accuracy 0.2832\n",
      "Epoch 16 Batch 550 Loss 0.6437 Accuracy 0.2832\n",
      "Epoch 16 Batch 600 Loss 0.6439 Accuracy 0.2831\n",
      "Epoch 16 Batch 650 Loss 0.6442 Accuracy 0.2831\n",
      "Epoch 16 Batch 700 Loss 0.6440 Accuracy 0.2830\n",
      "Epoch 16 Batch 750 Loss 0.6444 Accuracy 0.2832\n",
      "Epoch 16 Batch 800 Loss 0.6443 Accuracy 0.2831\n",
      "Epoch 16 Batch 850 Loss 0.6441 Accuracy 0.2833\n",
      "Epoch 16 Batch 900 Loss 0.6439 Accuracy 0.2834\n",
      "Epoch 16 Batch 950 Loss 0.6433 Accuracy 0.2834\n",
      "Epoch 16 Batch 1000 Loss 0.6423 Accuracy 0.2833\n",
      "Epoch 16 Batch 1050 Loss 0.6419 Accuracy 0.2833\n",
      "Epoch 16 Batch 1100 Loss 0.6417 Accuracy 0.2834\n",
      "Epoch 16 Batch 1150 Loss 0.6417 Accuracy 0.2834\n",
      "Epoch 16 Batch 1200 Loss 0.6420 Accuracy 0.2835\n",
      "Epoch 16 Batch 1250 Loss 0.6417 Accuracy 0.2835\n",
      "Epoch 16 Batch 1300 Loss 0.6414 Accuracy 0.2836\n",
      "Epoch 16 Batch 1350 Loss 0.6413 Accuracy 0.2836\n",
      "Epoch 16 Batch 1400 Loss 0.6407 Accuracy 0.2836\n",
      "Epoch 16 Batch 1450 Loss 0.6405 Accuracy 0.2835\n",
      "Epoch 16 Batch 1500 Loss 0.6406 Accuracy 0.2836\n",
      "Epoch 16 Loss 0.6407 Accuracy 0.2836\n",
      "Time taken for 1 epoch: 460.3745505809784 secs\n",
      "\n",
      "Epoch 17 Batch 0 Loss 0.6836 Accuracy 0.2930\n",
      "Epoch 17 Batch 50 Loss 0.6292 Accuracy 0.2866\n",
      "Epoch 17 Batch 100 Loss 0.6254 Accuracy 0.2855\n",
      "Epoch 17 Batch 150 Loss 0.6263 Accuracy 0.2860\n",
      "Epoch 17 Batch 200 Loss 0.6282 Accuracy 0.2864\n",
      "Epoch 17 Batch 250 Loss 0.6272 Accuracy 0.2860\n",
      "Epoch 17 Batch 300 Loss 0.6274 Accuracy 0.2859\n",
      "Epoch 17 Batch 350 Loss 0.6277 Accuracy 0.2860\n",
      "Epoch 17 Batch 400 Loss 0.6272 Accuracy 0.2861\n",
      "Epoch 17 Batch 450 Loss 0.6259 Accuracy 0.2855\n",
      "Epoch 17 Batch 500 Loss 0.6261 Accuracy 0.2857\n",
      "Epoch 17 Batch 550 Loss 0.6250 Accuracy 0.2856\n",
      "Epoch 17 Batch 600 Loss 0.6246 Accuracy 0.2857\n",
      "Epoch 17 Batch 650 Loss 0.6245 Accuracy 0.2853\n",
      "Epoch 17 Batch 700 Loss 0.6249 Accuracy 0.2855\n",
      "Epoch 17 Batch 750 Loss 0.6251 Accuracy 0.2855\n",
      "Epoch 17 Batch 800 Loss 0.6247 Accuracy 0.2856\n",
      "Epoch 17 Batch 850 Loss 0.6249 Accuracy 0.2857\n",
      "Epoch 17 Batch 900 Loss 0.6248 Accuracy 0.2858\n",
      "Epoch 17 Batch 950 Loss 0.6248 Accuracy 0.2858\n",
      "Epoch 17 Batch 1000 Loss 0.6245 Accuracy 0.2859\n",
      "Epoch 17 Batch 1050 Loss 0.6243 Accuracy 0.2861\n",
      "Epoch 17 Batch 1100 Loss 0.6237 Accuracy 0.2861\n",
      "Epoch 17 Batch 1150 Loss 0.6237 Accuracy 0.2862\n",
      "Epoch 17 Batch 1200 Loss 0.6231 Accuracy 0.2860\n",
      "Epoch 17 Batch 1250 Loss 0.6229 Accuracy 0.2860\n",
      "Epoch 17 Batch 1300 Loss 0.6228 Accuracy 0.2860\n",
      "Epoch 17 Batch 1350 Loss 0.6229 Accuracy 0.2861\n",
      "Epoch 17 Batch 1400 Loss 0.6227 Accuracy 0.2861\n",
      "Epoch 17 Batch 1450 Loss 0.6224 Accuracy 0.2862\n",
      "Epoch 17 Batch 1500 Loss 0.6228 Accuracy 0.2862\n",
      "Epoch 17 Loss 0.6228 Accuracy 0.2862\n",
      "Time taken for 1 epoch: 469.6315248012543 secs\n",
      "\n",
      "Epoch 18 Batch 0 Loss 0.6382 Accuracy 0.2879\n",
      "Epoch 18 Batch 50 Loss 0.6116 Accuracy 0.2864\n",
      "Epoch 18 Batch 100 Loss 0.6069 Accuracy 0.2871\n",
      "Epoch 18 Batch 150 Loss 0.6064 Accuracy 0.2880\n",
      "Epoch 18 Batch 200 Loss 0.6091 Accuracy 0.2888\n",
      "Epoch 18 Batch 250 Loss 0.6087 Accuracy 0.2893\n",
      "Epoch 18 Batch 300 Loss 0.6095 Accuracy 0.2893\n",
      "Epoch 18 Batch 350 Loss 0.6109 Accuracy 0.2892\n",
      "Epoch 18 Batch 400 Loss 0.6092 Accuracy 0.2887\n",
      "Epoch 18 Batch 450 Loss 0.6098 Accuracy 0.2887\n",
      "Epoch 18 Batch 500 Loss 0.6096 Accuracy 0.2889\n",
      "Epoch 18 Batch 550 Loss 0.6092 Accuracy 0.2887\n",
      "Epoch 18 Batch 600 Loss 0.6089 Accuracy 0.2888\n",
      "Epoch 18 Batch 650 Loss 0.6086 Accuracy 0.2884\n",
      "Epoch 18 Batch 700 Loss 0.6078 Accuracy 0.2881\n",
      "Epoch 18 Batch 750 Loss 0.6087 Accuracy 0.2884\n",
      "Epoch 18 Batch 800 Loss 0.6096 Accuracy 0.2889\n",
      "Epoch 18 Batch 850 Loss 0.6090 Accuracy 0.2889\n",
      "Epoch 18 Batch 900 Loss 0.6091 Accuracy 0.2889\n",
      "Epoch 18 Batch 950 Loss 0.6090 Accuracy 0.2891\n",
      "Epoch 18 Batch 1000 Loss 0.6082 Accuracy 0.2890\n",
      "Epoch 18 Batch 1050 Loss 0.6080 Accuracy 0.2891\n",
      "Epoch 18 Batch 1100 Loss 0.6080 Accuracy 0.2891\n",
      "Epoch 18 Batch 1150 Loss 0.6081 Accuracy 0.2891\n",
      "Epoch 18 Batch 1200 Loss 0.6077 Accuracy 0.2891\n",
      "Epoch 18 Batch 1250 Loss 0.6077 Accuracy 0.2891\n",
      "Epoch 18 Batch 1300 Loss 0.6077 Accuracy 0.2891\n",
      "Epoch 18 Batch 1350 Loss 0.6074 Accuracy 0.2891\n",
      "Epoch 18 Batch 1400 Loss 0.6071 Accuracy 0.2892\n",
      "Epoch 18 Batch 1450 Loss 0.6068 Accuracy 0.2891\n",
      "Epoch 18 Batch 1500 Loss 0.6067 Accuracy 0.2892\n",
      "Epoch 18 Loss 0.6067 Accuracy 0.2891\n",
      "Time taken for 1 epoch: 476.43325090408325 secs\n",
      "\n",
      "Epoch 19 Batch 0 Loss 0.5897 Accuracy 0.3061\n",
      "Epoch 19 Batch 50 Loss 0.5984 Accuracy 0.2912\n",
      "Epoch 19 Batch 100 Loss 0.5942 Accuracy 0.2907\n",
      "Epoch 19 Batch 150 Loss 0.5956 Accuracy 0.2924\n",
      "Epoch 19 Batch 200 Loss 0.5949 Accuracy 0.2918\n",
      "Epoch 19 Batch 250 Loss 0.5948 Accuracy 0.2919\n",
      "Epoch 19 Batch 300 Loss 0.5950 Accuracy 0.2918\n",
      "Epoch 19 Batch 350 Loss 0.5940 Accuracy 0.2919\n",
      "Epoch 19 Batch 400 Loss 0.5942 Accuracy 0.2919\n",
      "Epoch 19 Batch 450 Loss 0.5943 Accuracy 0.2917\n",
      "Epoch 19 Batch 500 Loss 0.5940 Accuracy 0.2915\n",
      "Epoch 19 Batch 550 Loss 0.5945 Accuracy 0.2917\n",
      "Epoch 19 Batch 600 Loss 0.5945 Accuracy 0.2916\n",
      "Epoch 19 Batch 650 Loss 0.5939 Accuracy 0.2914\n",
      "Epoch 19 Batch 700 Loss 0.5934 Accuracy 0.2913\n",
      "Epoch 19 Batch 750 Loss 0.5934 Accuracy 0.2912\n",
      "Epoch 19 Batch 800 Loss 0.5931 Accuracy 0.2912\n",
      "Epoch 19 Batch 850 Loss 0.5932 Accuracy 0.2913\n",
      "Epoch 19 Batch 900 Loss 0.5930 Accuracy 0.2914\n",
      "Epoch 19 Batch 950 Loss 0.5925 Accuracy 0.2913\n",
      "Epoch 19 Batch 1000 Loss 0.5923 Accuracy 0.2914\n",
      "Epoch 19 Batch 1050 Loss 0.5923 Accuracy 0.2916\n",
      "Epoch 19 Batch 1100 Loss 0.5922 Accuracy 0.2917\n",
      "Epoch 19 Batch 1150 Loss 0.5917 Accuracy 0.2917\n",
      "Epoch 19 Batch 1200 Loss 0.5916 Accuracy 0.2917\n",
      "Epoch 19 Batch 1250 Loss 0.5910 Accuracy 0.2916\n",
      "Epoch 19 Batch 1300 Loss 0.5910 Accuracy 0.2916\n",
      "Epoch 19 Batch 1350 Loss 0.5910 Accuracy 0.2918\n",
      "Epoch 19 Batch 1400 Loss 0.5909 Accuracy 0.2918\n",
      "Epoch 19 Batch 1450 Loss 0.5912 Accuracy 0.2919\n",
      "Epoch 19 Batch 1500 Loss 0.5912 Accuracy 0.2917\n",
      "Epoch 19 Loss 0.5912 Accuracy 0.2918\n",
      "Time taken for 1 epoch: 467.1607675552368 secs\n",
      "\n",
      "Epoch 20 Batch 0 Loss 0.5957 Accuracy 0.2930\n",
      "Epoch 20 Batch 50 Loss 0.5789 Accuracy 0.2931\n",
      "Epoch 20 Batch 100 Loss 0.5794 Accuracy 0.2934\n",
      "Epoch 20 Batch 150 Loss 0.5769 Accuracy 0.2935\n",
      "Epoch 20 Batch 200 Loss 0.5791 Accuracy 0.2937\n",
      "Epoch 20 Batch 250 Loss 0.5774 Accuracy 0.2928\n",
      "Epoch 20 Batch 300 Loss 0.5774 Accuracy 0.2926\n",
      "Epoch 20 Batch 350 Loss 0.5769 Accuracy 0.2923\n",
      "Epoch 20 Batch 400 Loss 0.5770 Accuracy 0.2926\n",
      "Epoch 20 Batch 450 Loss 0.5775 Accuracy 0.2929\n",
      "Epoch 20 Batch 500 Loss 0.5783 Accuracy 0.2930\n",
      "Epoch 20 Batch 550 Loss 0.5783 Accuracy 0.2929\n",
      "Epoch 20 Batch 600 Loss 0.5783 Accuracy 0.2927\n",
      "Epoch 20 Batch 650 Loss 0.5793 Accuracy 0.2930\n",
      "Epoch 20 Batch 700 Loss 0.5791 Accuracy 0.2931\n",
      "Epoch 20 Batch 750 Loss 0.5787 Accuracy 0.2929\n",
      "Epoch 20 Batch 800 Loss 0.5788 Accuracy 0.2933\n",
      "Epoch 20 Batch 850 Loss 0.5786 Accuracy 0.2934\n",
      "Epoch 20 Batch 900 Loss 0.5787 Accuracy 0.2935\n",
      "Epoch 20 Batch 950 Loss 0.5787 Accuracy 0.2935\n",
      "Epoch 20 Batch 1000 Loss 0.5789 Accuracy 0.2937\n",
      "Epoch 20 Batch 1050 Loss 0.5783 Accuracy 0.2937\n",
      "Epoch 20 Batch 1100 Loss 0.5782 Accuracy 0.2939\n",
      "Epoch 20 Batch 1150 Loss 0.5778 Accuracy 0.2939\n",
      "Epoch 20 Batch 1200 Loss 0.5775 Accuracy 0.2939\n",
      "Epoch 20 Batch 1250 Loss 0.5777 Accuracy 0.2940\n",
      "Epoch 20 Batch 1300 Loss 0.5774 Accuracy 0.2940\n",
      "Epoch 20 Batch 1350 Loss 0.5777 Accuracy 0.2941\n",
      "Epoch 20 Batch 1400 Loss 0.5775 Accuracy 0.2940\n",
      "Epoch 20 Batch 1450 Loss 0.5774 Accuracy 0.2940\n",
      "Epoch 20 Batch 1500 Loss 0.5774 Accuracy 0.2940\n",
      "Saving checkpoint for epoch 20 at ./checkpoints/train-2/ckpt-4\n",
      "Epoch 20 Loss 0.5774 Accuracy 0.2940\n",
      "Time taken for 1 epoch: 460.56825065612793 secs\n",
      "\n",
      "Epoch 21 Batch 0 Loss 0.6615 Accuracy 0.3183\n",
      "Epoch 21 Batch 50 Loss 0.5671 Accuracy 0.2971\n",
      "Epoch 21 Batch 100 Loss 0.5641 Accuracy 0.2960\n",
      "Epoch 21 Batch 150 Loss 0.5649 Accuracy 0.2962\n",
      "Epoch 21 Batch 200 Loss 0.5662 Accuracy 0.2964\n",
      "Epoch 21 Batch 250 Loss 0.5666 Accuracy 0.2959\n",
      "Epoch 21 Batch 300 Loss 0.5664 Accuracy 0.2960\n",
      "Epoch 21 Batch 350 Loss 0.5674 Accuracy 0.2962\n",
      "Epoch 21 Batch 400 Loss 0.5650 Accuracy 0.2958\n",
      "Epoch 21 Batch 450 Loss 0.5663 Accuracy 0.2962\n",
      "Epoch 21 Batch 500 Loss 0.5668 Accuracy 0.2964\n",
      "Epoch 21 Batch 550 Loss 0.5667 Accuracy 0.2966\n",
      "Epoch 21 Batch 600 Loss 0.5661 Accuracy 0.2964\n",
      "Epoch 21 Batch 650 Loss 0.5663 Accuracy 0.2962\n",
      "Epoch 21 Batch 700 Loss 0.5661 Accuracy 0.2960\n",
      "Epoch 21 Batch 750 Loss 0.5667 Accuracy 0.2960\n",
      "Epoch 21 Batch 800 Loss 0.5661 Accuracy 0.2958\n",
      "Epoch 21 Batch 850 Loss 0.5662 Accuracy 0.2961\n",
      "Epoch 21 Batch 900 Loss 0.5658 Accuracy 0.2961\n",
      "Epoch 21 Batch 950 Loss 0.5658 Accuracy 0.2961\n",
      "Epoch 21 Batch 1000 Loss 0.5655 Accuracy 0.2962\n",
      "Epoch 21 Batch 1050 Loss 0.5651 Accuracy 0.2962\n",
      "Epoch 21 Batch 1100 Loss 0.5651 Accuracy 0.2963\n",
      "Epoch 21 Batch 1150 Loss 0.5650 Accuracy 0.2962\n",
      "Epoch 21 Batch 1200 Loss 0.5648 Accuracy 0.2963\n",
      "Epoch 21 Batch 1250 Loss 0.5648 Accuracy 0.2963\n",
      "Epoch 21 Batch 1300 Loss 0.5646 Accuracy 0.2963\n",
      "Epoch 21 Batch 1350 Loss 0.5649 Accuracy 0.2964\n",
      "Epoch 21 Batch 1400 Loss 0.5646 Accuracy 0.2964\n",
      "Epoch 21 Batch 1450 Loss 0.5644 Accuracy 0.2963\n",
      "Epoch 21 Batch 1500 Loss 0.5644 Accuracy 0.2963\n",
      "Epoch 21 Loss 0.5643 Accuracy 0.2963\n",
      "Time taken for 1 epoch: 463.17497992515564 secs\n",
      "\n",
      "Epoch 22 Batch 0 Loss 0.5480 Accuracy 0.2768\n",
      "Epoch 22 Batch 50 Loss 0.5508 Accuracy 0.2961\n",
      "Epoch 22 Batch 100 Loss 0.5521 Accuracy 0.2978\n",
      "Epoch 22 Batch 150 Loss 0.5522 Accuracy 0.2983\n",
      "Epoch 22 Batch 200 Loss 0.5515 Accuracy 0.2980\n",
      "Epoch 22 Batch 250 Loss 0.5524 Accuracy 0.2985\n",
      "Epoch 22 Batch 300 Loss 0.5510 Accuracy 0.2982\n",
      "Epoch 22 Batch 350 Loss 0.5528 Accuracy 0.2980\n",
      "Epoch 22 Batch 400 Loss 0.5518 Accuracy 0.2976\n",
      "Epoch 22 Batch 450 Loss 0.5520 Accuracy 0.2977\n",
      "Epoch 22 Batch 500 Loss 0.5529 Accuracy 0.2980\n",
      "Epoch 22 Batch 550 Loss 0.5529 Accuracy 0.2975\n",
      "Epoch 22 Batch 600 Loss 0.5524 Accuracy 0.2972\n",
      "Epoch 22 Batch 650 Loss 0.5524 Accuracy 0.2973\n",
      "Epoch 22 Batch 700 Loss 0.5522 Accuracy 0.2974\n",
      "Epoch 22 Batch 750 Loss 0.5520 Accuracy 0.2975\n",
      "Epoch 22 Batch 800 Loss 0.5521 Accuracy 0.2976\n",
      "Epoch 22 Batch 850 Loss 0.5520 Accuracy 0.2976\n",
      "Epoch 22 Batch 900 Loss 0.5520 Accuracy 0.2976\n",
      "Epoch 22 Batch 950 Loss 0.5517 Accuracy 0.2976\n",
      "Epoch 22 Batch 1000 Loss 0.5517 Accuracy 0.2978\n",
      "Epoch 22 Batch 1050 Loss 0.5516 Accuracy 0.2979\n",
      "Epoch 22 Batch 1100 Loss 0.5514 Accuracy 0.2979\n",
      "Epoch 22 Batch 1150 Loss 0.5513 Accuracy 0.2980\n",
      "Epoch 22 Batch 1200 Loss 0.5513 Accuracy 0.2980\n",
      "Epoch 22 Batch 1250 Loss 0.5514 Accuracy 0.2981\n",
      "Epoch 22 Batch 1300 Loss 0.5513 Accuracy 0.2980\n",
      "Epoch 22 Batch 1350 Loss 0.5513 Accuracy 0.2981\n",
      "Epoch 22 Batch 1400 Loss 0.5513 Accuracy 0.2982\n",
      "Epoch 22 Batch 1450 Loss 0.5514 Accuracy 0.2983\n",
      "Epoch 22 Batch 1500 Loss 0.5513 Accuracy 0.2982\n",
      "Epoch 22 Loss 0.5513 Accuracy 0.2982\n",
      "Time taken for 1 epoch: 462.4516794681549 secs\n",
      "\n",
      "Epoch 23 Batch 0 Loss 0.5537 Accuracy 0.2881\n",
      "Epoch 23 Batch 50 Loss 0.5400 Accuracy 0.2980\n",
      "Epoch 23 Batch 100 Loss 0.5393 Accuracy 0.2986\n",
      "Epoch 23 Batch 150 Loss 0.5394 Accuracy 0.2998\n",
      "Epoch 23 Batch 200 Loss 0.5392 Accuracy 0.2995\n",
      "Epoch 23 Batch 250 Loss 0.5407 Accuracy 0.3001\n",
      "Epoch 23 Batch 300 Loss 0.5417 Accuracy 0.2999\n",
      "Epoch 23 Batch 350 Loss 0.5414 Accuracy 0.2996\n",
      "Epoch 23 Batch 400 Loss 0.5417 Accuracy 0.2997\n",
      "Epoch 23 Batch 450 Loss 0.5416 Accuracy 0.2997\n",
      "Epoch 23 Batch 500 Loss 0.5424 Accuracy 0.2998\n",
      "Epoch 23 Batch 550 Loss 0.5422 Accuracy 0.3000\n",
      "Epoch 23 Batch 600 Loss 0.5419 Accuracy 0.3000\n",
      "Epoch 23 Batch 650 Loss 0.5412 Accuracy 0.2998\n",
      "Epoch 23 Batch 700 Loss 0.5413 Accuracy 0.2996\n",
      "Epoch 23 Batch 750 Loss 0.5418 Accuracy 0.3000\n",
      "Epoch 23 Batch 800 Loss 0.5423 Accuracy 0.3000\n",
      "Epoch 23 Batch 850 Loss 0.5420 Accuracy 0.3001\n",
      "Epoch 23 Batch 900 Loss 0.5420 Accuracy 0.3001\n",
      "Epoch 23 Batch 950 Loss 0.5418 Accuracy 0.3002\n",
      "Epoch 23 Batch 1000 Loss 0.5415 Accuracy 0.3003\n",
      "Epoch 23 Batch 1050 Loss 0.5410 Accuracy 0.3002\n",
      "Epoch 23 Batch 1100 Loss 0.5407 Accuracy 0.3003\n",
      "Epoch 23 Batch 1150 Loss 0.5404 Accuracy 0.3003\n",
      "Epoch 23 Batch 1200 Loss 0.5405 Accuracy 0.3004\n",
      "Epoch 23 Batch 1250 Loss 0.5404 Accuracy 0.3004\n",
      "Epoch 23 Batch 1300 Loss 0.5403 Accuracy 0.3004\n",
      "Epoch 23 Batch 1350 Loss 0.5404 Accuracy 0.3004\n",
      "Epoch 23 Batch 1400 Loss 0.5403 Accuracy 0.3005\n",
      "Epoch 23 Batch 1450 Loss 0.5406 Accuracy 0.3005\n",
      "Epoch 23 Batch 1500 Loss 0.5405 Accuracy 0.3005\n",
      "Epoch 23 Loss 0.5405 Accuracy 0.3005\n",
      "Time taken for 1 epoch: 465.03824496269226 secs\n",
      "\n",
      "Epoch 24 Batch 0 Loss 0.5653 Accuracy 0.3111\n",
      "Epoch 24 Batch 50 Loss 0.5399 Accuracy 0.3020\n",
      "Epoch 24 Batch 100 Loss 0.5342 Accuracy 0.3016\n",
      "Epoch 24 Batch 150 Loss 0.5313 Accuracy 0.3019\n",
      "Epoch 24 Batch 200 Loss 0.5313 Accuracy 0.3026\n",
      "Epoch 24 Batch 250 Loss 0.5324 Accuracy 0.3032\n",
      "Epoch 24 Batch 300 Loss 0.5309 Accuracy 0.3025\n",
      "Epoch 24 Batch 350 Loss 0.5308 Accuracy 0.3026\n",
      "Epoch 24 Batch 400 Loss 0.5298 Accuracy 0.3022\n",
      "Epoch 24 Batch 450 Loss 0.5301 Accuracy 0.3024\n",
      "Epoch 24 Batch 500 Loss 0.5300 Accuracy 0.3023\n",
      "Epoch 24 Batch 550 Loss 0.5300 Accuracy 0.3022\n",
      "Epoch 24 Batch 600 Loss 0.5304 Accuracy 0.3022\n",
      "Epoch 24 Batch 650 Loss 0.5295 Accuracy 0.3019\n",
      "Epoch 24 Batch 700 Loss 0.5296 Accuracy 0.3020\n",
      "Epoch 24 Batch 750 Loss 0.5296 Accuracy 0.3019\n",
      "Epoch 24 Batch 800 Loss 0.5295 Accuracy 0.3018\n",
      "Epoch 24 Batch 850 Loss 0.5297 Accuracy 0.3019\n",
      "Epoch 24 Batch 900 Loss 0.5298 Accuracy 0.3021\n",
      "Epoch 24 Batch 950 Loss 0.5298 Accuracy 0.3022\n",
      "Epoch 24 Batch 1000 Loss 0.5298 Accuracy 0.3024\n",
      "Epoch 24 Batch 1050 Loss 0.5295 Accuracy 0.3025\n",
      "Epoch 24 Batch 1100 Loss 0.5293 Accuracy 0.3025\n",
      "Epoch 24 Batch 1150 Loss 0.5290 Accuracy 0.3024\n",
      "Epoch 24 Batch 1200 Loss 0.5290 Accuracy 0.3025\n",
      "Epoch 24 Batch 1250 Loss 0.5288 Accuracy 0.3024\n",
      "Epoch 24 Batch 1300 Loss 0.5283 Accuracy 0.3023\n",
      "Epoch 24 Batch 1350 Loss 0.5285 Accuracy 0.3024\n",
      "Epoch 24 Batch 1400 Loss 0.5287 Accuracy 0.3024\n",
      "Epoch 24 Batch 1450 Loss 0.5286 Accuracy 0.3023\n",
      "Epoch 24 Batch 1500 Loss 0.5290 Accuracy 0.3024\n",
      "Epoch 24 Loss 0.5290 Accuracy 0.3024\n",
      "Time taken for 1 epoch: 463.5315339565277 secs\n",
      "\n",
      "Epoch 25 Batch 0 Loss 0.4898 Accuracy 0.2878\n",
      "Epoch 25 Batch 50 Loss 0.5191 Accuracy 0.3007\n",
      "Epoch 25 Batch 100 Loss 0.5163 Accuracy 0.3024\n",
      "Epoch 25 Batch 150 Loss 0.5171 Accuracy 0.3043\n",
      "Epoch 25 Batch 200 Loss 0.5170 Accuracy 0.3045\n",
      "Epoch 25 Batch 250 Loss 0.5169 Accuracy 0.3043\n",
      "Epoch 25 Batch 300 Loss 0.5172 Accuracy 0.3038\n",
      "Epoch 25 Batch 350 Loss 0.5180 Accuracy 0.3043\n",
      "Epoch 25 Batch 400 Loss 0.5187 Accuracy 0.3045\n",
      "Epoch 25 Batch 450 Loss 0.5190 Accuracy 0.3046\n",
      "Epoch 25 Batch 500 Loss 0.5192 Accuracy 0.3044\n",
      "Epoch 25 Batch 550 Loss 0.5192 Accuracy 0.3042\n",
      "Epoch 25 Batch 600 Loss 0.5195 Accuracy 0.3042\n",
      "Epoch 25 Batch 650 Loss 0.5188 Accuracy 0.3038\n",
      "Epoch 25 Batch 700 Loss 0.5192 Accuracy 0.3039\n",
      "Epoch 25 Batch 750 Loss 0.5191 Accuracy 0.3037\n",
      "Epoch 25 Batch 800 Loss 0.5193 Accuracy 0.3038\n",
      "Epoch 25 Batch 850 Loss 0.5190 Accuracy 0.3038\n",
      "Epoch 25 Batch 900 Loss 0.5188 Accuracy 0.3039\n",
      "Epoch 25 Batch 950 Loss 0.5185 Accuracy 0.3039\n",
      "Epoch 25 Batch 1000 Loss 0.5183 Accuracy 0.3040\n",
      "Epoch 25 Batch 1050 Loss 0.5182 Accuracy 0.3041\n",
      "Epoch 25 Batch 1100 Loss 0.5180 Accuracy 0.3040\n",
      "Epoch 25 Batch 1150 Loss 0.5181 Accuracy 0.3041\n",
      "Epoch 25 Batch 1200 Loss 0.5181 Accuracy 0.3041\n",
      "Epoch 25 Batch 1250 Loss 0.5182 Accuracy 0.3041\n",
      "Epoch 25 Batch 1300 Loss 0.5183 Accuracy 0.3041\n",
      "Epoch 25 Batch 1350 Loss 0.5183 Accuracy 0.3041\n",
      "Epoch 25 Batch 1400 Loss 0.5183 Accuracy 0.3042\n",
      "Epoch 25 Batch 1450 Loss 0.5180 Accuracy 0.3041\n",
      "Epoch 25 Batch 1500 Loss 0.5182 Accuracy 0.3040\n",
      "Saving checkpoint for epoch 25 at ./checkpoints/train-2/ckpt-5\n",
      "Epoch 25 Loss 0.5182 Accuracy 0.3040\n",
      "Time taken for 1 epoch: 461.3777492046356 secs\n",
      "\n",
      "Epoch 26 Batch 0 Loss 0.5186 Accuracy 0.3022\n",
      "Epoch 26 Batch 50 Loss 0.5089 Accuracy 0.3044\n",
      "Epoch 26 Batch 100 Loss 0.5070 Accuracy 0.3053\n",
      "Epoch 26 Batch 150 Loss 0.5073 Accuracy 0.3053\n",
      "Epoch 26 Batch 200 Loss 0.5092 Accuracy 0.3057\n",
      "Epoch 26 Batch 250 Loss 0.5087 Accuracy 0.3052\n",
      "Epoch 26 Batch 300 Loss 0.5075 Accuracy 0.3054\n",
      "Epoch 26 Batch 350 Loss 0.5085 Accuracy 0.3053\n",
      "Epoch 26 Batch 400 Loss 0.5084 Accuracy 0.3053\n",
      "Epoch 26 Batch 450 Loss 0.5080 Accuracy 0.3056\n",
      "Epoch 26 Batch 500 Loss 0.5084 Accuracy 0.3055\n",
      "Epoch 26 Batch 550 Loss 0.5089 Accuracy 0.3057\n",
      "Epoch 26 Batch 900 Loss 0.5092 Accuracy 0.3055\n",
      "Epoch 26 Batch 950 Loss 0.5090 Accuracy 0.3055\n",
      "Epoch 26 Batch 1000 Loss 0.5090 Accuracy 0.3054\n",
      "Epoch 26 Batch 1050 Loss 0.5091 Accuracy 0.3056\n",
      "Epoch 26 Batch 1100 Loss 0.5090 Accuracy 0.3057\n",
      "Epoch 26 Batch 1150 Loss 0.5089 Accuracy 0.3058\n",
      "Epoch 26 Batch 1200 Loss 0.5090 Accuracy 0.3059\n",
      "Epoch 26 Batch 1250 Loss 0.5096 Accuracy 0.3060\n",
      "Epoch 26 Batch 1300 Loss 0.5094 Accuracy 0.3059\n",
      "Epoch 26 Batch 1350 Loss 0.5091 Accuracy 0.3058\n",
      "Epoch 26 Batch 1400 Loss 0.5089 Accuracy 0.3057\n",
      "Epoch 26 Batch 1450 Loss 0.5089 Accuracy 0.3057\n",
      "Epoch 26 Batch 1500 Loss 0.5090 Accuracy 0.3056\n",
      "Epoch 26 Loss 0.5090 Accuracy 0.3057\n",
      "Time taken for 1 epoch: 461.6476957798004 secs\n",
      "\n",
      "Epoch 27 Batch 0 Loss 0.4642 Accuracy 0.2653\n",
      "Epoch 27 Batch 50 Loss 0.5060 Accuracy 0.3074\n",
      "Epoch 27 Batch 100 Loss 0.5011 Accuracy 0.3071\n",
      "Epoch 27 Batch 150 Loss 0.5011 Accuracy 0.3084\n",
      "Epoch 27 Batch 200 Loss 0.5005 Accuracy 0.3088\n",
      "Epoch 27 Batch 250 Loss 0.5000 Accuracy 0.3082\n",
      "Epoch 27 Batch 300 Loss 0.5013 Accuracy 0.3085\n",
      "Epoch 27 Batch 350 Loss 0.5015 Accuracy 0.3080\n",
      "Epoch 27 Batch 400 Loss 0.5013 Accuracy 0.3081\n",
      "Epoch 27 Batch 450 Loss 0.5014 Accuracy 0.3082\n",
      "Epoch 27 Batch 500 Loss 0.5003 Accuracy 0.3079\n",
      "Epoch 27 Batch 550 Loss 0.5009 Accuracy 0.3079\n",
      "Epoch 27 Batch 600 Loss 0.5006 Accuracy 0.3078\n",
      "Epoch 27 Batch 650 Loss 0.5004 Accuracy 0.3077\n",
      "Epoch 27 Batch 700 Loss 0.5007 Accuracy 0.3076\n",
      "Epoch 27 Batch 750 Loss 0.5004 Accuracy 0.3073\n",
      "Epoch 27 Batch 800 Loss 0.5002 Accuracy 0.3072\n",
      "Epoch 27 Batch 850 Loss 0.4999 Accuracy 0.3071\n",
      "Epoch 27 Batch 900 Loss 0.5001 Accuracy 0.3071\n",
      "Epoch 27 Batch 950 Loss 0.5004 Accuracy 0.3073\n",
      "Epoch 27 Batch 1000 Loss 0.5003 Accuracy 0.3075\n",
      "Epoch 27 Batch 1050 Loss 0.4998 Accuracy 0.3075\n",
      "Epoch 27 Batch 1100 Loss 0.4995 Accuracy 0.3074\n",
      "Epoch 27 Batch 1150 Loss 0.4997 Accuracy 0.3076\n",
      "Epoch 27 Batch 1200 Loss 0.4997 Accuracy 0.3077\n",
      "Epoch 27 Batch 1250 Loss 0.4997 Accuracy 0.3076\n",
      "Epoch 27 Batch 1300 Loss 0.4994 Accuracy 0.3075\n",
      "Epoch 27 Batch 1350 Loss 0.4996 Accuracy 0.3075\n",
      "Epoch 27 Batch 1400 Loss 0.4997 Accuracy 0.3076\n",
      "Epoch 27 Batch 1450 Loss 0.4996 Accuracy 0.3075\n",
      "Epoch 27 Batch 1500 Loss 0.4996 Accuracy 0.3076\n",
      "Epoch 27 Loss 0.4996 Accuracy 0.3076\n",
      "Time taken for 1 epoch: 462.11815071105957 secs\n",
      "\n",
      "Epoch 28 Batch 0 Loss 0.5454 Accuracy 0.3247\n",
      "Epoch 28 Batch 50 Loss 0.4947 Accuracy 0.3105\n",
      "Epoch 28 Batch 100 Loss 0.4900 Accuracy 0.3097\n",
      "Epoch 28 Batch 150 Loss 0.4922 Accuracy 0.3104\n",
      "Epoch 28 Batch 200 Loss 0.4931 Accuracy 0.3103\n",
      "Epoch 28 Batch 250 Loss 0.4927 Accuracy 0.3104\n",
      "Epoch 28 Batch 300 Loss 0.4922 Accuracy 0.3101\n",
      "Epoch 28 Batch 350 Loss 0.4915 Accuracy 0.3095\n",
      "Epoch 28 Batch 400 Loss 0.4918 Accuracy 0.3098\n",
      "Epoch 28 Batch 450 Loss 0.4908 Accuracy 0.3095\n",
      "Epoch 28 Batch 500 Loss 0.4914 Accuracy 0.3097\n",
      "Epoch 28 Batch 550 Loss 0.4913 Accuracy 0.3095\n",
      "Epoch 28 Batch 600 Loss 0.4916 Accuracy 0.3094\n",
      "Epoch 28 Batch 650 Loss 0.4915 Accuracy 0.3092\n",
      "Epoch 28 Batch 700 Loss 0.4909 Accuracy 0.3089\n",
      "Epoch 28 Batch 750 Loss 0.4911 Accuracy 0.3089\n",
      "Epoch 28 Batch 800 Loss 0.4915 Accuracy 0.3090\n",
      "Epoch 28 Batch 850 Loss 0.4916 Accuracy 0.3090\n",
      "Epoch 28 Batch 900 Loss 0.4915 Accuracy 0.3090\n",
      "Epoch 28 Batch 950 Loss 0.4917 Accuracy 0.3091\n",
      "Epoch 28 Batch 1000 Loss 0.4910 Accuracy 0.3090\n",
      "Epoch 28 Batch 1050 Loss 0.4909 Accuracy 0.3089\n",
      "Epoch 28 Batch 1100 Loss 0.4909 Accuracy 0.3089\n",
      "Epoch 28 Batch 1150 Loss 0.4909 Accuracy 0.3091\n",
      "Epoch 28 Batch 1200 Loss 0.4908 Accuracy 0.3091\n",
      "Epoch 28 Batch 1250 Loss 0.4909 Accuracy 0.3092\n",
      "Epoch 28 Batch 1300 Loss 0.4910 Accuracy 0.3092\n",
      "Epoch 28 Batch 1350 Loss 0.4907 Accuracy 0.3092\n",
      "Epoch 28 Batch 1400 Loss 0.4907 Accuracy 0.3091\n",
      "Epoch 28 Batch 1450 Loss 0.4909 Accuracy 0.3091\n",
      "Epoch 28 Batch 1500 Loss 0.4910 Accuracy 0.3091\n",
      "Epoch 28 Loss 0.4910 Accuracy 0.3091\n",
      "Time taken for 1 epoch: 465.10869216918945 secs\n",
      "\n",
      "Epoch 29 Batch 0 Loss 0.4664 Accuracy 0.2895\n",
      "Epoch 29 Batch 50 Loss 0.4803 Accuracy 0.3081\n",
      "Epoch 29 Batch 100 Loss 0.4816 Accuracy 0.3106\n",
      "Epoch 29 Batch 150 Loss 0.4800 Accuracy 0.3097\n",
      "Epoch 29 Batch 200 Loss 0.4790 Accuracy 0.3094\n",
      "Epoch 29 Batch 250 Loss 0.4830 Accuracy 0.3104\n",
      "Epoch 29 Batch 300 Loss 0.4828 Accuracy 0.3106\n",
      "Epoch 29 Batch 350 Loss 0.4820 Accuracy 0.3105\n",
      "Epoch 29 Batch 400 Loss 0.4803 Accuracy 0.3098\n",
      "Epoch 29 Batch 450 Loss 0.4807 Accuracy 0.3098\n",
      "Epoch 29 Batch 500 Loss 0.4816 Accuracy 0.3097\n",
      "Epoch 29 Batch 550 Loss 0.4821 Accuracy 0.3101\n",
      "Epoch 29 Batch 600 Loss 0.4822 Accuracy 0.3100\n",
      "Epoch 29 Batch 650 Loss 0.4819 Accuracy 0.3100\n",
      "Epoch 29 Batch 700 Loss 0.4817 Accuracy 0.3100\n",
      "Epoch 29 Batch 750 Loss 0.4816 Accuracy 0.3098\n",
      "Epoch 29 Batch 800 Loss 0.4816 Accuracy 0.3099\n",
      "Epoch 29 Batch 850 Loss 0.4820 Accuracy 0.3101\n",
      "Epoch 29 Batch 900 Loss 0.4820 Accuracy 0.3103\n",
      "Epoch 29 Batch 950 Loss 0.4820 Accuracy 0.3103\n",
      "Epoch 29 Batch 1000 Loss 0.4823 Accuracy 0.3104\n",
      "Epoch 29 Batch 1050 Loss 0.4823 Accuracy 0.3104\n",
      "Epoch 29 Batch 1100 Loss 0.4821 Accuracy 0.3105\n",
      "Epoch 29 Batch 1150 Loss 0.4819 Accuracy 0.3105\n",
      "Epoch 29 Batch 1200 Loss 0.4820 Accuracy 0.3106\n",
      "Epoch 29 Batch 1250 Loss 0.4822 Accuracy 0.3107\n",
      "Epoch 29 Batch 1300 Loss 0.4818 Accuracy 0.3107\n",
      "Epoch 29 Batch 1350 Loss 0.4819 Accuracy 0.3106\n",
      "Epoch 29 Batch 1400 Loss 0.4818 Accuracy 0.3106\n",
      "Epoch 29 Batch 1450 Loss 0.4819 Accuracy 0.3107\n",
      "Epoch 29 Batch 1500 Loss 0.4821 Accuracy 0.3107\n",
      "Epoch 29 Loss 0.4821 Accuracy 0.3107\n",
      "Time taken for 1 epoch: 460.216096162796 secs\n",
      "\n",
      "Epoch 30 Batch 0 Loss 0.4967 Accuracy 0.3294\n",
      "Epoch 30 Batch 50 Loss 0.4727 Accuracy 0.3106\n",
      "Epoch 30 Batch 100 Loss 0.4743 Accuracy 0.3129\n",
      "Epoch 30 Batch 150 Loss 0.4728 Accuracy 0.3130\n",
      "Epoch 30 Batch 200 Loss 0.4721 Accuracy 0.3127\n",
      "Epoch 30 Batch 250 Loss 0.4736 Accuracy 0.3127\n",
      "Epoch 30 Batch 300 Loss 0.4739 Accuracy 0.3128\n",
      "Epoch 30 Batch 350 Loss 0.4745 Accuracy 0.3131\n",
      "Epoch 30 Batch 400 Loss 0.4734 Accuracy 0.3130\n",
      "Epoch 30 Batch 450 Loss 0.4737 Accuracy 0.3126\n",
      "Epoch 30 Batch 500 Loss 0.4735 Accuracy 0.3122\n",
      "Epoch 30 Batch 550 Loss 0.4741 Accuracy 0.3124\n",
      "Epoch 30 Batch 600 Loss 0.4747 Accuracy 0.3123\n",
      "Epoch 30 Batch 650 Loss 0.4750 Accuracy 0.3123\n",
      "Epoch 30 Batch 700 Loss 0.4751 Accuracy 0.3123\n",
      "Epoch 30 Batch 750 Loss 0.4748 Accuracy 0.3120\n",
      "Epoch 30 Batch 800 Loss 0.4752 Accuracy 0.3122\n",
      "Epoch 30 Batch 850 Loss 0.4750 Accuracy 0.3122\n",
      "Epoch 30 Batch 900 Loss 0.4749 Accuracy 0.3123\n",
      "Epoch 30 Batch 950 Loss 0.4746 Accuracy 0.3122\n",
      "Epoch 30 Batch 1000 Loss 0.4744 Accuracy 0.3123\n",
      "Epoch 30 Batch 1050 Loss 0.4740 Accuracy 0.3124\n",
      "Epoch 30 Batch 1100 Loss 0.4738 Accuracy 0.3123\n",
      "Epoch 30 Batch 1150 Loss 0.4738 Accuracy 0.3123\n",
      "Epoch 30 Batch 1200 Loss 0.4737 Accuracy 0.3122\n",
      "Epoch 30 Batch 1250 Loss 0.4741 Accuracy 0.3124\n",
      "Epoch 30 Batch 1300 Loss 0.4737 Accuracy 0.3122\n",
      "Epoch 30 Batch 1350 Loss 0.4738 Accuracy 0.3123\n",
      "Epoch 30 Batch 1400 Loss 0.4737 Accuracy 0.3121\n",
      "Epoch 30 Batch 1450 Loss 0.4738 Accuracy 0.3122\n",
      "Epoch 30 Batch 1500 Loss 0.4739 Accuracy 0.3121\n",
      "Saving checkpoint for epoch 30 at ./checkpoints/train-2/ckpt-6\n",
      "Epoch 30 Loss 0.4738 Accuracy 0.3121\n",
      "Time taken for 1 epoch: 467.8076958656311 secs\n",
      "\n",
      "Epoch 31 Batch 0 Loss 0.4589 Accuracy 0.3283\n",
      "Epoch 31 Batch 50 Loss 0.4722 Accuracy 0.3153\n",
      "Epoch 31 Batch 100 Loss 0.4690 Accuracy 0.3142\n",
      "Epoch 31 Batch 150 Loss 0.4677 Accuracy 0.3140\n",
      "Epoch 31 Batch 200 Loss 0.4679 Accuracy 0.3146\n",
      "Epoch 31 Batch 250 Loss 0.4677 Accuracy 0.3138\n",
      "Epoch 31 Batch 300 Loss 0.4670 Accuracy 0.3138\n",
      "Epoch 31 Batch 350 Loss 0.4675 Accuracy 0.3141\n",
      "Epoch 31 Batch 400 Loss 0.4671 Accuracy 0.3138\n",
      "Epoch 31 Batch 450 Loss 0.4671 Accuracy 0.3137\n",
      "Epoch 31 Batch 500 Loss 0.4666 Accuracy 0.3136\n",
      "Epoch 31 Batch 550 Loss 0.4662 Accuracy 0.3136\n",
      "Epoch 31 Batch 600 Loss 0.4662 Accuracy 0.3137\n",
      "Epoch 31 Batch 650 Loss 0.4664 Accuracy 0.3134\n",
      "Epoch 31 Batch 700 Loss 0.4668 Accuracy 0.3136\n",
      "Epoch 31 Batch 750 Loss 0.4671 Accuracy 0.3137\n",
      "Epoch 31 Batch 800 Loss 0.4673 Accuracy 0.3137\n",
      "Epoch 31 Batch 850 Loss 0.4673 Accuracy 0.3138\n",
      "Epoch 31 Batch 900 Loss 0.4674 Accuracy 0.3139\n",
      "Epoch 31 Batch 950 Loss 0.4674 Accuracy 0.3138\n",
      "Epoch 31 Batch 1000 Loss 0.4673 Accuracy 0.3138\n",
      "Epoch 31 Batch 1050 Loss 0.4675 Accuracy 0.3140\n",
      "Epoch 31 Batch 1100 Loss 0.4671 Accuracy 0.3139\n",
      "Epoch 31 Batch 1150 Loss 0.4673 Accuracy 0.3141\n",
      "Epoch 31 Batch 1200 Loss 0.4673 Accuracy 0.3141\n",
      "Epoch 31 Batch 1250 Loss 0.4674 Accuracy 0.3142\n",
      "Epoch 31 Batch 1300 Loss 0.4673 Accuracy 0.3141\n",
      "Epoch 31 Batch 1350 Loss 0.4672 Accuracy 0.3140\n",
      "Epoch 31 Batch 1400 Loss 0.4670 Accuracy 0.3140\n",
      "Epoch 31 Batch 1450 Loss 0.4669 Accuracy 0.3140\n",
      "Epoch 31 Batch 1500 Loss 0.4669 Accuracy 0.3139\n",
      "Epoch 31 Loss 0.4669 Accuracy 0.3139\n",
      "Time taken for 1 epoch: 475.7173159122467 secs\n",
      "\n",
      "Epoch 32 Batch 0 Loss 0.4324 Accuracy 0.3114\n",
      "Epoch 32 Batch 50 Loss 0.4602 Accuracy 0.3147\n",
      "Epoch 32 Batch 100 Loss 0.4599 Accuracy 0.3144\n",
      "Epoch 32 Batch 150 Loss 0.4565 Accuracy 0.3144\n",
      "Epoch 32 Batch 200 Loss 0.4588 Accuracy 0.3156\n",
      "Epoch 32 Batch 250 Loss 0.4586 Accuracy 0.3158\n",
      "Epoch 32 Batch 300 Loss 0.4584 Accuracy 0.3154\n",
      "Epoch 32 Batch 350 Loss 0.4587 Accuracy 0.3152\n",
      "Epoch 32 Batch 400 Loss 0.4595 Accuracy 0.3152\n",
      "Epoch 32 Batch 450 Loss 0.4597 Accuracy 0.3151\n",
      "Epoch 32 Batch 500 Loss 0.4591 Accuracy 0.3150\n",
      "Epoch 32 Batch 550 Loss 0.4593 Accuracy 0.3150\n",
      "Epoch 32 Batch 600 Loss 0.4591 Accuracy 0.3148\n",
      "Epoch 32 Batch 650 Loss 0.4589 Accuracy 0.3148\n",
      "Epoch 32 Batch 700 Loss 0.4593 Accuracy 0.3149\n",
      "Epoch 32 Batch 750 Loss 0.4593 Accuracy 0.3147\n",
      "Epoch 32 Batch 800 Loss 0.4595 Accuracy 0.3147\n",
      "Epoch 32 Batch 850 Loss 0.4592 Accuracy 0.3148\n",
      "Epoch 32 Batch 900 Loss 0.4591 Accuracy 0.3149\n",
      "Epoch 32 Batch 950 Loss 0.4591 Accuracy 0.3149\n",
      "Epoch 32 Batch 1000 Loss 0.4592 Accuracy 0.3150\n",
      "Epoch 32 Batch 1050 Loss 0.4591 Accuracy 0.3150\n",
      "Epoch 32 Batch 1100 Loss 0.4593 Accuracy 0.3150\n",
      "Epoch 32 Batch 1150 Loss 0.4594 Accuracy 0.3151\n",
      "Epoch 32 Batch 1200 Loss 0.4592 Accuracy 0.3151\n",
      "Epoch 32 Batch 1250 Loss 0.4591 Accuracy 0.3151\n",
      "Epoch 32 Batch 1300 Loss 0.4592 Accuracy 0.3151\n",
      "Epoch 32 Batch 1350 Loss 0.4590 Accuracy 0.3150\n",
      "Epoch 32 Batch 1400 Loss 0.4590 Accuracy 0.3150\n",
      "Epoch 32 Batch 1450 Loss 0.4591 Accuracy 0.3149\n",
      "Epoch 32 Batch 1500 Loss 0.4593 Accuracy 0.3149\n",
      "Epoch 32 Loss 0.4593 Accuracy 0.3149\n",
      "Time taken for 1 epoch: 474.9735896587372 secs\n",
      "\n",
      "Epoch 33 Batch 0 Loss 0.4492 Accuracy 0.3092\n",
      "Epoch 33 Batch 50 Loss 0.4504 Accuracy 0.3142\n",
      "Epoch 33 Batch 100 Loss 0.4512 Accuracy 0.3165\n",
      "Epoch 33 Batch 150 Loss 0.4522 Accuracy 0.3168\n",
      "Epoch 33 Batch 200 Loss 0.4511 Accuracy 0.3166\n",
      "Epoch 33 Batch 250 Loss 0.4524 Accuracy 0.3169\n",
      "Epoch 33 Batch 300 Loss 0.4510 Accuracy 0.3165\n",
      "Epoch 33 Batch 350 Loss 0.4519 Accuracy 0.3168\n",
      "Epoch 33 Batch 400 Loss 0.4503 Accuracy 0.3161\n",
      "Epoch 33 Batch 450 Loss 0.4508 Accuracy 0.3161\n",
      "Epoch 33 Batch 500 Loss 0.4513 Accuracy 0.3163\n",
      "Epoch 33 Batch 550 Loss 0.4519 Accuracy 0.3166\n",
      "Epoch 33 Batch 600 Loss 0.4519 Accuracy 0.3163\n",
      "Epoch 33 Batch 650 Loss 0.4520 Accuracy 0.3163\n",
      "Epoch 33 Batch 700 Loss 0.4520 Accuracy 0.3162\n",
      "Epoch 33 Batch 750 Loss 0.4518 Accuracy 0.3162\n",
      "Epoch 33 Batch 800 Loss 0.4519 Accuracy 0.3163\n",
      "Epoch 33 Batch 850 Loss 0.4519 Accuracy 0.3164\n",
      "Epoch 33 Batch 900 Loss 0.4518 Accuracy 0.3163\n",
      "Epoch 33 Batch 950 Loss 0.4518 Accuracy 0.3163\n",
      "Epoch 33 Batch 1000 Loss 0.4520 Accuracy 0.3165\n",
      "Epoch 33 Batch 1050 Loss 0.4519 Accuracy 0.3165\n",
      "Epoch 33 Batch 1100 Loss 0.4518 Accuracy 0.3164\n",
      "Epoch 33 Batch 1150 Loss 0.4519 Accuracy 0.3165\n",
      "Epoch 33 Batch 1200 Loss 0.4519 Accuracy 0.3164\n",
      "Epoch 33 Batch 1250 Loss 0.4519 Accuracy 0.3164\n",
      "Epoch 33 Batch 1300 Loss 0.4519 Accuracy 0.3164\n",
      "Epoch 33 Batch 1350 Loss 0.4518 Accuracy 0.3164\n",
      "Epoch 33 Batch 1400 Loss 0.4515 Accuracy 0.3163\n",
      "Epoch 33 Batch 1450 Loss 0.4514 Accuracy 0.3162\n",
      "Epoch 33 Batch 1500 Loss 0.4516 Accuracy 0.3162\n",
      "Epoch 33 Loss 0.4516 Accuracy 0.3162\n",
      "Time taken for 1 epoch: 465.5863540172577 secs\n",
      "\n",
      "Epoch 34 Batch 0 Loss 0.4109 Accuracy 0.2907\n",
      "Epoch 34 Batch 50 Loss 0.4479 Accuracy 0.3170\n",
      "Epoch 34 Batch 100 Loss 0.4481 Accuracy 0.3184\n",
      "Epoch 34 Batch 150 Loss 0.4469 Accuracy 0.3193\n",
      "Epoch 34 Batch 200 Loss 0.4471 Accuracy 0.3192\n",
      "Epoch 34 Batch 250 Loss 0.4474 Accuracy 0.3190\n",
      "Epoch 34 Batch 300 Loss 0.4472 Accuracy 0.3188\n",
      "Epoch 34 Batch 350 Loss 0.4466 Accuracy 0.3185\n",
      "Epoch 34 Batch 400 Loss 0.4461 Accuracy 0.3184\n",
      "Epoch 34 Batch 450 Loss 0.4468 Accuracy 0.3187\n",
      "Epoch 34 Batch 500 Loss 0.4460 Accuracy 0.3182\n",
      "Epoch 34 Batch 550 Loss 0.4459 Accuracy 0.3182\n",
      "Epoch 34 Batch 600 Loss 0.4463 Accuracy 0.3183\n",
      "Epoch 34 Batch 650 Loss 0.4464 Accuracy 0.3181\n",
      "Epoch 34 Batch 700 Loss 0.4467 Accuracy 0.3180\n",
      "Epoch 34 Batch 750 Loss 0.4468 Accuracy 0.3180\n",
      "Epoch 34 Batch 800 Loss 0.4463 Accuracy 0.3178\n",
      "Epoch 34 Batch 850 Loss 0.4467 Accuracy 0.3181\n",
      "Epoch 34 Batch 900 Loss 0.4464 Accuracy 0.3180\n",
      "Epoch 34 Batch 950 Loss 0.4462 Accuracy 0.3178\n",
      "Epoch 34 Batch 1000 Loss 0.4462 Accuracy 0.3180\n",
      "Epoch 34 Batch 1050 Loss 0.4460 Accuracy 0.3179\n",
      "Epoch 34 Batch 1100 Loss 0.4459 Accuracy 0.3180\n",
      "Epoch 34 Batch 1150 Loss 0.4459 Accuracy 0.3180\n",
      "Epoch 34 Batch 1200 Loss 0.4458 Accuracy 0.3180\n",
      "Epoch 34 Batch 1250 Loss 0.4458 Accuracy 0.3180\n",
      "Epoch 34 Batch 1300 Loss 0.4457 Accuracy 0.3180\n",
      "Epoch 34 Batch 1350 Loss 0.4456 Accuracy 0.3179\n",
      "Epoch 34 Batch 1400 Loss 0.4455 Accuracy 0.3177\n",
      "Epoch 34 Batch 1450 Loss 0.4454 Accuracy 0.3177\n",
      "Epoch 34 Batch 1500 Loss 0.4456 Accuracy 0.3177\n",
      "Epoch 34 Loss 0.4456 Accuracy 0.3177\n",
      "Time taken for 1 epoch: 465.8581647872925 secs\n",
      "\n",
      "Epoch 35 Batch 0 Loss 0.4307 Accuracy 0.3146\n",
      "Epoch 35 Batch 50 Loss 0.4379 Accuracy 0.3175\n",
      "Epoch 35 Batch 100 Loss 0.4364 Accuracy 0.3192\n",
      "Epoch 35 Batch 150 Loss 0.4373 Accuracy 0.3199\n",
      "Epoch 35 Batch 200 Loss 0.4383 Accuracy 0.3197\n",
      "Epoch 35 Batch 250 Loss 0.4383 Accuracy 0.3199\n",
      "Epoch 35 Batch 300 Loss 0.4386 Accuracy 0.3194\n",
      "Epoch 35 Batch 350 Loss 0.4381 Accuracy 0.3191\n",
      "Epoch 35 Batch 400 Loss 0.4375 Accuracy 0.3188\n",
      "Epoch 35 Batch 450 Loss 0.4383 Accuracy 0.3192\n",
      "Epoch 35 Batch 500 Loss 0.4389 Accuracy 0.3194\n",
      "Epoch 35 Batch 550 Loss 0.4394 Accuracy 0.3195\n",
      "Epoch 35 Batch 600 Loss 0.4392 Accuracy 0.3193\n",
      "Epoch 35 Batch 650 Loss 0.4397 Accuracy 0.3192\n",
      "Epoch 35 Batch 700 Loss 0.4396 Accuracy 0.3193\n",
      "Epoch 35 Batch 750 Loss 0.4394 Accuracy 0.3192\n",
      "Epoch 35 Batch 800 Loss 0.4399 Accuracy 0.3193\n",
      "Epoch 35 Batch 850 Loss 0.4395 Accuracy 0.3192\n",
      "Epoch 35 Batch 900 Loss 0.4398 Accuracy 0.3193\n",
      "Epoch 35 Batch 950 Loss 0.4395 Accuracy 0.3193\n",
      "Epoch 35 Batch 1000 Loss 0.4394 Accuracy 0.3193\n",
      "Epoch 35 Batch 1050 Loss 0.4390 Accuracy 0.3193\n",
      "Epoch 35 Batch 1100 Loss 0.4390 Accuracy 0.3192\n",
      "Epoch 35 Batch 1150 Loss 0.4388 Accuracy 0.3193\n",
      "Epoch 35 Batch 1200 Loss 0.4387 Accuracy 0.3192\n",
      "Epoch 35 Batch 1250 Loss 0.4389 Accuracy 0.3193\n",
      "Epoch 35 Batch 1300 Loss 0.4389 Accuracy 0.3193\n",
      "Epoch 35 Batch 1350 Loss 0.4387 Accuracy 0.3192\n",
      "Epoch 35 Batch 1400 Loss 0.4388 Accuracy 0.3192\n",
      "Epoch 35 Batch 1450 Loss 0.4390 Accuracy 0.3192\n",
      "Epoch 35 Batch 1500 Loss 0.4393 Accuracy 0.3192\n",
      "Saving checkpoint for epoch 35 at ./checkpoints/train-2/ckpt-7\n",
      "Epoch 35 Loss 0.4393 Accuracy 0.3192\n",
      "Time taken for 1 epoch: 466.5848686695099 secs\n",
      "\n",
      "Epoch 36 Batch 0 Loss 0.4525 Accuracy 0.3302\n",
      "Epoch 36 Batch 50 Loss 0.4335 Accuracy 0.3198\n",
      "Epoch 36 Batch 100 Loss 0.4309 Accuracy 0.3196\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-0133f04feef8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;31m# inp -> portuguese, tar -> english\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m50\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "  \n",
    "  train_loss.reset_states()\n",
    "  train_accuracy.reset_states()\n",
    "  \n",
    "  # inp -> portuguese, tar -> english\n",
    "  for (batch, (inp, tar)) in enumerate(train_dataset):\n",
    "    train_step(inp, tar)\n",
    "    \n",
    "    if batch % 50 == 0:\n",
    "      print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
    "          epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
    "      \n",
    "  if (epoch + 1) % 5 == 0:\n",
    "    ckpt_save_path = ckpt_manager.save()\n",
    "    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "                                                         ckpt_save_path))\n",
    "    \n",
    "  print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
    "                                                train_loss.result(), \n",
    "                                                train_accuracy.result()))\n",
    "\n",
    "  print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y6APsFrgImLW"
   },
   "source": [
    "The following steps are used for evaluation:\n",
    "\n",
    "* Encode the input sentence using the Portuguese tokenizer (`tokenizer_pt`). Moreover, add the start and end token so the input is equivalent to what the model is trained with. This is the encoder input.\n",
    "* The decoder input is the `start token == tokenizer_en.vocab_size`.\n",
    "* Calculate the padding masks and the look ahead masks.\n",
    "* The `decoder` then outputs the predictions by looking at the `encoder output` and its own output (self-attention).\n",
    "* Select the last word and calculate the argmax of that.\n",
    "* Concatentate the predicted word to the decoder input as pass it to the decoder.\n",
    "* In this approach, the decoder predicts the next word based on the previous words it predicted.\n",
    "\n",
    "Note: The model used here has less capacity to keep the example relatively faster so the predictions maybe less right. To reproduce the results in the paper, use the entire dataset and base transformer model or transformer XL, by changing the hyperparameters above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5buvMlnvyrFm"
   },
   "outputs": [],
   "source": [
    "def evaluate(inp_sentence):\n",
    "  start_token = [tokenizer_ru.vocab_size]\n",
    "  end_token = [tokenizer_ru.vocab_size + 1]\n",
    "  \n",
    "  # inp sentence is portuguese, hence adding the start and end token\n",
    "  inp_sentence = start_token + tokenizer_ru.encode(inp_sentence) + end_token\n",
    "  encoder_input = tf.expand_dims(inp_sentence, 0)\n",
    "  \n",
    "  # as the target is english, the first word to the transformer should be the\n",
    "  # english start token.\n",
    "  decoder_input = [tokenizer_en.vocab_size]\n",
    "  output = tf.expand_dims(decoder_input, 0)\n",
    "    \n",
    "  for i in range(MAX_LENGTH):\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
    "        encoder_input, output)\n",
    "  \n",
    "    # predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "    predictions, attention_weights = transformer(encoder_input, \n",
    "                                                 output,\n",
    "                                                 False,\n",
    "                                                 enc_padding_mask,\n",
    "                                                 combined_mask,\n",
    "                                                 dec_padding_mask)\n",
    "    \n",
    "    # select the last word from the seq_len dimension\n",
    "    predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
    "\n",
    "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "    \n",
    "    # return the result if the predicted_id is equal to the end token\n",
    "    if predicted_id == tokenizer_en.vocab_size+1:\n",
    "      return tf.squeeze(output, axis=0), attention_weights\n",
    "    \n",
    "    # concatentate the predicted_id to the output which is given to the decoder\n",
    "    # as its input.\n",
    "    output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "  return tf.squeeze(output, axis=0), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CN-BV43FMBej"
   },
   "outputs": [],
   "source": [
    "def plot_attention_weights(attention, sentence, result, layer):\n",
    "  fig = plt.figure(figsize=(16, 8))\n",
    "  \n",
    "  sentence = tokenizer_ru.encode(sentence)\n",
    "  \n",
    "  attention = tf.squeeze(attention[layer], axis=0)\n",
    "  \n",
    "  for head in range(attention.shape[0]):\n",
    "    ax = fig.add_subplot(2, 4, head+1)\n",
    "    \n",
    "    # plot the attention weights\n",
    "    ax.matshow(attention[head][:-1, :], cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 10}\n",
    "    \n",
    "    ax.set_xticks(range(len(sentence)+2))\n",
    "    ax.set_yticks(range(len(result)))\n",
    "    \n",
    "    ax.set_ylim(len(result)-1.5, -0.5)\n",
    "        \n",
    "    ax.set_xticklabels(\n",
    "        ['<start>']+[tokenizer_ru.decode([i]) for i in sentence]+['<end>'], \n",
    "        fontdict=fontdict, rotation=90)\n",
    "    \n",
    "    ax.set_yticklabels([tokenizer_en.decode([i]) for i in result \n",
    "                        if i < tokenizer_en.vocab_size], \n",
    "                       fontdict=fontdict)\n",
    "    \n",
    "    ax.set_xlabel('Head {}'.format(head+1))\n",
    "  \n",
    "  plt.tight_layout()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_dataset):\n",
    "    test_accuracy.reset_states()\n",
    "    acc = 0\n",
    "    counter = 0\n",
    "    for ru, en in test_dataset:\n",
    "        test_step(ru, en)\n",
    "#         break\n",
    "        counter += 1\n",
    "        \n",
    "    acc = test_accuracy.result()\n",
    "    acc /= counter\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "transformer.save_weights('best-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fadfb5e79d0>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.load_weights('best-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val_acc = test(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = test(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lU2_yG_vBGza"
   },
   "outputs": [],
   "source": [
    "def translate(sentence, plot=''):\n",
    "  result, attention_weights = evaluate(sentence)\n",
    "  \n",
    "  predicted_sentence = tokenizer_en.decode([i for i in result \n",
    "                                            if i < tokenizer_en.vocab_size])  \n",
    "\n",
    "  print('Input: {}'.format(sentence))\n",
    "  print('Predicted translation: {}'.format(predicted_sentence))\n",
    "  \n",
    "  if plot:\n",
    "    plot_attention_weights(attention_weights, sentence, result, plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, plot=''):\n",
    "  result, attention_weights = evaluate(sentence)\n",
    "  \n",
    "  predicted_sentence = tokenizer_en.decode([i for i in result \n",
    "                                            if i < tokenizer_en.vocab_size])  \n",
    "\n",
    "  return predicted_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YsxrAlvFG8SZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:       .\n",
      "Predicted translation: that 's the problem we have to solve .\n",
      "Real translation: this is a problem we have to solve .\n"
     ]
    }
   ],
   "source": [
    "translate(\"      .\")\n",
    "print (\"Real translation: this is a problem we have to solve .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7EH5y_aqI4t1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:         .\n",
      "Predicted translation: and the neighborhood home was also heard about this idea .\n",
      "Real translation: and my neighboring homes heard about this idea .\n"
     ]
    }
   ],
   "source": [
    "translate(\"        .\")\n",
    "print (\"Real translation: and my neighboring homes heard about this idea .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J-hVCTSUMlkb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:              .\n",
      "Predicted translation: i 'm very quick to share with you some stories about the things that have seen here .\n",
      "Real translation: so i 'll just share with you some stories very quickly of some magical things that have happened .\n"
     ]
    }
   ],
   "source": [
    "translate(\"             .\")\n",
    "print (\"Real translation: so i 'll just share with you some stories very quickly of some magical things that have happened .\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_1MxkSZvz0jX"
   },
   "source": [
    "You can pass different layers and attention blocks of the decoder to the `plot` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t-kFyiOLH0xg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:      -  .\n",
      "Predicted translation: this is the first book i have ever read .\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 8 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real translation: this is the first book i've ever done.\n"
     ]
    }
   ],
   "source": [
    "translate(\"     -  .\", plot='decoder_layer4_block2')\n",
    "print (\"Real translation: this is the first book i've ever done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RqQ1fIsLwkGE"
   },
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, you learned about positional encoding, multi-head attention, the importance of masking and how to create a transformer.\n",
    "\n",
    "Try using a different dataset to train the transformer. You can also create the base transformer or transformer XL by changing the hyperparameters above. You can also use the layers defined here to create [BERT](https://arxiv.org/abs/1810.04805) and train state of the art models. Futhermore, you can implement beam search to get better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = None\n",
    "\n",
    "with open('test.ru.txt', 'r') as f:\n",
    "    test_sentences = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 8.               2015     -.\n",
      "Predicted translation: 8. the winning man to tell the speaker above the foreplayful , wanted to offer the next components of the final documents to the 2015 conference of 2015 to later debate the inclusion of the election .\n",
      "---\n",
      "Input: 5. ,                ,    .\n",
      "Predicted translation: five is the operating room available to provide higher access to nuclear science and businesses , and not only in more fantastic countries , especially in developing countries .\n",
      "---\n",
      "Input:          , ,  ,      ,             .\n",
      "Predicted translation: the chirp technologies used to improve our fertilizer practices by generating bionics , tag and improve agricultural agricultural productivity and peak agricultural output for pests.com .\n",
      "---\n",
      "Input: .    \n",
      "Predicted translation: we stand around for the inclusion and we should put our elections .\n",
      "---\n",
      "Input: 21.        ,      ,    ,  , , ,   ,                        ,                ;\n",
      "Predicted translation: 21. conversations across borders are decoding across borders of civil society and primary infrastructure trade-off by innovation as trans-performers of inclusive drug programs by rebuilding reform .\n",
      "---\n",
      "Input: 4.         ,     ,      .\n",
      "Predicted translation: the fourth common guideline of good government should be respectful , accountability to the dialogue and protection , and reward of human rights .\n",
      "---\n",
      "Input: 5.    ,           :\n",
      "Predicted translation: 5.com appreciate the belief in that argument , and i want to get the focus to the national and the international public health .\n",
      "---\n",
      "Input:  15  *\n",
      "Predicted translation: a 15-by-day board board board meeting .\n",
      "---\n",
      "Input:  :   \n",
      "Predicted translation: the land-by-step response was a falafel : a misleading , and a buffer seat from the drawer .\n",
      "---\n",
      "Input: 7.   ,              ,             ,      ;\n",
      "Predicted translation: 7. we welcome the efforts of state countries , and interested parties and they are interested in conservation , and they are also making it more beautiful , accessible communities and community communities that are also making it community and communities that are communities and communities that they are \n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for s in test_sentences[:10]:\n",
    "    translate(s)\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated = list(map(translate_sentence, test_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8. the winning man to tell the speaker above the foreplayful , wanted to offer the next components of the final documents to the 2015 conference of 2015 to later debate the inclusion of the election .\n",
      "---\n",
      "five is the operating room available to provide higher access to nuclear science and businesses , and not only in more fantastic countries , especially in developing countries .\n",
      "---\n",
      "the chirp technologies used to improve our fertilizer practices by generating bionics , tag and improve agricultural agricultural productivity and peak agricultural output for pests.com .\n",
      "---\n",
      "we stand around for the inclusion and we should put our elections .\n",
      "---\n",
      "21. conversations across borders are decoding across borders of civil society and primary infrastructure trade-off by innovation as trans-performers of inclusive drug programs by rebuilding reform .\n",
      "---\n",
      "the fourth common guideline of good government should be respectful , accountability to the dialogue and protection , and reward of human rights .\n",
      "---\n",
      "5.com appreciate the belief in that argument , and i want to get the focus to the national and the international public health .\n",
      "---\n",
      "a 15-by-day board board board meeting .\n",
      "---\n",
      "the land-by-step response was a falafel : a misleading , and a buffer seat from the drawer .\n",
      "---\n",
      "7. we welcome the efforts of state countries , and interested parties and they are interested in conservation , and they are also making it more beautiful , accessible communities and community communities that are also making it community and communities that are communities and communities that they are \n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for s in range(10):\n",
    "    print(translated[s])\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test.en.txt', 'w') as g:\n",
    "    for s in translated:\n",
    "        g.write(s)\n",
    "        g.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datetime' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-cba1da188793>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'datetime' is not defined"
     ]
    }
   ],
   "source": [
    "str(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = open('eval-en-100.txt', 'w')\n",
    "\n",
    "\n",
    "with open('eval-ru-100.txt') as g:\n",
    "    for idx, line in enumerate(g):\n",
    "        entry = line[:-1]\n",
    "        en = translate_sentence(entry)\n",
    "        outfile.write(en + '\\n')\n",
    "        if (idx + 1) % 500 == 0:\n",
    "            print(f'{idx} lines passed, {str(datetime.datetime.now())}')\n",
    "        \n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of transformer.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
